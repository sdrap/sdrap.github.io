{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Stochastics","text":"<p>These lecture notes, still a work in progress, are for a course taught at Shanghai Jiao Tong University, for graduate students.</p>"},{"location":"#course-objective","title":"Course Objective","text":"<p>This lecture aims at the evolution over time of systems that are subject to uncertainty: stochastic processes.</p> <p>Throughout this lecture we will address the following fundamental concepts:</p> <ul> <li>Probability: probability space, probability measure, expectation, measure change, conditional expectation, stochastic kernel, independence as well as fundamental inequalities and relations.</li> <li>Discrete Martingales: discrete stochastic processes, information, stopping times, martingales and related fundamental results (convergence, law of large numbers, etc.)</li> <li>Markov Processes: concept of memoryless stochastic processes, discrete time results, construction of the Brownian motion.</li> <li>Ito-Integral and Calculus: construction of the stochastic integral, Ito-formula, applications.</li> <li>Stochastic Exponential: measure change, Girsanov formula and applications</li> <li>Stochastic Differential Equations: existence and uniqueness, strong vs weak solutions, ...</li> <li>SDE and PDE: Relation between PDE and SDE, Kolmogorov equation, Feynman-Kak formula and applications.</li> </ul>"},{"location":"#concrete-approach","title":"Concrete Approach","text":"<p>The theoretical lecture combines blackboard lectures with practical applications. Lecture notes will be provided and updated during the course. The evaluation of the lecture will consists of </p> <ul> <li>Homework: Once every two weeks to be handed out within two weeks by groups of 5-6</li> <li>Quizz: 3-4 quizz, in class, 30 min about the past lectures.</li> <li>Final Exam: 120 min final exam at the end of the semester.</li> </ul>"},{"location":"#prerequisite","title":"Prerequisite","text":"<p>The lecture suppose that students knows about basic algebra, analysis. A previous knowledge about measure theory might be of help but nor necessary.</p>"},{"location":"#literature","title":"Literature","text":"<p>The lecture follows the present lecture notes. However those are inspired by uncountably many textbooks on the topic from which we present a short selection:</p> <ul> <li>Rick Durret<sup>1</sup>: Excellent introduction to discrete stochastic processes (markov processesw and martingales and Brownian motion). No stochastic integral and SDEs</li> <li>Steve Shreve<sup>2</sup><sup>3</sup>: Both books with a focus on Finance. The first one is discrete the second continuous and both approchable.</li> <li>Olav Kallenberg<sup>4</sup>: Complete from probability to stochastic processes. General stochastic processes.</li> <li>Philip Protter<sup>5</sup>: Complete about general stochastic processes (w/wo jumps). Complex.</li> <li>Delacherie and Meyer<sup>6</sup><sup>7</sup>: The bible however machine typed and French</li> </ul>"},{"location":"#references","title":"References","text":"<ol> <li> <p>Rick Durrett. Probability: Theory and Examples. Cambridge Series in Statistical and Probabilistic Mathematics, 2010.\u00a0\u21a9</p> </li> <li> <p>Steven E. Shreve. Stochastic Calculus for Finance. Volume I of Springer Finance. Springer-Verlag, New York, 2004. ISBN 0-387-40100-8. The binomial asset pricing model.\u00a0\u21a9</p> </li> <li> <p>Steven E. Shreve. Stochastic Calculus for Finance. Volume II of Springer Finance. Springer-Verlag, New York, 2004. ISBN 0-387-40101-6. Continuous-time models.\u00a0\u21a9</p> </li> <li> <p>Olav Kallenberg. Foundations of Modern Probability. Probability and its Applications (New York). Springer-Verlag, New York, 2nd edition, 2002.\u00a0\u21a9</p> </li> <li> <p>Philip E. Protter. Stochastic Integration and Differential Equations. Springer, 2nd edition, 2005.\u00a0\u21a9</p> </li> <li> <p>Claude Dellacherie and Paul-Andr\u00e9 Meyer. Probabilities and Potential. A. Volume 29 of North-Holland Mathematics Studies. North-Holland Publishing Co., Amsterdam, 1978. ISBN 0-7204-0701-X.\u00a0\u21a9</p> </li> <li> <p>Claude Dellacherie and Paul Andr\u00e9 Meyer. Probabilities and Potential. B. Volume 72 of North-Holland Mathematics Studies. North-Holland Publishing Co., Amsterdam, 1982.\u00a0\u21a9</p> </li> </ol>"},{"location":"javascripts/node_modules/mathjax/","title":"MathJax","text":""},{"location":"javascripts/node_modules/mathjax/#beautiful-math-in-all-browsers","title":"Beautiful math in all browsers","text":"<p>MathJax is an open-source JavaScript display engine for LaTeX, MathML, and AsciiMath notation that works in all modern browsers.  It was designed with the goal of consolidating the recent advances in web technologies into a single, definitive, math-on-the-web platform supporting the major browsers and operating systems.  It requires no setup on the part of the user (no plugins to download or software to install), so the page author can write web documents that include mathematics and be confident that users will be able to view it naturally and easily.  Simply include MathJax and some mathematics in a web page, and MathJax does the rest.</p> <p>Some of the main features of MathJax include:</p> <ul> <li> <p>High-quality display of LaTeX, MathML, and AsciiMath notation in HTML pages</p> </li> <li> <p>Supported in most browsers with no plug-ins, extra fonts, or special   setup for the reader</p> </li> <li> <p>Easy for authors, flexible for publishers, extensible for developers</p> </li> <li> <p>Supports math accessibility, cut-and-paste interoperability, and other   advanced functionality</p> </li> <li> <p>Powerful API for integration with other web applications</p> </li> </ul> <p>See http://www.mathjax.org/ for additional details about MathJax, and https://docs.mathjax.org for the MathJax documentation.</p>"},{"location":"javascripts/node_modules/mathjax/#mathjax-components","title":"MathJax Components","text":"<p>MathJax version 3 uses files called components that contain the various MathJax modules that you can include in your web pages or access on a server through NodeJS.  Some components combine all the pieces you need to run MathJax with one or more input formats and a particular output format, while other components are pieces that can be loaded on demand when needed, or by a configuration that specifies the pieces you want to combine in a custom way.  For usage instructions, see the MathJax documentation.</p> <p>Components provide a convenient packaging of MathJax's modules, but it is possible for you to form your own custom components, or to use MathJax's modules directly in a node application on a server.  There are web examples showing how to use MathJax in web pages and how to build your own components, and node examples illustrating how to use components in node applications or call MathJax modules directly.</p>"},{"location":"javascripts/node_modules/mathjax/#whats-in-this-repository","title":"What's in this Repository","text":"<p>This repository contains only the component files for MathJax, not the source code for MathJax (which are available in a separate MathJax source repository).  These component files are the ones served by the CDNs that offer MathJax to the web.  In version 2, the files used on the web were also the source files for MathJax, but in version 3, the source files are no longer on the CDN, as they are not what are run in the browser.</p> <p>The components are stored in the <code>es5</code> directory, and are in ES5 format for the widest possible compatibility.  In the future, we may make an <code>es6</code> directory containing ES6 versions of the components.</p>"},{"location":"javascripts/node_modules/mathjax/#installation-and-use","title":"Installation and Use","text":""},{"location":"javascripts/node_modules/mathjax/#using-mathjax-components-from-a-cdn-on-the-web","title":"Using MathJax components from a CDN on the web","text":"<p>If you are loading MathJax from a CDN into a web page, there is no need to install anything.  Simply use a <code>script</code> tag that loads MathJax from the CDN.  E.g.,</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>See the MathJax documentation, the MathJax Web Demos, and the MathJax Component Repository for more information.</p>"},{"location":"javascripts/node_modules/mathjax/#hosting-your-own-copy-of-the-mathjax-components","title":"Hosting your own copy of the MathJax Components","text":"<p>If you want to host MathJax from your own server, you can do so by installing the <code>mathjax</code> package using <code>npm</code> and moving the <code>es5</code> directory to an appropriate location on your server:</p> <pre><code>npm install mathjax@3\nmv node_modules/mathjax/es5 &lt;path-to-server-location&gt;/mathjax\n</code></pre> <p>Note that we are still making updates to version 2, so include <code>@3</code> when you install, since the latest chronological version may not be version 3.</p> <p>Alternatively, you can get the files via GitHub:</p> <pre><code>git clone https://github.com/mathjax/MathJax.git mj-tmp\nmv mj-tmp/es5 &lt;path-to-server-location&gt;/mathjax\nrm -rf mj-tmp\n</code></pre> <p>Then (in either case) you can use a script tag like the following:</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"&lt;url-to-your-site&gt;/mathjax/tex-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>where <code>&lt;url-to-your-site&gt;</code> is replaced by the URL to the location where you moved the MathJax files above.</p> <p>See the documentation for details.</p>"},{"location":"javascripts/node_modules/mathjax/#using-mathjax-components-in-a-node-application","title":"Using MathJax components in a node application","text":"<p>To use MathJax components in a node application, install the <code>mathjax</code> package:</p> <pre><code>npm install mathjax@3\n</code></pre> <p>(we are still making updates to version 2, so you should include <code>@3</code> since the latest chronological version may not be version 3).</p> <p>Then require <code>mathjax</code> within your application:</p> <pre><code>require('mathjax').init({ ... }).then((MathJax) =&gt; { ... });\n</code></pre> <p>where the first <code>{ ... }</code> is a MathJax configuration, and the second <code>{ ... }</code> is the code to run after MathJax has been loaded.  E.g.</p> <pre><code>require('mathjax').init({\n  loader: {load: ['input/tex', 'output/svg']}\n}).then((MathJax) =&gt; {\n  const svg = MathJax.tex2svg('\\\\frac{1}{x^2-1}', {display: true});\n  console.log(MathJax.startup.adaptor.outerHTML(svg));\n}).catch((err) =&gt; console.log(err.message));\n</code></pre> <p>Note: this technique is for node-based application only, not for browser applications.  This method sets up an alternative DOM implementation, which you don't need in the browser, and tells MathJax to use node's <code>require()</code> command to load external modules.  This setup will not work properly in the browser, even if you webpack it or bundle it in other ways.</p> <p>See the documentation and the MathJax Node Repository for more details.</p>"},{"location":"javascripts/node_modules/mathjax/#reducing-the-size-of-the-components-directory","title":"Reducing the Size of the Components Directory","text":"<p>Since the <code>es5</code> directory contains all the component files, so if you are only planning one use one configuration, you can reduce the size of the MathJax directory by removing unused components. For example, if you are using the <code>tex-chtml.js</code> component, then you can remove the <code>tex-mml-chtml.js</code>, <code>tex-svg.js</code>, <code>tex-mml-svg.js</code>, <code>tex-chtml-full.js</code>, and <code>tex-svg-full.js</code> configurations, which will save considerable space.  Indeed, you should be able to remove everything other than <code>tex-chtml.js</code>, and the <code>input/tex/extensions</code>, <code>output/chtml/fonts/woff-v2</code>, <code>adaptors</code>, <code>a11y</code>, and <code>sre</code> directories.  If you are using the results only on the web, you can remove <code>adaptors</code> as well.</p> <p>If you are not using A11Y support (e.g., speech generation, or semantic enrichment), then you can remove <code>a11y</code> and <code>sre</code> as well (though in this case you may need to disable the assistive tools in the MathJax contextual menu in order to avoid MathJax trying to load them when they aren't there).</p> <p>If you are using SVG rather than CommonHTML output (e.g., <code>tex-svg.js</code> rather than <code>tex-chtml.js</code>), you can remove the <code>output/chtml/fonts/woff-v2</code> directory.  If you are using MathML input rather than TeX (e.g., <code>mml-chtml.js</code> rather than <code>tex-chtml.js</code>), then you can remove <code>input/tex/extensions</code> as well.</p>"},{"location":"javascripts/node_modules/mathjax/#the-component-files-and-pull-requests","title":"The Component Files and Pull Requests","text":"<p>The <code>es5</code> directory is generated automatically from the contents of the MathJax source repository.  You can rebuild the components using the command</p> <pre><code>npm run make-es5 --silent\n</code></pre> <p>Note that since the contents of this repository are generated automatically, you should not submit pull requests that modify the contents of the <code>es5</code> directory.  If you wish to submit a modification to MathJax, you should make a pull request in the MathJax source repository.</p>"},{"location":"javascripts/node_modules/mathjax/#mathjax-community","title":"MathJax Community","text":"<p>The main MathJax website is http://www.mathjax.org, and it includes announcements and other important information.  A MathJax user forum for asking questions and getting assistance is hosted at Google, and the MathJax bug tracker is hosted at GitHub.</p> <p>Before reporting a bug, please check that it has not already been reported.  Also, please use the bug tracker (rather than the help forum) for reporting bugs, and use the user's forum (rather than the bug tracker) for questions about how to use MathJax.</p>"},{"location":"javascripts/node_modules/mathjax/#mathjax-resources","title":"MathJax Resources","text":"<ul> <li>MathJax Documentation</li> <li>MathJax Components</li> <li>MathJax Source Code</li> <li>MathJax Web Examples</li> <li>MathJax Node Examples</li> <li>MathJax Bug Tracker</li> <li>MathJax Users' Group</li> </ul>"},{"location":"lecture/00-Introduction/000-notations/","title":"Notations","text":""},{"location":"lecture/00-Introduction/000-notations/#mathematical-notations","title":"Mathematical Notations","text":"<p>The following notations will be used throughout the course:</p> <ul> <li>Natural Numbers: \\(\\mathbb{N} = \\{1, 2, \\ldots\\}\\), \\(\\mathbb{N}_0 = \\{0, 1, 2, \\ldots\\}\\).</li> <li>Integers: \\(\\mathbb{Z} = \\{\\ldots, -2, -1, 0, 1, 2, \\ldots\\}\\)</li> <li>Rational Numbers: \\(\\mathbb{Q} = \\{ p/q\\colon p \\in \\mathbb{Z}, q \\in \\mathbb{N}\\}\\)</li> <li>Real Numbers: \\(\\mathbb{R}\\)</li> <li>Vectors in \\(\\mathbb{R}^d\\) are denoted in bold font, \\(\\boldsymbol{x} = (x^1, \\dots, x^d)\\), and are assumed to be column vectors.  </li> <li>Vectors with positive components \\(\\mathbb{R}^d_+ = \\{\\boldsymbol{x} \\in \\mathbb{R}^d : x^k \\geq 0, k=1,\\ldots,d\\}\\) and vectors with strictly positive components \\(\\mathbb{R}^d_{++} = \\{\\boldsymbol{x} \\in \\mathbb{R}^d : x^k &gt; 0, k=1,\\ldots,d\\}\\).  </li> <li>Scalar Product: \\(\\boldsymbol{x} \\cdot \\boldsymbol{y} := \\sum x_k y_k\\) denotes the scalar product of \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\) in \\(\\mathbb{R}^d\\).  </li> <li>\\(\\beta \\boldsymbol{x} := (\\beta x_1, \\ldots, \\beta x_d)\\) represents the multiplication of \\(\\boldsymbol{x}\\) in \\(\\mathbb{R}^d\\) by a scalar \\(\\beta \\in \\mathbb{R}\\).  </li> <li>\\(\\boldsymbol{x} + \\boldsymbol{y} := (x_1 + y_1, \\ldots, x_d + y_d)\\) represents vector addition in \\(\\mathbb{R}^d\\).  </li> <li> <p>For scalars \\(x, y \\in \\mathbb{R}\\), the following notations are used:  </p> \\[   x \\vee y = \\max\\{x, y\\}, \\quad x \\wedge y = \\min\\{x, y\\}, \\quad x^+ = \\max\\{x, 0\\}, \\quad x^- = \\max\\{-x, 0\\}. \\] <p>Notably, \\(x = x^+ - x^-\\) and \\(|x| = x^+ + x^-\\).  </p> </li> </ul>"},{"location":"lecture/00-Introduction/000-notations/#colorenvironment-conventions","title":"Color/Environment conventions","text":"<p>Definition</p> <p>For a ... we define</p> <p>Remark</p> <p>Note that  </p> <p>Example</p> <p>As an example we consider </p> <p>Theorem</p> <p>Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space...</p> <p>Proposition</p> <p>Assuming no-arbitrage for the financial market, the followign assertions holds...</p> <p>Corollary</p> <p>As a corrolary to the previous proposition, it holds</p> <p>Lemma</p> <p>In the case where \\(P^\\ast\\) is equivalent to \\(P\\), it holds...</p> <p>Proof</p> <p>In a first step we show that \\((i)\\) implies \\((ii)\\)...</p> <p>Exercise</p>"},{"location":"lecture/01-Sets-Functions/010-introduction/","title":"Introduction","text":"<p>This Chapter is dedicated to the fundamentals behind measure theory. It covers and recall basic notions such as set theory, the algebraic properties of which. It defines the notion of measurable and topological spaces. It finally describe the properties of functions\u2014measurable and continuous\u2014that are compatibles with this algebraic set structures.</p> <ul> <li>Sets</li> <li>Measurable / Topological Spaces</li> <li>Measurable / Continuous Functions</li> </ul>"},{"location":"lecture/01-Sets-Functions/011-sets/","title":"Sets","text":"<p>This lecture will not cover the foundations of logic and the axiomatic approach to set theory. We stick to the naive set theory in the sense that we will not encounter \"too large objects\" for which paradox may arise if the objects are defined too naively\u2014see Russell's paradox. We refer to Jech<sup>1</sup> for a comprehensive overview of set theory.</p> <p>The language of set theory consists of the binary predicate \\(\\in\\) that describes the membership relation between objects, that is, \\(x\\in A\\) means that \\(x\\) is an element or a member of \\(A\\). A set is a collection of objects that belong to it. We usually denote sets with capital letters \\(A,B,\\ldots\\) and elements of these sets with lowercase letters \\(x,y,\\ldots\\). Note that sets can also be elements of other sets. We can define sets either by an explicit listing such as \\(A=\\{\\text{pig},\\text{horse},\\text{table}\\}\\), or by a \"property rule\" \\(P\\), i.e. \\(A=\\{x\\colon x\\text{ has property }P\\}\\). To avoid logical problems, we assume that this rule applies only to some elements of a given larger set. For instance,</p> \\[2\\mathbb{Z}:=\\{x\\in\\mathbb{Z}\\colon x\\text{ is even}\\}\\] <p>Typical sets we will use are:</p> <ul> <li>Natural numbers: \\(\\mathbb{N}=\\{1,2,3,\\ldots\\}\\) and \\(\\mathbb{N}_0=\\{0,1,2,\\ldots\\}\\);</li> <li>Integers: \\(\\mathbb{Z}=\\{\\ldots,-2,-1,0,1,2,\\ldots\\}\\);</li> <li>Rational numbers: \\(\\mathbb{Q}\\);</li> <li>Real numbers: \\(\\mathbb{R}\\);</li> <li>\\(\\mathbb{R}^d\\), the \\(d\\)-dimensional Euclidean space.</li> </ul> <p>Let us introduce the following operations or relations between sets:</p> <ul> <li>Inclusion \\(\\subseteq\\): Given two sets \\(A\\) and \\(B\\), we denote by \\(A\\subseteq B\\) that every element \\(x\\) in \\(A\\) also belongs to \\(B\\).     The inclusion defines a partial order (it is reflexive, transitive, and antisymmetric).</li> <li>Empty set \\(\\emptyset\\): The set containing no elements is called the empty set and is denoted by \\(\\emptyset\\).     It is the smallest set with respect to inclusion and is universal in this axiomatic system.</li> <li>Intersection \\(\\cap\\): The intersection of two sets \\(A\\) and \\(B\\), denoted by \\(A\\cap B\\), is defined as the largest set that is contained in both \\(A\\) and \\(B\\).     In other words, \\(A\\cap B=\\{x\\colon x\\in A\\text{ and }x\\in B\\}\\).     Two sets \\(A\\) and \\(B\\) are said to be disjoint if their intersection is empty, i.e. \\(A\\cap B=\\emptyset\\).</li> <li>Union \\(\\cup\\): The union of two sets \\(A\\) and \\(B\\), denoted by \\(A\\cup B\\), is defined as the smallest set that contains all elements of either \\(A\\) or \\(B\\).     In other words, \\(A\\cup B=\\{x\\colon x\\in A\\text{ or }x\\in B\\}\\).</li> <li>Complement \\({}^c\\): Given a set \\(X\\), the complement of a subset \\(A\\) (relative to \\(X\\)) is given by \\(A^c=\\{x\\in X\\colon x\\not\\in A\\}\\).</li> <li>Difference \\(\\setminus\\): Given two subsets \\(A\\) and \\(B\\) of a set \\(X\\), the difference \\(A\\setminus B\\) is defined as \\(\\{x\\colon x\\in A\\text{ and }x\\not\\in B\\}\\), which is equivalent to \\(A\\cap B^c\\).</li> <li>Symmetric difference \\(\\Delta\\) Given two subsets \\(A\\) and \\(B\\) of a set \\(X\\), the symmetric difference is defined as \\(A\\Delta B:=(A\\setminus B)\\cup(B\\setminus A)\\).</li> </ul> <p>Given a set \\(X\\), we can define its power set as \\(2^X:=\\{A\\subseteq X\\}\\). Note that the power set always contains both \\(X\\) and the empty set \\(\\emptyset\\). In particular, \\(2^\\emptyset=\\{\\emptyset\\}\\) is the set containing only the empty set.</p> <p>Proposition</p> <p>The power set \\(2^X\\) of a set \\(X\\), together with the operations \\(\\cup\\), \\(\\cap\\), \\({}^c\\), and the two distinguished elements \\(\\emptyset\\) and \\(X\\), satisfies the properties of a Boolean algebra.</p> <ol> <li>\\(\\emptyset^c=X\\) and \\(X^c=\\emptyset\\).</li> <li>Identity laws: \\(A\\cap\\emptyset=\\emptyset\\), \\(A\\cap X=A\\), \\(A\\cup\\emptyset=A\\), and \\(A\\cup X=X\\).</li> <li>Complement laws: \\(A\\cap A^c=\\emptyset\\) and \\(A\\cup A^c=X\\).</li> <li>Double complement law: \\((A^c)^c=A\\).</li> <li>Idempotent laws: \\(A\\cap A=A\\) and \\(A\\cup A=A\\).</li> <li>De Morgan laws: \\((A\\cap B)^c=A^c\\cup B^c\\) and \\((A\\cup B)^c=A^c\\cap B^c\\).</li> <li>Commutative laws: \\(A\\cap B=B\\cap A\\) and \\(A\\cup B=B\\cup A\\).</li> <li>Associative laws: \\(A\\cap(B\\cap C)=(A\\cap B)\\cap C\\) and \\(A\\cup(B\\cup C)=(A\\cup B)\\cup C\\).</li> <li>Distributive laws: \\(A\\cap(B\\cup C)=(A\\cap B)\\cup(A\\cap C)\\) and \\(A\\cup(B\\cap C)=(A\\cup B)\\cap(A\\cup C)\\).</li> </ol> Proof <p>Left as an exercise.</p> Remark: Boolean Ring <p>The power set \\(2^X\\) of a set \\(X\\), together with the operations \\(\\Delta\\), \\(\\cap\\), and the two distinguished elements \\(\\emptyset\\) and \\(X\\), satisfies the properties of a Boolean ring, that is:</p> <ol> <li>\\(\\Delta\\) and \\(\\cap\\) are associative and commutative.</li> <li>Identity law: \\(A\\Delta\\emptyset=A\\) and \\(A\\cap X=A\\).</li> <li>Inverse law: \\(A\\Delta A^c=X\\).</li> <li>Distributive law: \\(A\\cap(B\\Delta C)=(A\\cap B)\\Delta(A\\cap C)\\).</li> </ol> <p>The Cartesian product \\(A\\times B\\) of two sets \\(A\\) and \\(B\\) is the collection of ordered pairs \\((x,y)\\) such that \\(x\\in A\\) and \\(y\\in B\\).</p> <p>Given the Cartesian product, we can define functions:</p> <p>Definition: Functions</p> <p>A function \\(f:X\\to Y\\) is an ordered triple \\((X, Y, G)\\), where \\(G\\subseteq X\\times Y\\) is called the graph of \\(f\\) and satisfies the property that for every \\(x\\in X\\), there exists a unique \\(y\\in Y\\) such that \\((x,y)\\in G\\).(1)</p> <ol> <li>This definition requires \\(Y\\) to be non-empty if \\(X\\) is non-empty. If \\(X\\) is empty, then \\(G=\\emptyset\\), and this function is called the empty function.</li> </ol> <p>For \\(x\\in X\\), this unique element \\(y\\in Y\\) such that \\((x,y)\\in G\\) is denoted by \\(f(x)\\).</p> <p>A function is called</p> <ul> <li>injective if \\(x\\neq y\\) implies \\(f(x)\\neq f(y)\\);</li> <li>surjective if for every \\(y\\in Y\\), there exists \\(x\\in X\\) such that \\(f(x)=y\\);</li> <li>bijective if it is both injective and surjective.</li> </ul> <p>Given a set \\(I\\) and a non-empty set \\(X\\), a family of elements in \\(X\\) is a function \\(f:I\\to X\\), \\(i\\mapsto f(i)\\). We usually denote this function as \\((x_i)_{i\\in I}\\), where \\(x_i\\in X\\) for every \\(i\\).(1) If \\(I=\\mathbb{N}\\), we also call it a sequence or a countable family. If there is no risk of confusion, we use the notation \\((x_i)\\) for an arbitrary family of elements in \\(X\\) and \\((x_n)\\) for a countable family\u2014or sequence\u2014in \\(X\\).</p> <ol> <li>A family \\((x_i)_{i\\in I}\\) is different from the collection \\(\\{x_i: i\\in I\\}\\).   For instance, consider the family \\((x_n)_{n\\in\\mathbb{N}}\\) where \\(x_n=1\\) if \\(n\\) is odd and \\(x_n=0\\) if \\(n\\) is even.   Then \\(\\{x_n:n\\in\\mathbb{N}\\}=\\{0,1\\}\\), while \\((x_n)_{n\\in\\mathbb{N}}=(1,0,1,0,1,\\ldots)\\).}</li> </ol> <p>Given a family of sets \\((X_i)=(X_i)_{i\\in I}\\), we define the Cartesian product of these sets as the collection of families \\((x_i)=(x_i)_{i\\in I}\\) such that \\(x_i\\) is in \\(X_i\\) for every \\(i\\), denoted by</p> \\[ \\prod X_i=\\prod_{i\\in I}X_i=\\{(x_i)\\colon x_i\\in X_i \\text{ for every } i\\}. \\] <p>Proposition</p> <p>The Boolean algebra \\(2^X\\) is complete, meaning that for every family of sets \\((A_i)\\), there exists a minimum and a maximum with respect to inclusion in \\(2^X\\).(1)</p> <ol> <li>That is, there exist subsets \\(A\\) and \\(B\\) such that \\(A\\subseteq A_i\\subseteq B\\) for every \\(i\\), and if \\(\\tilde{A}\\) and \\(\\tilde{B}\\) also satisfy this property, then \\(\\tilde{A}\\subseteq A\\) and \\(B\\subseteq \\tilde{B}\\).</li> </ol> <p>We denote this minimum and maximum by</p> \\[ \\begin{align*}   \\bigcap A_i&amp;=\\{x\\in X\\colon x\\in A_i \\text{ for all } i\\}   &amp;   \\bigcup A_i&amp; =\\{x\\in X\\colon x\\in A_i \\text{ for some } i\\} \\end{align*} \\] <p>Furthermore, it holds that</p> \\[ \\begin{equation} \\left(\\bigcap A_i\\right)^c=\\bigcup A_i^c, \\quad \\left(\\bigcup A_i\\right)^c=\\bigcap A_i^c \\end{equation} \\] <p>as well as</p> \\[ \\begin{equation} C\\cap\\left(\\bigcup A_i\\right)=\\bigcup (C\\cap A_i), \\quad C\\cup\\left(\\bigcap A_i\\right)=\\bigcap (A_i\\cup C) \\end{equation} \\] Proof <p>Left to the reader.</p> <p>Given a function \\(f:X\\to Y\\) and subsets \\(A\\subseteq X\\) and \\(B\\subseteq Y\\), we define the</p> <ul> <li> <p>image: </p> \\[ \\begin{equation} f(A) := \\left\\{ y\\in Y\\colon y=f(x) \\text{ for some } x\\in A \\right\\}\\subseteq Y \\end{equation} \\] </li> <li> <p>pre-image:</p> \\[ \\begin{equation} f^{-1}(B) := \\left\\{ x\\in X\\colon f(x)\\in B \\right\\}\\subseteq X \\end{equation} \\] </li> </ul> <p></p> <p>Proposition</p> <p>The image and pre-image define functions from \\(2^X\\) to \\(2^Y\\) and \\(2^Y\\) to \\(2^X\\), respectively, with the following properties:</p> \\[ \\begin{align}   f(A_1) &amp;\\subseteq f(A_2), &amp; f^{-1}(B_1) &amp;\\subseteq f^{-1}(B_2)\\\\   f\\left(\\cup A_i\\right) &amp;= \\cup f(A_i), &amp; f^{-1}\\left(\\cup B_j\\right) &amp;= \\cup f^{-1}(B_j)\\\\   f\\left(\\cap A_i\\right) &amp;\\subseteq \\cap f(A_i), &amp; f^{-1}\\left(\\cap B_j\\right) &amp;= \\cap f^{-1}(B_j)\\\\   f(A^c) &amp;\\subseteq f(A)^c, &amp; f^{-1}(B^c) &amp;= \\left(f^{-1}(B)\\right)^c\\\\   A &amp;\\subseteq f^{-1}\\left(f(A)\\right), &amp; B &amp;\\subseteq f\\left(f^{-1}(B)\\right) \\end{align} \\] Proof <p>Left to the reader.</p> <p>Remark</p> <p>The pre-image function\u2014unlike the image function\u2014respects the Boolean operations of the power set. In other words, it is a morphism. Any structure defined on a subset of \\(2^Y\\) with intersection, union, and complementation will carry over to a substructure of \\(2^X\\) with the same properties via the pre-image function.</p> <p>As for the cardinality of sets, we say that a set \\(X\\) has cardinality smaller than \\(Y\\) if there exists an injective function \\(f:X\\to Y\\). Two sets have same cardinality if such a function is bijective. A set \\(X\\) is called </p> <ul> <li>finite if there exists \\(n\\) in \\(\\mathbb{N}\\) such \\(X\\) has the same cardinality as \\(\\{1, \\ldots, n\\}\\).   In this case \\(n\\) is the cardinality of this set.</li> <li>enumerable if \\(X\\) has the same cardinality as \\(\\mathbb{N}\\).   In this case we denote by \\(\\aleph_0\\) the cardinality of this set.</li> <li>countable if \\(X\\) is either finite or enumerable.</li> </ul> <p>The emptyset has cardinality \\(0\\) and is the only such set. If \\(X\\) is finite of cardinality \\(n\\) in \\(\\mathbb{N}\\), then the cardinality of \\(2^X\\) is \\(2^n\\). In general, the cardinality of \\(2^X\\) is always strictly greater than the cardinality of \\(X\\), that is, while there obviously exists an injective function \\(X \\ni x \\mapsto \\{x\\} \\in 2^X\\), the reverse is not true. The set of real numbers of any interval of real numbers has a cardinality that is striclty larger than \\(\\mathbb{N}\\). However, \\(\\mathbb{Z}\\) is obviously enumerable by the injective function \\(f\\colon \\mathbb{Z}\\to \\mathbb{N}\\) such that \\(n \\mapsto f(n)=2n\\) if \\(n\\) is positive and \\(n \\mapsto f(n) = -2n -1\\) if \\(n\\) is strictly negative. Classsical but less obvious is that \\(\\mathbb{Q}\\) has the same cardinality as \\(\\mathbb{N}\\).</p> <p>Proposition</p> <p>Let \\((X_n)\\) be a countable family of countable sets, then \\(\\cup X_n\\) is countable.</p> <p>In particular \\(\\mathbb{Q}\\) is countable.</p> Proof <p>Define \\(X = \\cup X_n\\). Without loss of generality, we can assume that the family is enumerable, that is \\((X_n) = (X_n)_{n=1, \\ldots}\\). Furthermore, up to redefining sequentially \\(Y_1 = X_1\\) and \\(Y_{n+1} = X_{n+1} \\setminus Y{n}\\) for which holds \\(\\cup Y_n = X\\), we can also assume that the \\((X_n)\\) are pairewize disjoint. Up to slight modification, we can also assume that each \\(X_n\\) is itself enumerable. Since each set is enumerable, we can write uniquely \\(X_n =\\{ x_{n,m}: m \\in \\mathbb{N}\\}\\) such that \\(X = \\{x_{n, m}\\colon n \\in \\mathbb{N} \\text{ and } m \\in \\mathbb{N}\\}\\) which has the same cardinality as \\(\\mathbb{N}\\times \\mathbb{N}\\). However \\(\\mathbb{N}\\times \\mathbb{N}\\) has the same cardinality as \\(\\mathbb{N}\\). Indeed, it is equal to the countable union of the disjoint finite sets \\(Y_k =\\{(m, n)\\colon m+n = k\\}\\) each being of cardinality \\(k\\) from which an injection can be built into \\(\\mathbb{N}\\).</p> <p>As for \\(\\mathbb{Q}\\), it is the countable union of the countable sets \\(X_n = \\{m / n\\colon m \\in \\mathbb{Z} \\}\\).</p> <p>Given a sequence \\((X_n)\\) of sets we define the \\(\\limsup\\) and \\(\\liminf\\) of this sequence as the sets:</p> \\[ \\begin{align*}   \\limsup X_n &amp;:= \\cap_n \\cup_{k\\geq n} X_k &amp; \\liminf X_n &amp;:= \\cup_n \\cap_{k \\geq n} X_k \\end{align*} \\] <p>In other term, the \\(\\limsup\\) represent the set of those elements \\(x\\) which are contained in infinitely many \\(X_n\\) while the \\(\\liminf\\) represent the set of those elements \\(x\\) that are contained in all but finitely many \\(X_n\\). For \\(x\\) in \\(\\liminf X_n\\), it follows that there exists \\(n_0\\) such that \\(x\\) belongs to any \\(X_n\\) for \\(n\\geq n_0\\).  Hence \\(x\\) is in \\(\\cup_{k\\geq n} X_k\\) for any \\(n\\) showing that \\(x\\) is in \\(\\limsup X_n\\). Therefore \\(\\liminf X_n \\subseteq \\limsup X_n\\). The sequence of sets converges if \\(\\liminf X_n = \\limsup X_n\\).</p> <p>Definition</p> <p>Given a subset \\(A\\) of \\(X\\), we define the indicator function of \\(A\\) as the function</p> \\[ \\begin{equation*}   \\begin{split}     1_A \\colon X &amp; \\longrightarrow \\mathbb{R}\\\\                x &amp; \\longmapsto 1_A(x) =                      \\begin{cases}                       1 &amp; \\text{if }x \\in A\\\\                       0 &amp; \\text{otherwize }                     \\end{cases}   \\end{split} \\end{equation*} \\] <p> </p> <p>Exercise</p> <ul> <li>Show that \\((0,1)\\) has the same cardinality as \\((0, 1]\\).</li> <li>Let \\(f\\colon \\mathbb{R} \\to \\mathbb{R}\\) be an increasing function.   Show that the set \\(X = \\{x \\in \\mathbb{R}\\colon \\lim_{y\\nearrow x} f(y) &lt; \\lim_{y\\searrow x} f(y)\\}\\) of discountinuity of \\(f\\) is countable.</li> <li>Show that \\((\\limsup X_n^c)^c = \\liminf X_n\\), \\(\\liminf X_n = \\{x \\in X\\colon \\liminf 1_{A_n}(x) = 1\\}\\) and \\(\\limsup X_n = \\{x \\in X \\colon \\limsup 1_{A_n}(x) = 1\\}\\).</li> <li>Show that \\(1_{\\cup A_n} = \\sum 1_{A_n}\\) whenever \\((A_n)\\) are disjoints.   Show that \\(1_{\\cap A_n} = \\prod 1_{A_n}\\).</li> </ul> <ol> <li> <p>Thomas Jech. Set Theory \u2013 The Third Millennium Edition, revised and expanded. Springer-Verlag Berlin Heidelberg, 2003.\u00a0\u21a9</p> </li> </ol>"},{"location":"lecture/01-Sets-Functions/012-measurability-topology/","title":"Measurability, Topology","text":""},{"location":"lecture/01-Sets-Functions/012-measurability-topology/#measured-spaces","title":"Measured Spaces","text":"<p>Given a set \\(X\\), we want to describe the class of sets that we intend to measure with the help of measure.</p> <p>Definition: \\(\\sigma\\)-Algebra</p> <p>A collection \\(\\mathcal{F}\\) of subsets of \\(X\\) is called a \\(\\sigma\\)-algebra if</p> <ol> <li>\\(\\emptyset\\) is in \\(\\mathcal{F}\\);</li> <li>for any \\(A\\) in \\(\\mathcal{A}\\) it follows that \\(A^c\\) (stable under complementation);</li> <li>for any countable family \\((A_n)\\) of elements of \\(\\mathcal{F}\\) it follows that \\(\\cap A_n\\) is in \\(\\mathcal{F}\\) (stable under countable intersection).</li> </ol> <p>Elements of \\(\\mathcal{F}\\) are usually refered to as measurable sets (or events in probability theory). A tuple \\((X, \\mathcal{F})\\) where \\(\\mathcal{F}\\) is a \\(\\sigma\\)-algebra is called a measurable space.</p> <p>By complementation stability and \\(\\emptyset\\) in \\(\\mathcal{F}\\), it follows that \\(X\\) belongs to \\(\\mathcal{F}\\). Note that due to De Morgan's law, it follows that a \\(\\sigma\\)-algebra is stable under countable union. Eventually, the third property can be replaced by stability under countable union.</p> <p>The most simple example of \\(\\sigma\\)-algebra are </p> <ul> <li>the trivial \\(\\sigma\\)-algebra: \\(\\mathcal{F} = \\{\\emptyset, X\\}\\)</li> <li>the power set: \\(\\mathcal{F} = 2^X\\)</li> </ul> <p>These coincide to the smallest and largest possible \\(\\sigma\\)-algebra, respectively, on \\(X\\). Indeed for any \\(\\sigma\\)-algebra \\(\\mathcal{F}\\) it holds that \\(\\{\\emptyset, X\\} \\subseteq \\mathcal{F}\\subseteq 2^X\\). Even if \\(\\sigma\\)-algebra are conceptually simple objects, it is quite difficult to describe them in a meaningful way from simple components. This is in particular relevant when we want to define measure from simple events and extend it to the whole \\(\\sigma\\)-algebra. Therefore we introduce simpler class of sets that will serve as building blocks for \\(\\sigma\\)-algebra.</p> <p>Definition: Semi-Ring, Ring, Algebra, \\(\\pi\\)-system, \\(\\lambda\\)-system, Monotone Class</p> <p>A collection \\(\\mathcal{S}\\) of subsets of \\(X\\) is called a semi-ring if</p> <ul> <li>\\(\\emptyset\\) is in \\(\\mathcal{S}\\);</li> <li>for any \\(A\\) and \\(B\\) in \\(\\mathcal{S}\\) it follows that \\(A\\cap B\\) is in \\(\\mathcal{S}\\);</li> <li>for any \\(A\\) and \\(B\\) in \\(\\mathcal{S}\\), there exists disjoints \\(C_1, \\ldots, C_n\\) in \\(\\mathcal{S}\\) such \\(A\\setminus B = \\cup C_n\\).</li> </ul> <p>A collection \\(\\mathcal{R}\\) of subsets of \\(X\\) is called a ring if</p> <ul> <li>for any \\(A\\) and \\(B\\) in \\(\\mathcal{R}\\) it follows that \\(A\\cap B\\) is in \\(\\mathcal{R}\\);</li> <li>for any \\(A\\) and \\(B\\) in \\(\\mathcal{R}\\) if follows that \\(A\\Delta B\\) is in \\(\\mathcal{R}\\).</li> </ul> <p>A collection \\(\\mathcal{A}\\) of subsets of \\(X\\) is called an algebra if</p> <ul> <li>\\(\\emptyset\\) is in \\(\\mathcal{A}\\);</li> <li>for any \\(A\\) in \\(\\mathcal{A}\\) it follows that \\(A^c\\) is in \\(\\mathcal{A}\\); </li> <li>for any \\(A\\) and \\(B\\) in \\(\\mathcal{A}\\) it follows that \\(A\\cap B\\) is in \\(\\mathcal{A}\\).</li> </ul> <p>A collection \\(\\mathcal{P}\\) of subsets of \\(X\\) is called a \\(\\pi\\)-system if</p> <ol> <li>for any \\(A\\) and \\(B\\) in \\(\\mathcal{A}\\) it follows that \\(A\\cap B\\) is in \\(\\mathcal{P}\\)</li> </ol> <p>A collection \\(\\mathcal{L}\\) of subsets of \\(X\\) is called a \\(\\lambda\\)-system if</p> <ul> <li>\\(\\emptyset\\) is in \\(\\mathcal{L}\\)</li> <li>for any \\(A\\) in \\(\\mathcal{L}\\) it follows that \\(A^c\\) is in \\(\\mathcal{L}\\);</li> <li>for any disjoint countable family \\((A_n)\\) in \\(\\mathcal{L}\\) it follows that \\(\\cup A_n\\) is in \\(\\mathcal{L}\\).</li> </ul> <p>Clearly a \\(\\sigma\\)-algebra is an algebra. Furthermore, for a ring \\(\\mathcal{R}\\) it follows that \\(\\emptyset = A\\Delta A\\) belongs to \\(\\mathcal{R}\\) and from \\(A\\cup B = (A\\Delta B)\\Delta (A\\cap B)\\) it is stable under intersection. Hence if \\(\\mathcal{R}\\) contains \\(X\\), it is automatically an algebra. Reciprocally, an algebra is in particular a ring. Finally, a ring is in particular a semi-ring.</p> <p>Exercise</p> <ul> <li> <p>Show that the collection \\(\\mathcal{S}\\) of intervals of the form \\((a, b]\\) in \\(\\mathbb{R}\\) is a semi-ring.   Analogously, it also holds that the collection of products \\((a_1, b_1]\\times \\ldots \\times (a_n, b_n]\\) is a semi-ring in \\(\\mathbb{R}^n\\).</p> </li> <li> <p>Let \\(X = \\prod_{n \\in \\mathbb{N}} \\{-1, 1\\} = \\{-1, 1\\}^{\\mathbb{N}}\\) which is the set of sequences with values \\(\\pm 1\\).     We call sets of the form</p> \\[     C = C_1 \\times C_2 \\times \\ldots C_n \\times \\{-1, 1\\}\\times \\{-1, 1\\}\\times \\ldots \\] <p>where \\(n\\) is an integer and \\(C_k \\subseteq \\{-1, 1\\}\\) for \\(k=1, \\ldots, n\\) a finite cylinder.</p> <p>Show that the collection \\(\\mathcal{S}\\) of finite cylinders is a semi-ring.</p> </li> </ul> <p>Semi-rings are relatively easy to define from small building blocks. The reason why the other class are defined is that by explicit set operations, it is possible to go from one class to the next.</p> <p>Let us formulate first the notion of class generated by. In the following, we call a class \\(\\mathcal{Z}\\) a \\(z\\)-class where \\(z\\) is either a ring, algebra, \\(\\pi\\)-system, \\(\\lambda\\)-system, monotone class, \\(\\sigma\\)-algebra (any of them EXCEPT semi-ring).</p> <p>Proposition</p> <p>Let \\((\\mathcal{Z}_i)\\) be any arbitrary family of \\(z\\)-classes on \\(X\\), then it follows that \\(\\mathcal{Z} = \\cap \\mathcal{Z}_i\\) is a \\(z\\)-class.</p> <p>For any class \\(\\mathcal{C}\\) of subsets of \\(X\\), there exists a smallest \\(z\\)-class denoted by \\(z(\\mathcal{C})\\) that contains \\(\\mathcal{C}\\).(1)</p> <ol> <li>Smallest in terms of the inclusion, that is, if \\(\\mathcal{Z}\\) is a \\(z\\)-class containing \\(\\mathcal{C}\\), then it holds that \\(z(\\mathcal{C})\\subseteq \\mathcal{Z}\\).</li> </ol> <p>Proof</p> <p>We provide the proof for the case where \\(z\\) stands for ring, the other classes follows the same argumentation. Let \\(A\\) and \\(B\\) be elements of \\(\\cap \\mathcal{Z}_i\\), since \\(\\mathcal{Z}_i\\) is a ring for any \\(i\\), it follows that \\(A\\cap B\\) and \\(A\\Delta B\\) are in \\(\\mathcal{Z}_i\\) for any \\(i\\), thus in \\(\\cap \\mathcal{Z}_i\\) showing that it is a ring.</p> <p>Let now \\(\\mathcal{C}\\) be a class of subsets of \\(X\\) and consider the family \\((\\mathcal{Z}_i)\\) of all \\(z\\)-class on \\(X\\) that contains \\(\\mathcal{C}\\). This family is not empty as \\(2^X\\) belongs to the family. Taking the intersection per definition defines the smallest \\(z\\)-class containing \\(\\mathcal{C}\\).</p> <p>Semi-rings are not part of those results, however it is simple to describe the smallest ring generated by a semi-ring.</p> <p></p> <p>Proposition</p> <p>The ring \\(r(\\mathcal{S})\\) generated by a semi-ring \\(\\mathcal{S}\\) is precisely the sets of all finite unions \\(\\cup_{k=1}^n A_k\\) of disjoint elements \\(A_1, \\ldots, A_n\\) in \\(\\mathcal{S}\\).</p> <p>Proof</p> <p>Let \\(\\mathcal{R}\\) be the collection of finite unions of disjoint elements in \\(\\mathcal{S}\\). By definition, the ring generated by \\(\\mathcal{S}\\) contains finite unions of disjoint elements of \\(\\mathcal{S}\\), and therefore \\(\\mathcal{R}\\) is contained in the ring generated by \\(\\mathcal{S}\\). We are left to show that \\(\\mathcal{R}\\) is itself a ring.</p> <p>Let \\(A=\\cup_{1\\leq k\\leq n} A_k\\) and \\(B=\\cup_{1\\leq l\\leq m}B_l\\) be two elements of \\(\\mathcal{R}\\), where \\((A_k)_{1\\leq k\\leq n}\\) and \\((B_l)_{1\\leq l\\leq m}\\) are two finite families of pairwise disjoint sets in \\(\\mathcal{S}\\). By the distributivity of \\(\\cap\\) over \\(\\cup\\), it follows that</p> \\[ A\\cap B= \\cup_{1\\leq k\\leq n,1\\leq l\\leq m }A_k\\cap B_l. \\] <p>Since \\(\\mathcal{S}\\) is stable under intersection, it follows that \\(A\\cap B\\) is a finite union of pairwise disjoint elements \\((C_{kl})=(A_k\\cap B_l)\\) in \\(\\mathcal{S}\\), showing that \\(\\mathcal{R}\\) is stable under intersection.</p> <p>Let us now show that \\(A\\Delta B=(A\\setminus B)\\cup (B\\setminus A)\\) is in \\(\\mathcal{R}\\). It is enough to show that \\(A\\setminus B\\) is in \\(\\mathcal{R}\\). It holds</p> \\[ \\begin{equation} A\\setminus B=\\bigcap_{l=1}^m (A\\setminus B_l). \\end{equation} \\] <p>Since \\(\\mathcal{R}\\) is stable under intersection, it is enough to show that \\(A\\setminus C\\) is in \\(\\mathcal{R}\\) for every \\(C\\) in \\(\\mathcal{S}\\). It holds</p> \\[ \\begin{equation} A\\setminus C=\\bigcup_{k=1}^n \\left(A_k\\setminus C\\right). \\end{equation} \\] <p>Since \\(A_k\\) and \\(C\\) are elements of the semi-ring \\(\\mathcal{S}\\), it follows that</p> \\[ \\begin{equation} A_k\\setminus C=\\bigcup_{i=1}^{n_{k}} D_{ki} \\end{equation} \\] <p>for each \\(1\\leq k\\leq n\\), where \\((D_{ki})_{1\\leq i\\leq n_{k}}\\) is a finite disjoint family of elements in \\(\\mathcal{S}\\). Hence,</p> \\[ \\begin{equation} A\\setminus C =\\bigcup_{k=1}^n\\bigcup_{i=1}^{n_k} D_{ki} \\end{equation} \\] <p>is a finite union of pairwise disjoint elements in \\(\\mathcal{S}\\), which completes the proof.</p> <p>The following theorems are similar and show how to go up the ladder from simple classes (for instance rings) to \\(\\sigma\\)-algebra.</p> <p>Dynkin's \\(\\pi\\)-\\(\\lambda\\) Theorem</p> <p>Let \\(X\\) be a set and \\(\\mathcal{P}\\) be a \\(\\pi\\)-system. Then, the \\(\\lambda\\)-system generated by \\(\\mathcal{P}\\) is a \\(\\sigma\\)-algebra, that is \\(\\lambda(\\mathcal{P})=\\sigma(\\mathcal{P})\\).</p> Proof <p>We first show that if \\(\\mathcal{C}\\) is a \\(\\lambda\\)-system closed under finite intersection, then it is a \\(\\sigma\\)-algebra. By definition of a \\(\\lambda\\)-system, we just have to check the stability under arbitrary countable union. To this end, let \\((A_n)\\) be a sequence of elements in \\(\\mathcal{C}\\) and define</p> \\[ B_n=A_n\\setminus \\left(\\cup_{k&lt;n} A_k\\right)=A_n\\cap\\left(\\cap_{k&lt;n}A_k^c\\right), \\quad n&gt;1, \\quad B_1=A_1. \\] <p>As \\(\\mathcal{C}\\) is closed under complementation and we assumed that \\(\\mathcal{C}\\) is closed under finite intersection, it follows that \\((B_n)\\) is a sequence of elements in \\(\\mathcal{C}\\). From \\(\\cup B_n=\\cup A_n\\) and \\((B_n)\\) pairwise disjoint, it follows from the \\(\\lambda\\)-system assumption on \\(\\mathcal{C}\\) that \\(\\cup A_n=\\cup B_n\\) is in \\(\\mathcal{C}\\).</p> <p>Now, it clearly holds \\(\\lambda(\\mathcal{P})\\subseteq \\sigma(\\mathcal{P})\\). From what we just showed, we just have to check that \\(\\lambda(\\mathcal{P})\\) is closed under finite intersection, since then \\(\\lambda(\\mathcal{P})\\) would be a \\(\\sigma\\)-algebra containing \\(\\mathcal{P}\\) and so \\(\\sigma(\\mathcal{P})\\subseteq \\lambda(\\mathcal{P})\\). For \\(D \\in \\lambda(\\mathcal{P})\\), define</p> \\[ \\mathcal{D}_D=\\{A\\subseteq X\\colon A\\cap D \\in \\lambda(\\mathcal{P})\\} \\] <p>which is a \\(\\lambda\\)-system. Indeed, \\(\\emptyset\\in \\mathcal{D}_D\\). If \\(A \\in \\mathcal{D}_D\\), it follows that</p> \\[ A^c\\cap D=(A^c\\cup D^c)\\cap D=(A\\cap D)^c\\cap D=\\left( (A\\cap D)\\cup D^c\\right)^c. \\] <p>By assumption, \\(A\\cap D \\in \\lambda(\\mathcal{P})\\), and since \\(\\lambda(\\mathcal{P})\\) is stable under complementation and countable intersection of disjoint elements, it follows that \\(A^c\\cap D\\in \\lambda (\\mathcal{P})\\) and therefore \\(A^c \\in \\mathcal{D}_D\\). Let now \\((A_n)\\) be a sequence of pairwise disjoint elements in \\(\\mathcal{D}_D\\). From the stability of \\(\\lambda(\\mathcal{P})\\) under countable union of pairwise disjoint elements and the fact that \\((\\cup A_n)\\cap D=\\cup (A_n\\cap D)\\) it follows that \\(\\cup A_n \\in \\mathcal{D}_D\\). Hence, \\(\\mathcal{D}_D\\) is indeed a \\(\\lambda\\)-system.</p> <p>Since \\(\\mathcal{P}\\) is stable under finite intersection, it follows that \\(\\mathcal{P}\\subseteq \\mathcal{D}_B\\) for every \\(B\\) in \\(\\mathcal{P}\\). Hence \\(\\lambda(\\mathcal{P})\\subseteq \\mathcal{D}_B\\) for every \\(B\\) in \\(\\mathcal{P}\\). In particular, for every \\(A \\in \\lambda (\\mathcal{P})\\) and \\(B \\in \\mathcal{P}\\), it holds \\(A\\cap B\\) is in \\(\\lambda(\\mathcal{P})\\subseteq \\mathcal{D}_B\\). By definition, this also means that \\(B\\) is in \\(\\mathcal{D}_A\\) for every \\(B\\) in \\(\\mathcal{P}\\) and \\(A\\) in \\(\\lambda(\\mathcal{P})\\), showing that \\(\\mathcal{P}\\subseteq \\mathcal{D}_A\\) for every \\(A\\) in \\(\\lambda(\\mathcal{P})\\). Hence, \\(\\lambda(\\mathcal{P})\\subseteq \\mathcal{D}_A\\) for every \\(A \\in \\lambda(\\mathcal{P})\\). Thus, for \\(A,B \\in \\lambda(\\mathcal{P})\\) it holds \\(B\\in \\mathcal{D}_A\\), which by definition means \\(A\\cap B\\) is in \\(\\lambda(\\mathcal{P})\\), showing that \\(\\lambda(\\mathcal{P})\\) is closed under finite intersection and therefore, by the first step of the proof, a \\(\\sigma\\)-algebra.</p>"},{"location":"lecture/01-Sets-Functions/012-measurability-topology/#topological-spaces","title":"Topological Spaces","text":"<p>Similar to \\(\\sigma\\)-algebra that describes measurable sets, a topology is a description of the open sets of a set \\(X\\).</p> <p>Definition: Topology</p> <p>A collection \\(\\mathfrak{T}\\) of subsets of a set \\(X\\) is called a topology if:</p> <ul> <li>\\(\\emptyset\\) and \\(X\\) are in \\(\\mathfrak{T}\\).</li> <li>\\(\\mathfrak{T}\\) is closed under finite intersection.</li> <li>\\(\\mathfrak{T}\\) is closed under arbitrary union.</li> </ul> <p>A tuple \\((X, \\mathfrak{T})\\) is called a topological space.</p> <p>An element \\(O\\) in \\(\\mathcal{T}\\) is called an open set and the complement \\(F=O^c\\) of an open set \\(O\\) is called a closed set.</p> <p>A neighborhood of an element \\(x\\) in \\(X\\) is any subset \\(U\\subseteq X\\) such that \\(x \\in O \\subseteq U\\) for some open set \\(O\\).</p> <p>A topology is stable under arbitrary union and finite intersection but not under complementation. Like \\(\\sigma\\)-algebras, arbitrary intersection of topologies remains a topology which allows to define topologies generated by class of sets. Obviously, a topology can equivalently be defined by its collection of closed set that must contain \\(\\emptyset\\) and \\(X\\), be closed under finite union as well as arbitrary union.</p> <p>Proposition</p> <p>Let \\((\\mathfrak{T}_i)\\) be any arbitrary family of topologies on \\(X\\), then it follows that \\(\\mathfrak{T} = \\cap \\mathfrak{T}_i\\) is a topology.</p> <p>For any class \\(\\mathcal{B}\\) of subsets of \\(X\\), there exists a smallest topology denoted by \\(\\mathfrak{T}(\\mathcal{B})\\) that contains \\(\\mathcal{B}\\).</p> <p>Proof</p> <p>The proof follows exactly the same argumentation as for the \\(\\sigma\\)-algebra.</p> <p>Definition</p> <p>A collection \\(\\mathfrak{B}\\) of subsets of a set \\(X\\) is called a topological base if:</p> <ul> <li>\\(\\cup\\{O\\colon O\\in \\mathfrak{B}\\}=X\\).</li> <li>For every \\(x \\in O_1\\cap O_2\\) for \\(O_1,O_2 \\in \\mathfrak{B}\\), there exists \\(O_3 \\in \\mathfrak{B}\\) with \\(x \\in O_3\\) and such that \\(O_3\\subseteq O_1\\cap O_2\\).</li> </ul> <p>Just like the relation between a semi-ring and a ring, it holds:</p> <p>Proposition</p> <p>Let \\(\\mathfrak{B}\\) be a topological base, and \\(\\mathfrak{t}(\\mathfrak{B})\\) be the topology generated by \\(\\mathfrak{B}\\). Then, \\(\\mathfrak{t}(\\mathfrak{B})\\) is exactly the collection of arbitrary unions of elements in \\(\\mathfrak{B}\\).</p> Proof <p>Denote by \\(\\mathcal{U}(\\mathfrak{B})\\) the collection of arbitrary unions of elements in \\(\\mathfrak{B}\\). By definition of \\(\\mathfrak{t}(\\mathfrak{B})\\), it follows that:</p> \\[ \\mathfrak{B}\\subseteq \\mathcal{U}(\\mathfrak{B})\\subseteq \\mathfrak{t}(\\mathfrak{B}). \\] <p>Since \\(\\mathfrak{t}(\\mathfrak{B})\\) is the smallest topology containing \\(\\mathfrak{B}\\), we just have to show that \\(\\mathcal{U}(\\mathfrak{B})\\) is a topology itself. First, \\(X\\) belongs to \\(\\mathcal{U}(\\mathfrak{B})\\) due to the first assumption of a topological base. Since any union over an empty family is empty, it follows that \\(\\emptyset\\) is in \\(\\mathcal{U}(\\mathfrak{B})\\). By definition, \\(\\mathcal{U}(\\mathfrak{B})\\) is stable under arbitrary union. We are left to show that \\(\\mathcal{U}(\\mathfrak{B})\\) is stable under intersection. Let \\(\\tilde{O}_1=\\cup O_i,\\tilde{O}_2=\\cup O_j \\in \\mathcal{U}(\\mathfrak{B})\\) for families \\((O_i),(O_j)\\) of elements in \\(\\mathfrak{B}\\). It follows that:</p> \\[ \\begin{equation} \\tilde{O}_1\\cap\\tilde{O}_2=\\cup_{i,j} (O_i\\cap O_j). \\end{equation} \\] <p>By definition of a topological base, for every \\(i,j\\) and every \\(x\\) in \\(O_i\\cap O_j\\), there exists \\(O_{i,j}^x\\) in \\(\\mathfrak{B}\\) such that \\(x \\in O_{i,j}^x\\subseteq O_i\\cap O_j\\). Hence,</p> \\[ \\begin{equation} \\cup_{x \\in O_i\\cap O_j}O_{i,j}^x=O_i\\cap O_j. \\end{equation} \\] <p>From this, it follows that:</p> \\[ \\begin{equation} \\tilde{O}_1\\cap\\tilde{O}_2=\\cup_{i,j, x \\in O_i\\cap O_j} O_{i,j}^x \\in \\mathcal{U}(\\mathfrak{B}), \\end{equation} \\] <p>showing that \\(\\mathcal{U}(\\mathfrak{B})\\) is a topology.</p> <p>For a subset \\(A\\) of \\(X\\), we define the interior and closure of \\(A\\):</p> \\[ \\begin{align} \\text{Int}(A)&amp;=\\cup\\{O\\colon O\\text{ open with }O\\subseteq A\\}, &amp; \\text{Cl}(A)&amp;=\\cap\\{F\\colon F\\text{ closed and }A\\subseteq F\\}. \\end{align} \\] <p>Clearly, \\(A\\) is open or closed if, and only if, \\(A=\\text{Int}(A)\\) or \\(A=\\text{Cl}(A)\\), respectively. Furthermore, by De Morgan's law, it holds that \\((\\text{Int}(A^c))^c = \\text{Cl}(A)\\) while \\((\\text{Cl}(A^c))^c = \\text{Int}(A)\\).</p> <p>Let us succintly mention several definition related to specific topologies.</p> <ul> <li>Dense: A subset \\(Y\\) of \\(X\\) is called dense in \\(X\\) if \\(\\text{Cl}(Y) =X\\).</li> <li>Hausdorf: A topological space \\(X\\) is called Hausdorf if for any two distinct elements \\(x\\neq y\\) there exists open sets \\(O_x \\ni x\\) and \\(O_y \\ni y\\) with \\(O_x \\cap O_y = \\emptyset\\).</li> <li>Separable: A topological space \\(X\\) is called separable if there exists a countable dense subset \\(Y\\).     That is, there exists a sequence \\((x_n)\\) of elements in \\(X\\) such that any open set \\(O\\) contains at least one element of the sequence.</li> <li>First Countable: A topological space \\(X\\) is called first countable if for each element \\(x\\) there exists a countable family of open sets \\((O_n^x)\\) such that any neighborhood \\(U\\) of \\(x\\) there exists \\(n_0\\) with \\(x \\in O_{n_0}^x \\subseteq U\\).</li> <li>Second Countable: A topological space \\(X\\) is called second countable if it cna be generated by a countable topological basis.   Clearly, any second countable topological space is in particular first countable.</li> </ul> <p>Towards our goal, a very important example of topological space are metric space.</p> <p>Definition: Metric Space</p> <p>A tuple \\((X, d)\\) where \\(d\\colon X \\times X \\to [0, \\infty)\\) is called a metric space if the function \\(d\\) satisfies:</p> <ul> <li>\\(d(x, y) = 0\\) if and only if \\(x = y\\);</li> <li>Symetrie: \\(d(x, y) = d(y,x)\\) for any \\(x\\) and \\(y\\) in \\(X\\);</li> <li>Triangular Inequality: \\(d(x, z) \\leq d(x, y) + d(y, z)\\) for any \\(x\\), \\(y\\) and \\(z\\) in \\(X\\).</li> </ul> <p>Given a metric space, \\(x\\) in \\(X\\) and \\(\\varepsilon&gt;0\\), we define the (open) ball \\(B_{\\varepsilon}(x)\\) centered in \\(x\\) and of radius \\(\\varepsilon\\) as</p> \\[   B_{\\varepsilon}(x) = \\{y \\in X \\colon d(x, y)&lt;\\varepsilon\\} \\] <p>A sequence \\((x_n)\\) of elements in \\(X\\) is said to converge to \\(x\\) in \\(X\\) if \\(d(x_n , x)\\to 0\\).</p> <p>Due to the triangular inequality, it holds that the set \\(\\mathcal{B}\\) of all open balls is a topological basis and the resulting topology is called the metric topology. Furthermore, any metric space is Hausdorf. Indeed, for any two \\(x\\neq y\\), taking \\(\\epsilon = d(x,y)&gt;0\\) due to the first property, from the triangular inequality it follows that that \\(B_{\\varepsilon/3}(x)\\cap B_{\\varepsilon/3}(y) = \\emptyset\\). Clearly, any element \\(x\\) admits a countable neighborhood basis(1) from the balls \\(B_{1/n}(x)\\) for all integers \\(n\\). In other terms, any metric space is first countable. Finally, if \\(Y = (x_n)\\) is a countable dense subset of \\(X\\), then the countable family \\(\\mathfrak{B} = (B_{1/m}(x_n))_{m,n}\\) is a topological base. We summarize these properties into one proposition that also show that the topology of a metric space can be characterized by sequences.</p> <ol> <li>A neighborhood basis of an element \\(x\\) is a family of of neighborhoods \\(\\mathcal{V}(x)\\) of \\(x\\) such that for any neighborhood \\(U\\) of \\(x\\), there exists \\(V \\in \\mathcal{V}(x)\\) such that \\(V \\subseteq U\\).</li> </ol> <p>Proposition</p> <p>Any metric space is Hausdorf, first countable. Furthermore, if it is separable, then it it is second countable. Finally, a set \\(F\\) is closed if and only if for any converging sequence \\((x_n)\\) of elements of \\(F\\), the limit belongs to \\(F\\).</p> <p>Proof</p> <p>We are just left to show the equivalence of closedness with closedeness under sequential limits.</p> <p>Suppose that \\(F\\) is closed and let \\((x_n)\\) be a sequence of elements in \\(F\\) converging to \\(x\\). by contradiction, assume that \\(x\\) belongs to \\(F^c = O\\) which is open, in particular a neighborhood of \\(x\\). However, since \\(x_n \\to x\\), it follows that fall all \\(n\\) large enough, \\(x_n\\) is in \\(O = F^c\\) which is a contradiction with \\((x_n)\\subseteq F\\). Hence, if \\(F\\) is closed, any limit of converging sequence of elements in \\(F\\) belongs to \\(F\\).</p> <p>Reciprocally, suppose that \\(F\\) is stable under sequential limits of its elements. Let \\(x\\) be an element of \\(\\textrm{Cl}(F)\\). By definition, it follows that \\(B_{1/n}(x) \\cap F \\neq \\emptyset\\) for every \\(n\\). For each \\(n\\) select \\(x_n\\) in this intersection defining a sequence \\((x_n)\\) which converges to \\(x\\). By assumption \\(x\\) is therefore in \\(F\\) showing that \\(\\textrm{Cl}(F) \\subseteq F\\) which ends the proof.</p> <p>Example</p> <p>The real line is a metric space for the metric \\(d(x,y) = |x-y|\\). More generaly any norm defines a metric, therefore all the \\(p\\)-norms on \\(\\mathbb{R}^d\\) for instance.</p> <p>A sequence \\((x_n)\\) in a metric space is called Cauchy if for \\(\\varepsilon&gt;0\\), it holds that \\(d(x_n, x_m)\\leq \\varepsilon\\) for all \\(n\\) and \\(m\\) large enough. Clearly, due to the triangular inequality every converging sequence is Cauchy, the reciprocal is however not true, a classical example of which being \\(\\mathbb{Q}\\). We therefore call a metric space on which any Cauchy sequence is also converging a complete metric space.</p> <p>From a topology, it is possible to consider the \\(\\sigma\\)-algebra generated by the topological base</p> <p>Definition: Borel \\(\\sigma\\)-algebra</p> <p>Let \\((X,\\mathfrak{T})\\) be a topological space. The \\(\\sigma\\)-algebra generated by the open sets of \\(X\\) is called the Borel \\(\\sigma\\)-algebra and usually denoted by \\(\\mathcal{B}(X)\\).</p> <p>Exercise</p> <p>Show that the Borel \\(\\sigma\\)-algebra of \\(\\mathbb{R}\\) satisfies</p> \\[ \\begin{equation} \\mathcal{B}(\\mathbb{R})=\\sigma(\\mathcal{C}_k). \\end{equation} \\] <p>for \\(k=1,\\ldots,17\\), where:</p> \\[ \\begin{align}     \\mathcal{C}_1 &amp; =\\left\\{F\\colon F\\text{ closed subset of }\\mathbb{R}\\right\\}, \\\\     \\mathcal{C}_2 &amp; =\\left\\{]a,b[\\colon a\\leq b \\text{ with } a,b\\in \\mathbb{R}\\right\\}, &amp; \\mathcal{C}_{10} &amp; =\\left\\{[a,b]\\colon a\\leq b \\text{ with } a,b\\in \\mathbb{R}\\right\\}, \\\\     \\mathcal{C}_3 &amp; =\\left\\{]a,b]\\colon a\\leq b \\text{ with } a,b\\in \\mathbb{R}\\right\\}, &amp; \\mathcal{C}_{11} &amp; =\\left\\{[a,b[\\colon a\\leq b \\text{ with } a,b\\in \\mathbb{R}\\right\\}, \\\\     \\mathcal{C}_4 &amp; =\\left\\{]-\\infty,b]\\colon b\\in \\mathbb{R}\\right\\},                   &amp; \\mathcal{C}_{12} &amp; =\\left\\{]-\\infty,b[\\colon b\\in \\mathbb{R}\\right\\}, \\\\     \\mathcal{C}_5 &amp; =\\left\\{[a,\\infty[\\colon a\\in \\mathbb{R}\\right\\},                    &amp; \\mathcal{C}_{13} &amp; =\\left\\{]a,\\infty[\\colon a\\in \\mathbb{R}\\right\\}, \\\\     \\mathcal{C}_6 &amp; =\\left\\{]a,b[\\colon a\\leq b \\text{ with } a,b\\in \\mathbb{Q}\\right\\}, &amp; \\mathcal{C}_{14} &amp; =\\left\\{[a,b]\\colon a\\leq b \\text{ with } a,b\\in \\mathbb{Q}\\right\\}, \\\\     \\mathcal{C}_7 &amp; =\\left\\{]a,b]\\colon a\\leq b \\text{ with } a,b\\in \\mathbb{Q}\\right\\}, &amp; \\mathcal{C}_{15} &amp; =\\left\\{[a,b[\\colon a\\leq b \\text{ with } a,b\\in \\mathbb{Q}\\right\\}, \\\\     \\mathcal{C}_8 &amp; =\\left\\{]-\\infty,b]\\colon b\\in \\mathbb{Q}\\right\\},                   &amp; \\mathcal{C}_{16} &amp; =\\left\\{]-\\infty,b[\\colon b\\in \\mathbb{Q}\\right\\}, \\\\     \\mathcal{C}_9 &amp; =\\left\\{[a,\\infty[\\colon a\\in \\mathbb{Q}\\right\\},                    &amp; \\mathcal{C}_{17} &amp; =\\left\\{]a,\\infty[\\colon a\\in \\mathbb{Q}\\right\\}. \\end{align} \\]"},{"location":"lecture/01-Sets-Functions/013-measurable-continuous-functions/","title":"Measurable/Continuous Functions, Initial-Product \\(\\sigma\\)-Algebra/Topology","text":"<p>The following subsection deals with the consequence of the properties of preimages. Generically, it works as follows. Let \\(f\\colon X\\to Y\\) be a function and \\(\\mathcal{C}\\) be a collection of subsets of \\(Y\\) that is stable under intersection, union, or complement. Then, by Pre-image Proposition, the collection \\(\\{f^{-1}(B)\\colon B\\in \\mathcal{C}\\}\\) inherits the same stability properties. This is why measurability, as well as continuity, is defined in terms of preimages.</p> <p>For ease of notation, given a collection \\(\\mathcal{C}\\) of subsets of \\(Y\\), we use the notation</p> \\[ \\begin{equation}     f^{-1}(\\mathcal{C})=\\{f^{-1}(B)\\colon B\\in \\mathcal{C}\\}. \\end{equation} \\] <p>Also, in probability theory, we use the following shorthand notation</p> \\[ \\begin{equation}     \\left\\{ f \\in A \\right\\}:=f^{-1}(A)=\\left\\{ x\\in X\\colon f(x)\\in A \\right\\}. \\end{equation} \\] <p>If \\(Y=\\mathbb{R}\\), we also use the notations</p> \\[ \\begin{equation}     \\left\\{ f\\leq t \\right\\}:=f^{-1}\\left( (-\\infty,t] \\right), \\quad \\left\\{ s &lt; f \\leq t \\right\\}:=f^{-1}\\left((s,t]\\right), \\quad \\text{etc.} \\end{equation} \\] <p>Definition: Measurability/Continuity</p> <p>Let \\((X,\\mathcal{F})\\) and \\((Y, \\mathcal{G})\\) be two measurable spaces. We say that a function \\(f:X\\to Y\\) is \\(\\mathcal{F}\\)-\\(\\mathcal{G}\\)-measurable if</p> \\[ \\begin{equation}     f^{-1}(B) \\in \\mathcal{F}, \\quad\\text{for every measurable set }B \\in \\mathcal{G}. \\end{equation} \\] <p>In other words, if and only if \\(f^{-1}(\\mathcal{G})\\subseteq \\mathcal{F}\\).</p> <p>Let \\((X,\\mathfrak{S})\\) and \\((Y,\\mathfrak{T})\\) be two topological spaces. We say that a function \\(f:X\\to Y\\) is \\(\\mathfrak{S}\\)-\\(\\mathfrak{T}\\)-continuous if</p> \\[ \\begin{equation}     f^{-1}(O) \\in \\mathfrak{S}, \\quad\\text{for every open set } O\\in \\mathfrak{T}. \\end{equation} \\] <p>In other words, if and only if \\(f^{-1}(\\mathfrak{T})\\subseteq \\mathfrak{S}\\).</p> <p>Lemma</p> <p>The composition of measurable/continuous functions is measurable/continuous, respectively.</p> Proof <p>We only prove that the composition of measurable functions is measurable, the continuous case being similar. Let \\((X,\\mathcal{F})\\), \\((Y,\\mathcal{G})\\), and \\((Z,\\mathcal{H})\\) be three measurable spaces and \\(f\\colon X\\to Y\\) and \\(g\\colon Y\\to Z\\) be two measurable functions and denote \\(h=g\\circ f\\). For every \\(C\\in \\mathcal{H}\\), it holds that</p> \\[ \\begin{equation}   h^{-1}(C)=f^{-1}(g^{-1}(C))=f^{-1}(B) \\quad \\text{where } B=g^{-1}(C). \\end{equation} \\] <p>Since \\(g\\) is measurable, it follows that \\(B=g^{-1}(C)\\in \\mathcal{G}\\). Further, the measurability of \\(f\\) implies that \\(h^{-1}(C)=f^{-1}(B)\\in \\mathcal{F}\\) showing that \\(h\\) is measurable.</p> <p>Lemma</p> <p>Let \\(f \\colon X\\to Y\\) be a function.  </p> <ul> <li>If \\((Y,\\mathcal{G})\\) is a measurable space, then \\(f^{-1}(\\mathcal{G})=\\{f^{-1}(B)\\colon B \\in \\mathcal{G}\\}\\) is a \\(\\sigma\\)-algebra called the \\(\\sigma\\)-algebra generated by \\(f\\) and denoted by \\(\\sigma(f)\\).  </li> <li>If \\((Y,\\mathfrak{T})\\) is a topological space, then \\(f^{-1}(\\mathfrak{T})=\\{f^{-1}(O)\\colon O\\in \\mathfrak{T}\\}\\) is a topology called the topology generated by \\(f\\) and denoted by \\(\\mathfrak{t}(f)\\).  </li> </ul> Proof <p>The proof is a straightforward application of Pre-image Proposition.</p> <p>If \\(f\\colon X\\to Y\\) is an \\(\\mathcal{F}\\)-\\(\\mathcal{G}\\)-measurable function where \\(\\mathcal{F}\\) is a \\(\\sigma\\)-algebra on \\(X\\), then it follows that \\(\\sigma(f)\\subseteq \\mathcal{F}\\). In other words, \\(\\sigma(f)\\) is the smallest \\(\\sigma\\)-algebra for which \\(f\\) is measurable. The same remark holds for continuous functions: \\(\\mathfrak{t}(f)\\) is the smallest topology for which \\(f\\) is continuous. More generally, let \\((f_i)\\) be a family of functions \\(f_i\\colon X \\to Y_i\\) where \\((Y_i,\\mathcal{G}_i)\\) is a family of measurable spaces. Then,</p> \\[ \\begin{equation}     \\sigma(f_i\\colon i)=\\sigma(\\{f_i^{-1}(B)\\colon B\\in \\mathcal{G}_i, i\\}) \\end{equation} \\] <p>is the smallest \\(\\sigma\\)-algebra such that each \\(f_i\\) is measurable. Similarly, when replacing \\(\\sigma\\)-algebra with topologies, the same holds. These are called the initial \\(\\sigma\\)-algebra or topology.</p> <p>Definition: Product \\(\\sigma\\)-algebra</p> <p>Let \\((X_i,\\mathcal{F}_i)\\) be a non-empty family of measurable spaces. The product \\(\\sigma\\)-algebra, denoted by \\(\\otimes \\mathcal{F}_i\\) on the product state space \\(X =\\prod X_i\\), is defined as the \\(\\sigma\\)-algebra generated by the family of projections:</p> \\[ \\begin{align}     \\pi_i\\colon X &amp;\\longrightarrow X_i, \\\\     x=(x_i)&amp;\\longmapsto x_i. \\end{align} \\] <p>That is,</p> \\[ \\begin{equation}     \\otimes \\mathcal{F}_i =\\sigma\\left(\\left\\{ \\pi^{-1}_i(A_i)\\colon A_i\\in \\mathcal{F}_i, \\text{ for any }i \\right\\}\\right). \\end{equation} \\] <p>Exercice</p> <p>In two dimensions, let \\((X,\\mathcal{F})\\) and \\((Y,\\mathcal{G})\\) be two measurable spaces, and define the projections:</p> \\[ \\begin{align}     \\pi_1 \\colon  X\\times Y &amp;\\longrightarrow X, &amp; \\pi_2\\colon  X\\times Y &amp; \\longrightarrow Y, \\\\     (x,y)&amp;\\longmapsto \\pi_1((x,y))=x, &amp; (x,y)&amp;\\longmapsto \\pi_2((x,y))=y. \\end{align} \\] <p>In particular, for \\(A\\) in \\(\\mathcal{F}\\) and \\(B\\) in \\(\\mathcal{G}\\), one has:</p> \\[ \\begin{equation}     \\begin{split}         \\pi^{-1}_1(A)&amp;=\\left\\{ (x,y)\\colon \\pi_1((x,y))=x \\in A \\right\\}=A\\times Y, \\\\         \\pi^{-1}_2(B)&amp;=\\left\\{ (x,y)\\colon \\pi_2((x,y))=y \\in B \\right\\}=X\\times B.     \\end{split} \\end{equation} \\] <p>It follows that:</p> \\[ \\begin{equation}     \\mathcal{F}\\otimes \\mathcal{G}=\\sigma\\left(\\left\\{ A\\times Y, X\\times B\\colon A \\in \\mathcal{F}, B \\in \\mathcal{G} \\right\\}\\right). \\end{equation} \\] <p>As an exercise, show that:</p> \\[ \\begin{equation}     \\mathcal{F}\\otimes \\mathcal{G}=\\sigma\\left(\\left\\{A\\times B\\colon A\\in \\mathcal{F}, B\\in \\mathcal{G}\\right\\}\\right). \\end{equation} \\] <p>Topologies and \\(\\sigma\\)-algebras are often complex abstract systems of sets. In practical terms, checking continuity or measurability for a function might, at first sight, seem like an impossible task. However, topologies and \\(\\sigma\\)-algebras are often described in terms of a simple collection of sets from which they are generated. The morphism properties of pre-images allows us to restrict the verification of continuity or measurability to this simpler generating set.</p> <p>Proposition</p> <p>Let \\(f:X\\to Y\\) be a function and \\(\\mathcal{C}\\), \\(\\mathfrak{C}\\) be collections of subsets of \\(Y\\). It follows that</p> \\[ \\begin{equation}     \\sigma(f)=f^{-1}\\left(\\sigma(\\mathcal{C})\\right)=\\sigma\\left( f^{-1}(\\mathcal{C})\\right)=\\sigma\\left(\\left\\{ f^{-1}(B)\\colon B \\in \\mathcal{C} \\right\\} \\right). \\end{equation} \\] <p>and</p> \\[ \\begin{equation}     \\mathfrak{t}(f)=f^{-1}\\left(\\mathfrak{t}(\\mathfrak{C})\\right)=\\mathfrak{t}\\left( f^{-1}(\\mathfrak{C})\\right)=\\mathfrak{t}\\left(\\left\\{ f^{-1}(B)\\colon B \\in \\mathfrak{C} \\right\\} \\right). \\end{equation} \\] <p>Proof</p> <p>Left as an exercise.</p> <p>Remark</p> <p>The use of this proposition is often as follows. Let \\(f:X\\to Y\\) be a function where \\((X,\\mathcal{F})\\) and \\((Y,\\mathcal{G})\\) are measurable spaces. Suppose that \\(\\mathcal{G}=\\sigma(\\mathcal{C})\\) for some collection \\(\\mathcal{C}\\) of subsets of \\(Y\\). It follows that \\(f\\) is measurable if and only if</p> \\[ \\begin{equation}     f^{-1}(B)\\in \\mathcal{F}, \\quad \\text{ for every }B\\in \\mathcal{C}. \\end{equation} \\] <p>In particular, if \\(Y=\\mathbb{R}\\) and \\(\\mathcal{G}=\\mathcal{B}(\\mathbb{R})\\) is the Borel \\(\\sigma\\)-algebra of \\(\\mathbb{R}\\), then \\(f\\) is measurable if and only if  </p> \\[ \\begin{equation}     \\{f\\leq t\\} \\in \\mathcal{F} \\end{equation} \\] <p>for every \\(t \\in \\mathbb{R}\\).</p> <p>Corollary</p> <p>Let \\((X,\\mathcal{F})\\) be a measurable space and \\((Y,\\mathfrak{T})\\) be a topological space. A function \\(f \\colon X \\to Y\\) is measurable with respect to the \\(\\sigma\\)-algebra if \\(f^{-1}(O)\\) is measurable for any open set \\(O\\).</p> <p>Furthermore, if the topology is generated by a countable basis \\(\\mathfrak{B}\\), we can consider only those \\(O\\) in the topological basis.</p> <p>In particular if \\(X\\) and \\(Y\\) are topological spaces endowed with their respective \\(\\sigma\\)-algebra, if \\(f\\) is continuous then it is measurable.</p> <p>Finally, if \\(Y = \\mathbb{R}\\) and \\(f\\) is lower/upper semi-continuous if follows that \\(f\\) is measurable.</p> <p>Proof</p> <p>The first assertion is a direct consequence of the previous results, the seond one follows from the fact that every open set \\(O\\) can be written as a countable union of elements in \\(\\mathfrak{B}\\).</p> <p>As for the third one, it follows from \\(f^{-1}(O)\\) is open henceforth measurable for any open set \\(O\\).</p> <p>As for the last one it follows from \\(f^{-1}(]-\\infty, t])\\) is closed for any \\(t\\) henceforth measurable.</p> <p>Proposition</p> <p>Let \\(f,g, f_n\\colon X\\to \\mathbb{R}\\) be measurable functions where \\((X,\\mathcal{F})\\) is a measurable space. It holds that:</p> <ul> <li>\\(af+bg\\) is measurable for every \\(a,b \\in \\mathbb{R}\\).</li> <li>\\(fg\\) is measurable.</li> <li>\\(\\max(f,g)\\) and \\(\\min(f,g)\\) are measurable.</li> <li> <p>\\(\\sup f_n\\) and \\(\\inf g_n\\) are extended real-valued measurable functions.(1)</p> <ol> <li>With respect to the Borel \\(\\sigma\\)-algebra on \\([-\\infty,\\infty]\\) generated by the metric \\(d(x,y)=|\\arctan(x)-\\arctan(y)|\\), which coincides with the Euclidean topology on \\(\\mathbb{R}\\).</li> </ol> </li> <li> <p>\\(\\liminf f_n:=\\inf_n\\sup_{k\\geq n}f_k\\) and \\(\\limsup f_n:=\\inf_n\\sup_{k\\geq n}f_k\\) are extended real-valued measurable functions.</p> </li> <li>\\(A:=\\{\\lim f_n \\text{ exists}\\}:=\\{\\omega \\colon \\lim f_n(\\omega)\\text{ exists}\\}=\\{\\liminf f_n=\\limsup f_n\\}\\) is measurable.</li> </ul> <p>Proof</p> <p>First, let \\(\\varphi:\\mathbb{R}\\times \\mathbb{R}\\to \\mathbb{R}\\) be a continuous function. It follows that the function \\(\\varphi(f,g)\\) is measurable for the following reason. First, the mapping \\(T:x \\to \\mathbb{R}\\times \\mathbb{R}\\), \\(x \\mapsto (f(x),g(x))\\) is measurable with respect to the product Borel \\(\\sigma\\)-algebra on \\(\\mathbb{R}\\times \\mathbb{R}\\). Indeed, for every two Borel sets \\(A, B\\) of the real line, it follows that</p> \\[ T^{-1}(A\\times B)=\\{f\\in A\\}\\cap\\{g\\in B\\} \\] <p>which is an element in \\(\\mathcal{F}\\) by the measurability of \\(f\\) and \\(g\\). Since the product sets \\(A\\times B\\) for \\(A,B\\) Borel sets in \\(\\mathbb{R}\\) generate the Borel product \\(\\sigma\\)-algebra on \\(\\mathbb{R}^2\\), it follows that \\(T\\) is measurable. By continuity of \\(\\varphi\\), it follows that \\(\\varphi\\) is measurable, and therefore \\(\\varphi\\circ T\\) is measurable. Taking \\(\\varphi(x,y)=ax+by\\), \\(\\varphi(x,y)=xy\\), \\(\\varphi(x,y)=\\max(x,y)\\), and \\(\\varphi(x,y)=\\min(x,y)\\), the first three points follow.</p> <p>Let \\(a \\in \\mathbb{R}\\). It holds that</p> \\[ \\{\\sup_n f_n \\leq a\\}=\\{f_n\\leq a\\colon \\text{ for every }n\\}=\\cap_n \\{f_n\\leq a\\} \\] <p>which is measurable since \\(\\{f_n\\leq a\\}\\) is measurable. Since \\((-\\infty,a]\\) generates the Borel \\(\\sigma\\)-algebra, it follows that \\(\\sup_n f_n\\) is measurable. The same argument applies to \\(\\inf f_n\\) using \\(\\{\\inf f_n\\geq a\\}\\).</p> <p>The measurability of \\(\\liminf f_n\\) follows by the same argument using countable intersection and union. The last point follows directly.</p>"},{"location":"lecture/02-Measure/020-introduction/","title":"Introduction","text":"<p>This Chapter introduce the notion of measure. From the definition on a \\(\\sigma\\)-algebra it goes forward to measure defined on simpler class of sets such as \"Lebesgue/Stieljes\" measure. How to extend those measures to larger class of sets such as ring. It finishes with Carath\u00e9odory's extension theorem that allows to extend those meaningfull measures to the \\(\\sigma\\)-algebra.</p> <ul> <li>Measures</li> <li>From Semi-Ring to Ring</li> <li>From Ring to \\(\\sigma\\)-Alegebra: Carath\u00e9odory's Extension Theorem</li> </ul>"},{"location":"lecture/02-Measure/021-measure/","title":"Measure","text":"<p>In the previous chapter, we described the class of sets that can be measures. This present chapter deals with how to measure these sets in a consistent way. Let us first discuss the most natural example, which is the Lebesgue measure on the real line.</p> <p>This function, historically denoted by \\(\\lambda\\), should provide the \"measure\" of subsets of the real line. It should have the following properties: the measure of an interval should be equal to its length. In other terms,</p> \\[ \\begin{equation}     \\lambda\\left[ (a,b] \\right]=b-a \\end{equation} \\] <p>for \\(a&lt;b\\) reals.</p> <p>It should also have an additive property, meaning that the length of disjoint intervals should equal their sum,</p> \\[ \\begin{equation}     \\lambda\\left[ (a_1,b_1]\\cup(a_2,b_2] \\right]=(b_1-a_1)+(b_2-a_2)=\\lambda\\left[ (a_1,b_1] \\right]+\\lambda\\left[ (a_2,b_2] \\right]. \\end{equation} \\] <p>Finally, if a subset \\(A=\\cup (a_n,b_n]\\) is a countable union of disjoint intervals \\((a_n,b_n]\\), then its measure should be equal to the limit of the total length of all intervals, that is,</p> \\[ \\begin{equation}     \\lambda\\left[ A \\right]=\\lambda\\left[ \\cup (a_n,b_n] \\right]=\\sum (b_n-a_n)=\\sum \\lambda\\left[ (a_n,b_n] \\right]. \\end{equation} \\] <p>This raises the following questions:</p> <ul> <li>First, on which subclass of sets of the real line should \\(\\lambda\\) be defined?</li> <li>Second, which continuity property should this set function satisfy?</li> </ul> <p>We saw that the collection of intervals of the form \\((a,b]\\) forms a semi-ring on which the Lebesgue measure can be defined easily. As for the additivity property of the measure, we then have to extend it to the ring it generates. Finally, as for the continuity property, we have to extend it to the \\(\\sigma\\)-algebra it generates since it is stable under finite union.``</p> <p>Throughout, given a class of sets \\(\\mathcal{C}\\) containing the emptyset and a set function \\(\\mu \\colon \\mathcal{C} \\to \\mathbb{R}\\cup\\{\\pm \\infty\\}\\) we say that \\(\\mu\\) is</p> <ul> <li>positive: if \\(\\mu[A]\\geq 0\\) for every \\(A\\);</li> <li>additive: if for any two disjoint sets \\(A\\) and \\(B\\) in \\(\\mathcal{C}\\) with \\(A\\cup B\\) in \\(\\mathcal{C}\\) it holds \\(\\mu[A\\cup B] = \\mu[A] + \\mu[B]\\);</li> <li> <p>sub-additive: if for any two sets \\(A\\) and \\(B\\) in \\(\\mathcal{C}\\) it holds \\(\\mu[A\\cup B] \\leq \\mu[A] + \\mu[B]\\);</p> </li> <li> <p>\\(\\sigma\\)-additive: if for any countable family of pairwize disjoint sets \\((A_n)\\) in \\(\\mathcal{C}\\) such that \\(\\cup A_n\\) is in \\(\\mathcal{C}\\), it holds \\(\\mu [\\cup A_n] = \\sum \\mu[A_n]\\);</p> </li> <li>\\(\\sigma\\)-sub-additive: if for any countable family of sets \\((A_n)\\) in \\(\\mathcal{C}\\) such that \\(\\cup A_n\\) is in \\(\\mathcal{C}\\), it holds \\(\\mu [\\cup A_n] \\leq  \\sum \\mu[A_n]\\);</li> </ul> <p>We say that a set function \\(\\tilde{\\mu}\\) defined on a \\(\\tilde{\\mathcal{C}}\\supseteq \\mathcal{C}\\) extends \\(\\mu\\) is they coincide on \\(\\mathcal{C}\\).</p> <p>Measure</p> <p>Let \\(\\mathcal{F}\\) be a \\(\\sigma\\)-algebra. A set function \\(\\mu \\colon \\mathcal{F} \\to [0, \\infty]\\) is called a measure if</p> <ul> <li>\\(\\mu[\\emptyset] = 0\\);</li> <li> <p>\\(\\sigma\\)-Additivity: for any countable family \\((A_n)\\) of pairwize disjoint sets in \\(\\mathcal{F}\\), it holds</p> \\[     \\mu\\left[ \\cup A_n \\right] = \\sum \\mu[A_n] \\] </li> </ul> <p>A triple \\((X, \\mathcal{F}, \\mu)\\) where \\(\\mathcal{F}\\) is a \\(\\sigma\\)-algebra on \\(X\\) and \\(\\mu\\) is a measure on \\(\\mathcal{F}\\) is called a measured space.</p> <p>Let \\(\\mathcal{S}\\) be a semi-ring. A set function \\(\\mu \\colon \\mathcal{S}\\to [0, \\infty]\\) is called a pre-measure if</p> <ul> <li>\\(\\mu[\\emptyset] = 0\\);</li> <li> <p>Additivity: for any two dijoints sets \\(A\\) and \\(B\\) in \\(\\mathcal{S}\\)  such that \\(A \\cup B\\) is in \\(\\mathcal{S}\\) it holds</p> \\[     \\mu\\left[ A \\cup B \\right] = \\mu[A] + \\mu[B] \\] </li> </ul> <p>In other terms, a measure on a \\(\\sigma\\)-algebra is a positive set function equal to \\(0\\) on the emptyset and \\(\\sigma\\)-additive, while a pre-measure on a ring is just with additivity.</p> <p>We say that a measure \\(\\mu\\) is </p> <ul> <li>finite if \\(\\mu[X]&lt;\\infty\\)</li> <li>\\(\\sigma\\)-finite: if \\(\\mu[A_n]&lt;\\infty\\) for some increasing sequence \\((A_n)\\) with \\(\\cup A_n = X\\).</li> </ul> <p>Remark</p> <p>This lecture is essentially about stochastics and we will therefore mainly consider finite measures \\(\\mu\\) normalized to \\(\\mu[X] = 1\\). Such a measure is called a probability measure.</p> <p>Example: Probability on a Countable set</p> <p>Suppose that \\(X=\\{x_1, \\ldots, x_N\\}\\) is a finite set with \\(\\sigma\\)-algebra \\(\\mathcal{F} = 2^X\\). Since any subset \\(A\\) of \\(X\\) can be written as a finite union of disjoint singletons</p> \\[     A = \\cup_{i \\in I}x_i \\] <p>for some \\(I\\subseteq \\{1, \\ldots, N\\}\\), from \\(\\sigma\\)-additivity, it holds</p> \\[     \\mu[A] = \\sum_{i \\in I}\\mu[\\{x_i\\}] \\] <p>the measure is equivalently given by a positive function \\(m\\colon X\\to [0, \\infty]\\) with \\(m(x_i) = \\mu[\\{x_i\\}]\\).</p> <p>Due to \\(\\sigma\\)-additivity, the same argumentation holds for \\(X = (x_n)\\) being a countable set where measures on \\(\\mathcal{F} = 2^X\\) are entirely described by functions \\(m\\colon X \\to [0, \\infty]\\) through \\(m(x_n) = \\mu[\\{x_n\\}]\\).</p> <p>Example: Dirac Measure</p> <p>Let \\((X, \\mathcal{F})\\) be a measurable space and \\(x_0\\) be an element of \\(X\\). The dirac measure \\(\\delta_{x_0}\\colon \\mathcal{F} \\to [0, 1]\\) is defined as</p> \\[    \\begin{equation}        \\delta_{x_0}[A]=        \\begin{cases}            1 &amp; \\text{if }x_0\\text{ is in } A\\\\            0 &amp; \\text{otherwise}        \\end{cases}    \\end{equation} \\] <p>for any \\(A\\) in \\(\\mathcal{F}\\). Straightforward inspection shows that it is a probability measure.</p> <p>Example: Counting Pre-Measure</p> <p>Let \\(\\mathcal{S}\\) be a semi-ring on \\(X\\). Define \\(\\mu\\colon \\mathcal{F}\\to [0, \\infty]\\) as</p> \\[    \\begin{equation}        \\mu[A]=        \\begin{cases}            \\# A &amp;\\text{if }A\\text{ is finite}\\\\            \\infty &amp; \\text{otherwise}        \\end{cases}, \\quad A \\in \\mathcal{F}.    \\end{equation} \\] <p>It is easy to check that \\(\\mu\\) is a pre-content, which is \\(\\sigma\\)-additive if and only if \\(A\\) is finite.</p> <p>Example: Normal Distribution</p> <p>For \\(X=\\mathbb{R}\\) and \\(\\mathcal{F} = \\mathcal{B}(\\mathbb{R})\\) the Borel \\(\\sigma\\)-algebra we define  </p> \\[    \\begin{equation}        \\mu[A]=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int_A e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\lambda(dx), \\quad A \\in \\mathcal{F},    \\end{equation} \\] <p>where \\(\\lambda\\) is the Lebesgue measure on \\(\\mathbb{R}\\) (we do not yet know if such an object exists).</p> <p>This is the famous normal distribution, which is a probability measure on the real line.</p> <p>Another way to generate measures is to use measurable functions and by the pre-image properties can also transport forward (pushforward) measures through measurable functions</p> <p>Definition: Pullback Measure</p> <p>Let f\\colon X \\to Y$ be a measurable function for the \\(\\sigma\\)-algebra \\(\\mathcal{F}\\) and \\(\\mathcal{G}\\) on \\(X\\) and \\(Y\\), respectively.</p> <p>Given a measure \\(\\mu\\) on \\(\\mathcal{F}\\), we can define the pushforward measure \\(\\mu_{\\#f} \\colon \\mathcal{G} \\to [0, \\infty]\\) as</p> \\[     \\mu_{\\# f}[B] = \\mu \\left[ f \\in B \\right] = \\mu\\left[ \\left\\{ x \\in X \\colon f(x) \\in B \\right\\} \\right]  \\] <p>for any \\(B\\) in \\(\\mathcal{G}\\).</p> <p>Easy inspection shows that \\(\\mu_{\\# f}\\) is indeed a measure on \\(\\mathcal{G}\\).</p>"},{"location":"lecture/02-Measure/022-semi-ring-to-ring/","title":"From Semi-Ring to Ring","text":"<p>The definition of measure is well and fine, but beyond the obvious countable sets, we want to be able to construct measure on more complex sets such as the Lebesgue measure on the Borel \\(\\sigma\\)-algebra. In other terms if such objects do really make sense.</p> <p>In view of the Lebesgue's measure, let us consider the slightly more general concept of Stieljes integral. Consider the real line \\(X = \\mathbb{R}\\) with the semi-ring \\(\\mathcal{S}\\) of half open intervals \\((a, b]\\) for \\(a\\leq b\\). Given any increasing function \\(F\\colon \\mathbb{R} \\to \\mathbb{R}\\), increasing and right continuous, that is, \\(F(x) = \\lim_{y \\searrow x} F(y)\\). We define the set function \\(\\mu \\colon \\mathcal{S}\\to [0, \\infty]\\) given by</p> \\[ \\begin{equation*}  \\mu[(a, b] ] = F(b)-F(a) \\quad \\text{for any }a\\leq b \\end{equation*} \\] <p>It is direct to check that this indeed defines a pre-measure on \\(\\mathcal{S}\\). The case of the Lebesgue's measure is by taking \\(F(x) = x\\).</p> <p>This represents the basic requirement we want to have given \\(F\\) for a measure. The first step to carry is whether this set function can be extended uniquely to a set function on the ring generated by \\(\\mathcal{S}\\).</p> <p></p> <p>Proposition</p> <p>Let \\(\\mu\\) be a pre-measure on a semi-ring \\(\\mathcal{S}\\). There exists a unique pre-measure extension of \\(\\mu\\) on the ring \\(\\mathcal{R}=r(\\mathcal{S})\\) generated by \\(\\mathcal{S}\\).</p> <p>Proof</p> <p>We know from the proposition on the relationship between semi-ring and rings that every element of \\(\\mathcal{R}\\) can be written as a finite union of disjoint elements of \\(\\mathcal{S}\\). Let \\(A=\\cup_{1\\leq k\\leq n}A_k\\) for a finite disjoint family \\((A_k)_{1\\leq k\\leq n}\\) and define</p> \\[ \\begin{equation}     \\nu\\left[ A \\right]=\\sum_{1\\leq k\\leq n} \\mu\\left[ A_k \\right]. \\end{equation} \\] <p>This definition does not depend on the choice of the disjoint family. Indeed, suppose \\(A=\\cup_{1\\leq l\\leq m}B_j\\) for another finite disjoint family \\((B_l)_{1\\leq l\\leq m}\\) of elements of \\(\\mathcal{S}\\). Since \\(A_k=\\cup_{1\\leq l\\leq m}A_k\\cap B_l\\) as well as \\(B_l=\\cup_{1\\leq k\\leq n}A_k\\cap B_l\\) are both disjoint unions of elements in \\(\\mathcal{S}\\) for every \\(k\\) and \\(l\\), it holds</p> \\[ \\begin{align}     \\nu\\left[ A \\right] &amp;= \\sum_{1\\leq k\\leq n}\\mu\\left[A_k\\right]\\\\     &amp;=\\sum_{1\\leq k\\leq n}\\sum_{1\\leq l\\leq m}\\mu\\left[ A_k\\cap B_l \\right]\\\\     &amp;=\\sum_{1\\leq l\\leq m}\\sum_{1\\leq k\\leq n} \\mu\\left[ A_k\\cap B_l \\right]\\\\     &amp;=\\sum_{1\\leq l\\leq m}\\mu\\left[ B_l \\right]. \\end{align} \\] <p>This shows that \\(\\nu\\) is well defined on \\(\\mathcal{R}\\). By definition, it clearly holds that \\(\\nu\\) is an extension of \\(\\mu\\). Let us show that it is a content. Clearly, \\(\\nu[\\emptyset]=\\mu[\\emptyset]=0\\). For \\(A=\\cup_{1\\leq k\\leq n}A_k\\), a finite disjoint union of elements in \\(\\mathcal{R}\\), we can write each \\(A_k=\\cup_{1\\leq l\\leq m_k}B_{lk}\\) as a finite union of elements in \\(\\mathcal{S}\\). It follows that \\(A=\\cup_{1\\leq k}\\cup_{1\\leq l\\leq m_l}B_{kl}\\) is a finite disjoint union of elements in \\(\\mathcal{S}\\). Hence</p> \\[ \\begin{equation}     \\nu\\left[ A \\right]=\\sum_{1\\leq k\\leq n}\\sum_{1\\leq l\\leq m_l}\\mu\\left[ B_{kl} \\right]=\\sum_{q\\leq k\\leq n}\\nu\\left[\\cup_{1\\leq l\\leq m_k}B_{kl}  \\right]=\\sum_{1\\leq k\\leq n}\\nu\\left[ A_k \\right]. \\end{equation} \\] <p>This proves finite additivity.</p> <p>As for the uniqueness of the extension as a content, it is immediate from the finite additivity and the fact that every element in \\(\\mathcal{R}\\) can be written as a disjoint union of finitely many elements in \\(\\mathcal{S}\\).  </p> <p>Before handling the extension under the additional assumption that \\(\\mu\\) is \\(\\sigma\\)-additive, let us treat some properties of contents.</p> <p></p> <p>Lemma</p> <p>Let \\(\\mu\\) be a pre-measure on a semi-ring \\(\\mathcal{S}\\). Then it holds:</p> <ul> <li>(i) \\(\\mu\\) is monotone, that is \\(\\mu[A]\\leq \\mu[B]\\) for every \\(A\\subseteq B\\) with \\(A,B\\) in \\(\\mathcal{S}\\).</li> <li>(ii) \\(\\mu\\) is sub-additive.</li> <li>(iii) \\(\\mu\\) is \\(\\sigma\\)-additive if and only if \\(\\mu\\) is \\(\\sigma\\)-sub-additive.</li> </ul> <p>Let \\(\\mu\\) be a pre-measure on a ring \\(\\mathcal{R}\\). Then it holds:</p> <ul> <li>(a) \\(\\mu[B\\setminus A]=\\mu[B]-\\mu[A]\\) for every \\(A\\subseteq B\\) with \\(A,B\\) in \\(\\mathcal{R}\\).</li> <li>(b) \\(\\mu[A\\cup B]+\\mu[A\\cap B]=\\mu[A]+\\mu[B]\\) for every \\(A,B\\) in \\(\\mathcal{R}\\).</li> <li>(c) \\(\\sum \\mu[A_n]\\leq \\mu[A]\\) for every sequence \\((A_n)\\) of mutually disjoint sets in \\(\\mathcal{R}\\) and \\(A\\in \\mathcal{R}\\) such that \\(\\cup A_n \\subseteq A\\).</li> </ul> Proof <p>(i) Let \\(A\\subseteq B\\) with \\(A\\) and \\(B\\) in \\(\\mathcal{S}\\). It follows that \\(B\\setminus A=\\cup_{1\\leq k\\leq n}C_k\\) for a disjoint family \\((C_k)_{1\\leq k\\leq n}\\) in \\(\\mathcal{S}\\). Due to the additivity and positivity of \\(\\mu\\), it follows  </p> \\[ \\begin{align}     \\mu[B] &amp; =\\mu[A\\cup B\\setminus A]=\\mu[A\\cup (\\cup_{1\\leq k\\leq n}C_k)]\\\\     &amp; =\\mu[A]+\\sum_{1\\leq k\\leq n}\\mu[C_k]\\geq \\mu[A]. \\end{align} \\] <p>(ii) Let \\((A_k)_{1\\leq k\\leq n}\\) be a countable family of elements in \\(\\mathcal{S}\\) and \\(A\\in \\mathcal{S}\\) be such that \\(A\\subseteq \\cup_{k\\leq n}A_k\\). Define \\(B_1=A_1\\) and \\(B_k=A_k\\setminus (\\cup_{1\\leq l&lt;k}A_l)=\\cap_{1\\leq l&lt;k}(A_k\\setminus (A_k\\cap A_l))\\). By definition of a semi-ring, there exists \\((C_{kl})_{1\\leq l\\leq r_k}\\), a disjoint family in \\(\\mathcal{S}\\), such that \\(B_k=\\cup_{1\\leq l\\leq r_k} C_{kl}\\). Note also that \\(B_k\\subseteq A_k\\). Since \\(A_k\\setminus B_k=\\cap_{1\\leq l\\leq c_k}(A_k\\setminus (A_k\\cap C_{kl}))\\), a similar argument yields the existence of \\((D_{kj})_{1\\leq j\\leq p_k}\\), a disjoint family in \\(\\mathcal{R}\\), such that \\(A_k\\setminus B_k =\\cup_{1\\leq j\\leq p_k} D_{kj}\\). By additivity, we deduce that</p> \\[ \\begin{align}     \\mu[A_k] &amp;= \\mu[A_k\\setminus B_k \\cup B_k]\\\\     &amp; =\\mu[(\\cup_{1\\leq j\\leq p_k}D_{kj})\\cup (\\cup_{1\\leq l\\leq r_k}C_{kl})]\\\\     &amp;=\\sum_{1\\leq j\\leq p_k}\\mu[D_{kj}]+\\sum_{1\\leq l\\leq r_k}\\mu[C_{kl}]\\\\     &amp; \\geq \\sum_{1\\leq l\\leq r_k}\\mu[C_{kl}]. \\end{align} \\] <p>Using this inequality, the monotonicity and additivity of \\(\\mu\\), as well as the fact that  </p> \\[ A=A\\cap (\\cup_{1\\leq k\\leq n}A_k)=A\\cap(\\cup_{1\\leq k\\leq n}B_k)=A\\cap (\\cup_{k\\leq n}\\cup_{1\\leq l\\leq r_k}C_{kl})=\\cup_{1\\leq k\\leq n}\\cup_{1\\leq l\\leq r_k}(A\\cap C_{kl}), \\] <p>it follows that</p> \\[ \\begin{align}     \\mu[A] &amp;= \\mu\\left[\\cup_{1\\leq k\\leq n}\\cup_{1\\leq l\\leq r_k}(A\\cap C_{kl})  \\right]\\\\     &amp; = \\sum_{1\\leq k\\leq n}\\sum_{1\\leq l\\leq r_k}\\mu[A\\cap C_{kl}]\\\\     &amp; \\leq \\sum_{1\\leq k\\leq n}\\sum_{1\\leq l \\leq r_k}\\mu[C_{kl}]\\\\     &amp; \\leq \\sum_{1\\leq k\\leq n}\\mu[A_k], \\end{align} \\] <p>showing the sub-additivity.</p> <p>(iii) Suppose that \\(\\mu\\) is \\(\\sigma\\)-additive. The fact that \\(\\mu\\) is \\(\\sigma\\)-sub-additive follows from the same argumentation with a family \\((A_n)\\) instead of \\((A_k)_{1\\leq k\\leq n}\\). Suppose therefore that \\(\\mu\\) is a \\(\\sigma\\)-sub-additive content, and let us show that \\(\\mu\\) is \\(\\sigma\\)-additive. Let \\((A_n)\\) be a disjoint family in \\(\\mathcal{S}\\) such that \\(\\cup A_n \\in \\mathcal{S}\\). Denote by \\(\\nu\\) the unique extension to the ring \\(\\mathcal{R}\\) generated by \\(\\mathcal{S}\\). Since \\(\\nu=\\mu\\) on \\(\\mathcal{S}\\) and \\(\\nu\\) is additive as well as monotone, it follows that</p> \\[ \\begin{align}     \\sum \\mu[A_n] &amp; =\\sup_n \\sum_{1\\leq k\\leq n}\\mu[A_k]\\\\     &amp;=\\sup_n \\sum_{1\\leq k\\leq n}\\nu[A_k]\\\\     &amp;=\\sup_n \\nu[\\cup_{1\\leq k\\leq n}A_k]\\\\     &amp;\\leq\\sup_n \\nu[\\cup A_n]\\\\     &amp;=\\nu[A]=\\mu[A]. \\end{align} \\] <p>The \\(\\sigma\\)-sub-additivity yields the reverse equality, showing \\(\\sigma\\)-additivity.</p> <p>(a) Let \\(A\\subseteq B\\) for \\(A,B\\) in \\(\\mathcal{R}\\). It follows that \\(B\\setminus A \\in \\mathcal{R}\\). Hence, additivity yields \\(\\mu[B]=\\mu[A\\setminus B\\cup B]=\\mu[B\\setminus A]+\\mu[A]\\).</p> <p>(b) For \\(A,B\\) in \\(\\mathcal{R}\\), it holds \\(A\\cup B=A\\cup (B\\setminus A)\\) and \\(B=A\\cap B\\cup (B\\setminus A)\\). Since all elements of the decomposition are in \\(\\mathcal{R}\\), we get \\(\\mu[A\\cup B]=\\mu[A]+\\mu[B\\setminus A]\\) and \\(\\mu[B]=\\mu[A\\cap B]+\\mu[B\\setminus A]\\), from which follows the assertion.</p> <p>(c) Let \\((A_n)\\) be a pairwise disjoint sequence of elements in \\(\\mathcal{R}\\) and \\(A\\in \\mathcal{R}\\) such that \\(\\cup A_n \\subseteq A\\). By monotonicity, additivity, and stability of \\(\\mathcal{R}\\) under finite union, it holds \\(\\sum_{1\\leq k\\leq n}\\mu[A_k]=\\mu[\\cup_{1\\leq k\\leq n} A_k]\\leq \\mu[A]\\) for every \\(n\\). Hence the assertion.</p> <p>Extending a content from a semi-ring to a ring is fairly straightforward. However, for the sake of the next extension to a \\(\\sigma\\)-algebra, it may be interesting to see if the same result holds for a pre-measure.  </p> <p>Proposition</p> <p>Let \\(\\mu\\) be a pre-measure on a semi-ring \\(\\mathcal{S}\\). Suppose that \\(\\mu\\) is either \\(\\sigma\\)-additive or \\(\\sigma\\)-sub-additive. Then the unique extension of \\(\\mu\\) as a pre-measure \\(\\nu\\) on \\(\\mathcal{R}\\) -- the ring generated by \\(\\mathcal{S}\\) -- is also \\(\\sigma\\)-additive.</p> <p>Proof</p> <p>The fact that we can assume that \\(\\mu\\) is either \\(\\sigma\\)-additive or \\(\\sigma\\)-sub-additive follows from the equivalence between both properties due to the previous Lemma. Assume therefore that \\(\\mu\\) is \\(\\sigma\\)-additive and let us show that \\(\\nu\\) is so too. Let \\((A_n)\\) be a family of pairwise disjoint elements in \\(\\mathcal{R}\\) such that \\(A=\\cup A_n\\) is in \\(\\mathcal{R}\\). Denote by \\(A=\\cup_{1\\leq l\\leq m}B_l\\) and \\(A_n=\\cup_{1\\leq k\\leq r_n} B_{nk}\\) with \\((B_l)_{1\\leq l\\leq n}\\) and \\((B_{nk})_{1\\leq k\\leq r_n}\\) pairwise disjoint families in \\(\\mathcal{S}\\). Defining \\(C_{lnk}=B_l\\cap B_{nk}\\), it follows that</p> \\[ B_l=\\cup_{n}\\cup_{1\\leq k\\leq r_n}C_{lnk}, \\quad \\text{and} \\quad B_{nk}=\\cup_{1\\leq l\\leq m} C_{lnk} \\] <p>are disjoint decompositions into sets in \\(\\mathcal{S}\\). By additivity of \\(\\mu\\), it follows that \\(\\mu[B_{nk}]=\\sum_{1\\leq l\\leq m}\\mu[C_{lnk}]\\) and by \\(\\sigma\\)-additivity of \\(\\mu\\), also \\(\\mu[B_l]=\\sum_{n}\\sum_{1\\leq k\\leq r_n}\\mu[C_{lnk}]\\). Since we can switch infinite sums of positive numbers, it follows that</p> \\[ \\begin{align}     \\nu\\left[ A \\right] &amp; =\\sum_{1\\leq l\\leq m}\\mu[B_l]=\\sum_{1\\leq l\\leq m}\\sum_{n}\\sum_{1\\leq k\\leq r_n}\\mu[C_{lnk}]\\\\     &amp; =\\sum_{n}\\sum_{1\\leq k\\leq r_n}\\sum_{1\\leq l\\leq m}\\mu[C_{lnk}]\\\\     &amp;=\\sum_{n}\\sum_{1\\leq k\\leq r_n}\\mu[B_{nk}]=\\sum_n \\nu[A_n] \\end{align} \\] <p>showing the \\(\\sigma\\)-additivity.</p> <p>Example: Lebesgue/Stieljes Measure</p> <p>Let us study the example of the Lebesgue/Stieljes measure presented in the introduction of this section. Given a right continuous increasing function \\(F:\\mathbb{R}\\to \\mathbb{R}\\), on the semi-ring \\(\\mathcal{S}=\\{(a,b]\\colon a\\leq b, a,b \\in \\mathbb{R}\\}\\), the Lebesgue/Stieljes pre-measure is defined as the set function \\(\\mu[(a,b] ]=F(b)-F(a)\\). Let us show that \\(\\mu\\) is indeed a pre-measure.</p> <ul> <li>\\(\\mu[\\emptyset]=\\mu[(a,a]]=F(a)-F(a)=0\\).</li> <li> <p>Let \\((a_k,b_k]\\) be disjoint intervals for \\(1\\leq k\\leq n\\) such that \\(\\cup_{1\\leq k\\leq n} (a_k,b_k]=(a,b] \\in \\mathcal{S}\\).     Up to reordering, without loss of generality, we can assume that \\(a_1\\leq b_1\\leq a_{2}\\leq b_2\\leq \\ldots \\leq a_n\\leq b_n\\) since the intervals are disjoint.     Furthermore, since \\(\\cup_{1\\leq k\\leq n} (a_k,b_k]=(a,b]\\) is an interval itself, it follows that \\(a_k=b_{k+1}\\) for every \\(k=1,\\ldots, n-1\\) and \\(a=a_1\\) and \\(b=b_n\\).     Hence  </p> \\[   \\sum_{1\\leq k\\leq n}\\mu[(a_k,b_k]]=\\sum_{1\\leq k\\leq n}F(b_k)-F(a_k)=F(b_n)-F(a_1)=F(b)-F(a)=\\mu[(a,b]] \\] <p>showing the additivity.  </p> </li> </ul> <p>We can extend \\(\\mu\\) to the ring generated by \\(\\mathcal{S}\\) as shown in a previous proposition.</p> <p>Interestingly though, \\(\\mu\\) is \\(\\sigma\\)-additive on this ring. The reason for which is strongly related to compactness arguments. According to the previous proposition and lemma, it is enough to show that \\(\\mu\\) is \\(\\sigma\\)-sub-additive on the semi-ring \\(\\mathcal{S}\\). Let \\((a,b]\\in \\mathcal{S}\\) and \\(((a_n,b_n])\\) be a countable family in \\(\\mathcal{S}\\) such that \\((a,b]\\subseteq \\cup (a_n,b_n]\\).</p> <p>Let \\(\\varepsilon&gt;0\\), and by right continuity of \\(F\\) choose some \\(a^\\varepsilon\\) in \\((a,b)\\) such that \\(F(a^\\varepsilon)-F(a)&lt;\\varepsilon/2\\). Also, using right continuity of \\(F\\), choose \\(b^\\varepsilon_n&gt;b_n\\) for every \\(n\\) such that \\(F(b_n^\\varepsilon)-F(b_n)\\leq \\varepsilon 2^{-n-1}\\). It follows that</p> \\[   [a^\\varepsilon,b]\\subseteq (a^\\varepsilon,b]\\subseteq (a,b]\\subseteq \\cup (a_n,b_n]\\subseteq (a_n,b_n^\\varepsilon). \\] <p>However, \\([a^\\varepsilon,b]\\) is a bounded closed interval, and therefore compact. Hence, the open covering \\(\\cup (a_n,b_n^\\varepsilon)\\) of \\([a^\\varepsilon,b]\\) can be chosen finite, that is, there exists \\(n_0\\) such that</p> \\[   (a^\\varepsilon,b]\\subseteq [a^\\varepsilon,b]\\subseteq \\cup_{1\\leq k\\leq n_0}(a_k,b_k^\\varepsilon)\\subseteq \\cup_{k\\leq n_0}(a_k,b_k^\\varepsilon]. \\] <p>Hence, since \\(\\mu\\) is sub-additive by means of of the previous lemms, it follows that  </p> \\[ \\begin{align}   \\mu[(a,b]]  &amp; = F(b)-F(a)\\\\               &amp; \\leq \\varepsilon/2 + F(b)-F(a^\\varepsilon)\\\\               &amp; \\leq \\varepsilon/2 + \\sum_{k=1}^{n_0}\\left(F(b_k^\\varepsilon)-F(a_k)\\right)\\\\               &amp; \\leq \\varepsilon/2 + \\varepsilon \\sum_{k=1}^{n_0}2^{-k-1} +\\sum_{k=1}^{n_0} \\left(F(b_n)-F(a_n)\\right)\\\\               &amp; \\leq \\varepsilon +\\sum \\left(F(b_n)-F(a_n)\\right)\\\\               &amp; =\\varepsilon+\\sum \\mu[(a_n,b_n]] \\end{align} \\] <p>Since \\(\\varepsilon\\) is arbitrarily small, it follows that \\(\\mu\\) is \\(\\sigma\\)-sub-additive, hence \\(\\sigma\\)-additive on the semi-ring \\(\\mathcal{S}\\).</p> <p>From the previous proposition, \\(\\mu\\) extends uniquely to a \\(\\sigma\\)-additive content on the ring generated by \\(\\mathcal{S}\\).</p> <p>The main question now is whether it is possible to extend the Lebesgue or Lebesgue-Stieltjes content to a measure on the \\(\\sigma\\)-algebra \\(\\mathcal{B}(\\mathbb{R})\\) which is generated by \\(\\mathcal{S}\\).  </p>"},{"location":"lecture/02-Measure/023-ring-to-s-algebra/","title":"From Ring to \\(\\sigma\\)-Algebra: Carath\u00e9odory's Theorem","text":"<p>As mentioned in the previous section, the question is whether a \\(\\sigma\\)-additive pre-measure on a ring \\(\\mathcal{R}\\) can be extended to the \\(\\sigma\\)-algebra it generates. In case such an extension exists, a further relevant question is whether such an extension is unique.</p> <p>Before assessing the extension, we first address some central properties of the \\(\\sigma\\)-additivity in terms of continuity.</p> <p>Lemma</p> <p>Let \\(\\mathcal{R}\\) be a ring and \\(\\mu:\\mathcal{R}\\to [0,\\infty]\\) a finite content, that is \\(\\mu[A]&lt;\\infty\\) for every \\(A \\in \\mathcal{R}\\). Then the following properties of \\(\\mu\\) hold:</p> <ol> <li>\\(\\sigma\\)-additivity</li> <li>\\(\\sigma\\)-sub-additivity</li> <li>Lower semi-continuity: \\(\\sup_n \\mu[A_n]= \\mu[\\cup A_n]\\) for every countable family \\((A_n)\\) of increasing elements in \\(\\mathcal{R}\\) such that \\(\\cup A_n\\) is in \\(\\mathcal{R}\\).</li> <li>Upper semi-continuity: \\(\\inf_n \\mu[A_n]= \\mu[\\cap A_n]\\) for every countable family \\((A_n)\\) of decreasing elements in \\(\\mathcal{R}\\) such that \\(\\cap A_n\\) is in \\(\\mathcal{R}\\).</li> <li>Continuous at \\(\\emptyset\\): \\(\\inf_n \\mu[A_n]= 0\\) for every countable family \\((A_n)\\) of decreasing elements in \\(\\mathcal{R}\\) such that \\(\\cap A_n=\\emptyset\\).</li> </ol> <p>Proof</p> <ul> <li>The equivalence between \\(\\sigma\\)-additivity and \\(\\sigma\\)-sub-additivity is already shown in a previous Lemma in the context of a semi-ring.</li> <li> <p>\\(\\sigma\\)-additivity implies lower semi-continuity:     Let \\((A_n)\\) be an increasing sequence of elements in \\(\\mathcal{R}\\) such that \\(A=\\cup A_n\\) is in \\(\\mathcal{R}\\).     Defining \\(B_n=A_n\\setminus \\cup_{k&lt;n}A_k=A_n\\setminus B_{n-1}\\) for \\(n&gt;1\\) and \\(B_1=A_1\\) provides a disjoint sequence of elements in \\(\\mathcal{R}\\).     Since \\(A_n=\\cup_{1\\leq k\\leq n} B_k\\) and \\(A=\\cup B_n\\), it follows from \\(\\sigma\\)-additivity that  </p> \\[ \\mu[A]=\\sum \\mu[B_n]=\\sup \\sum_{1\\leq k\\leq n}\\mu[B_k]=\\sup \\mu[\\cup_{1\\leq k\\leq n}B_k]=\\sup \\mu[A_n] \\] </li> <li> <p>Lower semi-continuity implies upper semi-continuity:     Let \\((A_n)\\) be a decreasing sequence of elements in \\(\\mathcal{R}\\) such that \\(A=\\cap A_n\\) is in \\(\\mathcal{R}\\).     It follows that \\(B_n=A_1\\setminus A_n\\) defines an increasing sequence such that \\(B=\\cup B_n=A_1\\setminus \\cap A_n=A_1\\setminus A \\in \\mathcal{R}\\).     Lower semi-continuity, additivity, and the properties of Lemma [lem-propcontentsemiring] imply that</p> \\[   \\mu[A_1]-\\inf \\mu[A_n]=\\sup (\\mu[A_1]-\\mu[A_n])=\\sup \\mu[A_1\\setminus A_n]=\\mu[\\cup A_1\\setminus A_n]=\\mu[A_1]-\\mu[A] \\] </li> <li> <p>Upper semi-continuity implies continuity at \\(\\emptyset\\) (immediate).</p> </li> <li> <p>Continuity at \\(\\emptyset\\) implies \\(\\sigma\\)-additivity:     Let \\((A_n)\\) be a sequence of mutually disjoint elements of \\(\\mathcal{R}\\) such that \\(A=\\cup A_n\\) is in \\(\\mathcal{R}\\).     By means of Lemma, it follows that  </p> \\[   \\sum \\mu[A_n]\\leq \\mu[A] \\] <p>It follows that \\(B_n=A\\setminus (\\cup_{1\\leq k\\leq n}A_k)\\) is a decreasing family of elements of \\(\\mathcal{R}\\) such that \\(\\cap B_n=\\emptyset\\). By additivity of \\(\\mu\\), we have</p> \\[ \\mu[A]=\\mu[B_n\\cup (\\cup_{1\\leq k\\leq n}A_k)]=\\mu[B_n]+\\sum_{1\\leq k\\leq n}\\mu[A_k] \\] <p>for every \\(n\\). Since \\(\\sum_{q\\leq k\\leq n}\\mu[A_k]\\to \\sum \\mu[A_n]\\) and by continuity at \\(\\emptyset\\), \\(\\mu[B_n]\\to 0\\), it follows that \\(\\mu[A]=\\sum \\mu[A_n]\\), hence the \\(\\sigma\\)-additivity.</p> </li> </ul> <p>We are now in position to formulate the central extension theorem of Carath\u00e9odory.</p> <p></p> <p>Theorem: Carath\u00e9odory's Extension Theorem</p> <p>Let \\(X\\) be a non-empty set, \\(\\mathcal{S}\\) a semi-ring such that \\(X=\\cup A_n\\) for some countable family \\((A_n)\\) of elements in \\(\\mathcal{S}\\). Suppose that \\(\\mu:\\mathcal{R}\\to [0,\\infty]\\) is a pre-measure such that:</p> <ol> <li>\\(\\mu[A_n]&lt;\\infty\\) for every \\(n\\).</li> <li>\\(\\mu\\) is \\(\\sigma\\)-sub-additive or equivalently \\(\\sigma\\)-additive.</li> </ol> <p>Then \\(\\mu\\) can be uniquely extended to a measure \\(\\nu\\) on \\(\\mathcal{F}=\\sigma(\\mathcal{S})\\).</p> <p>Remark</p> <p>We already know from the assumption of this theorem that according to a previous proposition there exists a unique extension \\(\\nu\\) to the ring \\(\\mathcal{R}\\) generated by \\(\\mathcal{S}\\) such that \\(\\nu\\) is a \\(\\sigma\\)-additive content. We can therefore assume for the proof of Carath\u00e9odory's theorem that \\(\\mu\\) is a \\(\\sigma\\)-additive content on a ring.</p> <p>The existence is the complex and lengthy part of the proof. However uniqueness can be adressed through the following proposition.</p> <p>Proposition</p> <p>Let \\((X,\\mathcal{F})\\) be a measurable space and \\(\\mathcal{P}\\) a \\(\\pi\\)-system on \\(X\\) that generates \\(\\mathcal{F}\\) and such that there exists a sequence \\((A_n)\\) of elements of \\(\\mathcal{P}\\) with \\(\\cup A_n=X\\). Let \\(\\mu\\) and \\(\\nu\\) be two measures on \\(\\mathcal{F}\\) that coincide on \\(\\mathcal{P}\\) and such that \\(\\mu(A_n)=\\nu(A_n)&lt;\\infty\\) for every \\(n\\). Then \\(\\mu=\\nu\\).</p> <p>Proof</p> <p>Let \\(n\\) be an integer and consider the collection of sets \\(\\mathcal{C}^n=\\{A \\in \\mathcal{F}\\colon \\mu[A\\cap A_n]=\\nu[A\\cap A_n]\\}\\). Then \\(\\mathcal{C}^n\\) is a \\(\\lambda\\)-system. Indeed</p> <ul> <li>\\(X\\) is in \\(\\mathcal{C}^n\\) since \\(\\mu[X\\cap A_n]=\\mu[A_n]=\\nu[A_n]=\\nu[A_n\\cap X]\\).</li> <li>Stability under relative complements: For \\(A\\subseteq B\\) with \\(A\\) and \\(B\\) in \\(\\mathcal{C}^n\\), it holds that \\(\\mu[(B\\setminus A)\\cap A_n]=\\mu[B\\cap A_n]-\\mu[A\\cap A_n]=\\nu[B\\cap A_n]-\\nu[A\\cap A_n]=\\nu[(B\\setminus A)\\cap A_n]\\), showing that \\(B\\setminus A\\) belongs to \\(\\mathcal{C}^n\\).</li> <li> <p>Stability under countable disjoint unions: If \\((B_m)\\) is a disjoint sequence in \\(\\mathcal{C}^n\\), then</p> \\[ \\mu[(\\cup B_m) \\cap A_n]=\\sum_m \\mu[B_m\\cap A_n]=\\sum_m \\nu[B_m\\cap A_n]=\\nu[(\\cup B_m)\\cap A_n] \\] <p>showing that \\(\\cup B_m \\in \\mathcal{C}^n\\).  </p> </li> </ul> <p>Since \\(\\mathcal{P}\\subseteq \\mathcal{C}^n\\), and \\(\\mathcal{P}\\) is stable under intersection, by means of the Dynkin Lemma, it follows that \\(\\mathcal{F}=\\sigma(\\mathcal{P})\\subseteq \\mathcal{C}^n\\subseteq \\mathcal{F}\\). Thus, \\(\\mu=\\nu\\).  </p> <p>Example: Lebesgue/Stieljes Measure</p> <p>The conditions of Carath\u00e9odory's Theorem are fulfilled in the case of the Lebesgue/Stieltjes measure. Therefore, it defines a unique measure on the Borel \\(\\sigma\\)-algebra of \\(\\mathbb{R}\\).</p> <p>We are left to show Carath\u00e9odory's extension theorem. The proof relies on the concept of outer measure. So far we tried to built measures from the bottom up, from simple class of sets and trying to reach \\(\\sigma\\)-algebra. The idea of outer measure is a top down approach hopping that both concepts would meet at the right place, that is, the \\(\\sigma\\)-algebra.</p>"},{"location":"lecture/02-Measure/023-ring-to-s-algebra/#outer-measure","title":"Outer Measure","text":"<p>Definition: Outer Measure</p> <p>A set function \\(\\mu^\\ast \\colon 2^X \\to [0, \\infty]\\) is called an outer measure if</p> <ul> <li>\\(\\mu^\\ast[\\emptyset] = 0\\);</li> <li> <p>\\(\\sigma\\)-sub-additivity: For any countable family \\((B_n)\\) of subsets of \\(X\\) and \\(A \\subseteq \\cup B_n\\) it holds</p> \\[   \\mu^\\ast[A] \\leq \\sum \\mu^\\ast[B_n] \\] </li> </ul> <p>A set \\(A\\subseteq X\\) is called \\(\\mu^\\ast\\)-measurable if </p> \\[   \\mu^\\ast[E] = \\mu^\\ast\\left[ E\\cap A \\right] + \\mu^\\ast \\left[ E\\cap A^c \\right] \\] <p>for every \\(E\\subseteq X\\).</p> <p>Note that the second condition implies that an outer measure is monotone, that is \\(\\mu[A] \\leq \\mu[B]\\) for any \\(A\\subseteq B\\). The notion of \\(\\mu^\\ast\\)-measurability of a set depends naturally of the outer measure. Clearly, due to subadditivity, for any subsets \\(E\\) and \\(A\\) of \\(X\\) it holds</p> \\[ \\mu^\\ast[E] \\leq \\mu^\\ast[E \\cap A] + \\mu^\\ast[E\\cap A^c] \\] <p>Hence, \\(A\\) is \\(\\mu^\\ast\\)-measurable if the reverse inequality holds for any set \\(E\\subseteq X\\). The denomination measurable is justified by the following theorem.</p> <p>Theorem</p> <p>Let \\(\\mu^\\ast\\) be an outer measure on \\(X\\). Then, the collection \\(\\mathcal{M}\\) of all \\(\\mu^\\ast\\)-measurable is a \\(\\sigma\\)-algebra. Furthermore, the restriction of \\(\\mu^\\ast\\) on this collection is a measure.</p> <p>Proof</p> <p>We show that \\(\\mathcal{M}\\) is a \\(\\sigma\\)-algebra.</p> <p>Step 1 - \\(\\emptyset\\) belongs to \\(\\mathcal{M}\\): Clearly, \\(\\emptyset\\) is obviously \\(\\mu^\\ast\\) measurable.</p> <p>Step 2 - Stability under complementation: For \\(A\\) in \\(\\mathcal{M}\\), then for any \\(E \\subseteq X\\) it holds that</p> \\[       \\mu^*(E) = \\mu^*(E \\cap A) + \\mu^*(E \\cap A^c). \\] <p>Interchanging \\(A\\) and \\(A^c\\) yields \\(A^c\\) belongs to \\(\\mathcal{M}\\).</p> <p>Step 3 - Stability under union: Let \\(A\\) and \\(B\\) be two \\(\\mu^\\ast\\)-measurable sets. For any set \\(E\\) it holds</p> \\[     \\mu^\\ast[E] = \\mu^\\ast[E \\cap A] + \\mu^\\ast[E \\cap A^c] \\] <p>In particular for \\(B\\) it holds</p> \\[       \\mu^\\ast[E\\cap A^c] = \\mu^\\ast[(E\\cap A^c) \\cap B] + \\mu^\\ast[(E\\cap A^c) \\cap B^c] \\] <p>Since by sub-additivity it holds that \\(\\mu^\\ast[E\\cap(A\\cup B)] \\leq \\mu^\\ast[E \\cap A] + \\mu^\\ast[E\\cap(A^c \\cap B)]\\). Hence, it holds that</p> \\[   \\begin{align*}     \\mu^\\ast[E] &amp; =  \\mu^*(E \\cap A) + \\mu^\\ast[(E\\cap A^c) \\cap B] + \\mu^\\ast[(E\\cap A^c) \\cap B^c]\\\\                 &amp; = \\mu^*(E \\cap A) + \\mu^\\ast[(E\\cap A^c) \\cap B] + \\mu^\\ast[(E\\cap (A\\cup B)^c]\\\\                 &amp; \\geq \\mu^\\ast[E\\cap (A\\cup B)] + \\mu^\\ast[E\\cap (A\\cup B)^c]    \\end{align*} \\] <p>showing that \\(A\\cup B\\) is \\(\\mu^\\ast\\)-measurable.</p> <p>In particular \\(\\mathcal{M}\\) is a ring.</p> <p>Step 4 - stability under countable union:  Since \\(\\mathcal{M}\\) is a ring, it is enough to show that \\(\\cup A_n\\) is \\(\\mu^\\ast\\)-measurable for any countable family \\((A_n)\\) of pairwise disjoints \\(\\mu^\\ast\\)-measurable sets. Let \\(B_n =\\cup_{k\\leq n} A_k\\) which is measurable for each \\(n\\). By induction, it holds that \\(\\mu^\\ast[E\\cap B_n] = \\sum_{k\\leq n} \\mu^\\ast[E\\cap A_k]\\) for any set \\(E\\subseteq X\\). Indeed \\(B_n\\) is \\(\\mu^\\ast\\)-measurable and \\(\\mu^\\ast[E \\cap B_{n+1}] = \\mu^\\ast[E\\cap B_{n+1}\\cap B_n ] + \\mu^\\ast[E \\cap B_{n+1}\\cap B_n^c] = \\mu^\\ast[E\\cap B_n] + \\mu^\\ast[E\\cap A_{n+1}]\\). By monotonicity, it also holds that</p> \\[   \\mu^\\ast[E \\cap A] \\geq \\sup_n \\mu^\\ast[E \\cap B_n] = \\sum \\mu^\\ast[E \\cap A_k] \\] <p>which together with sub-additivity yields \\(\\mu^\\ast[E\\cap A] = \\sum \\mu^\\ast[E\\cap A_n]\\). Hence, for any \\(E\\) we get</p> \\[   \\mu^\\ast[E] = \\mu^\\ast[E\\cap B_n] + \\mu^\\ast[E\\cap B_n^c]\\geq \\sum_{k\\leq n}\\mu^\\ast[E\\cap B_k]+\\mu^\\ast[E\\cap A^c] \\] <p>from which follows that \\(\\mu^\\ast[E]\\geq \\mu^\\ast[E\\cap A] + \\mu^\\ast[E\\cap A^c]\\) showing that \\(A = \\cup A_n\\) is \\(\\mu^\\ast\\)-measurable. Thus \\(\\mathcal{M}\\) is a \\(\\sigma\\)-Algebra.</p> <p>The fact that \\(\\mu^\\ast\\) restricted to \\(\\mathcal{M}\\) is immediate since it is \\(\\sigma\\)-sub-additive.</p>"},{"location":"lecture/02-Measure/023-ring-to-s-algebra/#caratheodorys-extension-theorem-proof","title":"Carath\u00e9odory's Extension Theorem Proof","text":"<p>We are now in position to show the proof of Carath\u00e9odory's extension theorem.</p> <p>Proof: Carath\u00e9odory's Extension Theorem</p> <p>From the previous results, we know that we can extend uniquely \\(\\mu\\) to a \\(\\sigma\\)-additive and finite measure to the ring \\(\\mathcal{R}\\) generated by \\(\\mathcal{S}\\).</p> <p>We define the set function</p> \\[   \\mu^\\ast[E]:= \\inf \\left\\{ \\sum \\mu^\\ast[A_n] \\colon E\\subseteq \\cup A_n \\colon (A_n) \\text{ is a countable family in }\\mathcal{R} \\right\\} \\] <p>By definition, \\(\\mu^\\ast\\) coincides with \\(\\mu\\) on \\(\\mathcal{R}\\). Let us show that \\(\\mu^\\ast\\) is an outer measure. Clearly, \\(\\mu^\\ast[\\emptyset] = 0\\). Let us show that \\(\\mu^\\ast\\) is \\(\\sigma\\)-sub-additive by taking a countable family \\((E_n)\\) and \\(E\\) of sets such that \\(E\\subseteq \\cup E_n\\). Since if \\(\\mu^\\ast[E_n] = \\infty\\) for some \\(n\\), then trivialy it holds that \\(\\mu^\\ast[E]\\leq \\sum \\mu^\\ast[E_n]\\), we can then assume that \\(\\mu^\\ast[E_n]&lt;\\infty\\) for any \\(n\\). Choose families \\((A_{nk})\\) in \\(\\mathcal{R}\\) such that \\(E_n \\subseteq_k A_{nk}\\) and \\(\\sum_k \\mu^\\ast[A_{nk}] \\leq \\mu^\\ast[E_n] + \\varepsilon 2^{-k}\\). It follows that \\(E\\subseteq \\cup_{nk}A_{nk}\\) and </p> \\[   \\mu^\\ast[E]\\leq \\sum_{nk}\\mu^\\ast[A_{nk}] \\leq \\sum \\mu^\\ast[E_n] + \\varepsilon \\] <p>which by arbitrariness of \\(\\varepsilon\\) show \\(\\sigma\\)-subadditivity.</p> <p>From the previous Theorem on outer measure, it follows that the set \\(\\mathcal{M}\\) of \\(\\mu^\\ast\\)-measurable sets is a \\(\\sigma\\)-algebra on which \\(\\mu^\\ast\\) is a measure. Since any element of \\(\\mathcal{R}\\) is \\(\\mu^\\ast\\)-measurable, it follows that \\(\\sigma(\\mathcal{R})\\subseteq \\mathcal{M}\\).</p> <p>Hence \\(\\mu^\\ast\\) defines a measure on \\(\\sigma(\\mathcal{R})\\) coinciding with \\(\\mu\\) on \\(\\mathcal{R}\\) which ends the proof.</p> <p>Example: Lebesgue/Stieljes measure</p> <p>From the previous derivation together with this extension theorem, we conclude that the Lebesgue/Stieljes measure is indeed a measure.</p>"},{"location":"lecture/02-Measure/024-product-measure/","title":"Product Measure, Stochastic Kernel, Kolmogorov Extension Theorem","text":"<p>We saw in the previous chapter that we can extend pre-measure from semi-rings to the \\(\\sigma\\)-algebra generated by it.</p>"},{"location":"lecture/03-Integration/030-introduction/","title":"Introduction/Notations","text":"<p>After treating the notion of measurable spaces and measures, we will now address integration. However this lecture is about stochastics and therefore, even if many notions extends to more general theory we will stick to probability spaces. To do so, let us first fix the probabilistic historical jargon and notations.</p> <ul> <li> <p>\\((\\Omega, \\mathcal{F}, P)\\) is called a probability space if \\(\\mathcal{F}\\) is a \\(\\sigma\\)-algebra on \\(\\mathcal{F}\\) and \\(P\\) is a finite measure with \\(P[\\Omega] = 1\\);</p> </li> <li> <p>Elements of \\(\\Omega\\) are called states and usually denoted by \\(\\omega\\);</p> </li> <li>Elements of \\(\\mathcal{F}\\) are called events;</li> <li>The measure \\(P[A]\\) of an event \\(A\\) is called the **probability of ** \\(A\\).</li> <li>A random variable is a measurable function from \\(\\Omega\\) to \\(\\mathbb{R}\\) where \\(\\mathbb{R}\\) is endowed with the Borel \\(\\sigma\\)-algebra.      Usually, random variables are denoted with capital letters \\(X, Y, Z, \\cdots\\)</li> </ul> <p>As mentioned, the following does not change if you assume that \\(P\\) is a \\(\\sigma\\)-finite measure.  </p> <p>Throughout, we denote by \\(\\mathcal{L}^0:=\\mathcal{L}^0(\\Omega,\\mathcal{F})\\) the set of all random variables. Given a random variable \\(X\\), we also use the following shorthand notations for events:</p> \\[ \\begin{align*}   \\{X \\in B\\}   &amp; := \\{\\omega \\in \\Omega\\colon X(\\omega)\\in B\\}\\\\   \\{X \\leq t\\} &amp; := \\{\\omega \\in \\Omega\\colon X(\\omega)\\leq t\\}   \\ldots \\end{align*} \\] <p>for \\(B \\in \\mathcal{F}\\) and \\(t\\in \\mathbb{R}\\).  </p> <p>As for the measure of these events, we adopt the notations:</p> \\[ P[X \\in B] := P[\\{X \\in B\\}], \\quad P[X\\leq t]:= P[\\{X \\leq t\\}], \\dots \\] <p>Given an event \\(A\\) we define the indicator function \\(1_A\\) as the function:</p> \\[ \\begin{equation}   \\begin{split}      1_A\\colon \\Omega &amp;\\longrightarrow \\mathbb{R}\\\\     \\omega &amp;\\longmapsto 1_A(\\omega)=     \\begin{cases}         1 &amp;\\text{if } \\omega \\in A\\\\         0 &amp;\\text{otherwise}     \\end{cases}   \\end{split} \\end{equation} \\] <p> </p> <p>Throughout, we will also deal with extended real-valued random variables, that is, functions \\(X:\\Omega \\to [-\\infty,\\infty]\\) which are measurable. We denote this set by \\(\\bar{\\mathcal{L}}^0\\).  </p> <p>Note that for \\(X \\in \\bar{\\mathcal{L}}^0\\), we can write:</p> \\[ X=-\\infty 1_A +Y1_B+\\infty1_C \\] <p>where \\(A\\), \\(B\\), and \\(C\\) are pairwise disjoint and \\(Y\\in \\mathcal{L}^0\\).</p> <p>Further, we denote by \\(\\mathcal{L}^0_+\\) or \\(\\bar{\\mathcal{L}}^0_+\\) the set of those random variables or extended real-valued random variables that are positive.  </p> <p>This chapter is divided as follows</p> <ul> <li>Expectation, Lebesgue's Convergence</li> <li>\\(L^p\\)-Spaces and Classical Inequalities</li> <li>Radon-Nykodym and Conditional Expectation</li> <li>Independence</li> <li>Funini-Tonelli</li> <li>Uniform Integrability</li> </ul>"},{"location":"lecture/03-Integration/031-expectation/","title":"Expectation","text":""},{"location":"lecture/03-Integration/031-expectation/#integration-lebesgue-convergence-theorem","title":"Integration, Lebesgue Convergence Theorem","text":"<p>Definition: Simple Random Variables</p> <p>A random variable \\(X\\) is said to be simple if there exists a finite family \\((A_k)_{1\\leq k\\leq n}\\) of pairwise disjoint events and real nonzero numbers \\((\\alpha_k)_{1\\leq k\\leq n}\\) such that:</p> \\[   X=\\sum_{k=1}^n \\alpha_k 1_{A_k} \\] <p>We denote by \\(\\mathcal{L}^{0,step}\\) the set of all simple random variables.</p> <p> </p> <p>Warning</p> <p>Clearly, \\(\\mathcal{L}^{0,step}\\) is a subset of \\(\\mathcal{L}^0\\). However, the decomposition of a simple random variable \\(X\\) into \\((A_k)_{1\\leq k\\leq n}\\) and \\((\\alpha_k)_{1\\leq k\\leq n}\\) is not unique. Indeed, let \\(A\\) and \\(B\\) be two elements of \\(\\mathcal{F}\\) such that \\(A\\subseteq B\\). It follows that \\(X=1_B=1_{B\\setminus A}+1_A\\).</p> <p>Lemma</p> <p>The spaces \\(\\mathcal{L}^0\\) as well as \\(\\mathcal{L}^{0,step}\\) are vector spaces.</p> <p>Proof</p> <p>The proof is left as an exercise.</p> <p>Let \\(X=\\sum_{1\\leq k\\leq n}\\alpha_k 1_{A_k}\\) and \\(Y=\\sum_{1\\leq l\\leq m}\\beta_l 1_{B_l}\\) be two simple random variables. It is possible to find a finite family \\((C_r)_{1\\leq j\\leq r}\\) of pairwise disjoint elements in \\(\\mathcal{F}\\) and two finite families \\((\\tilde{\\alpha_j})_{1\\leq j\\leq r}\\) and \\((\\tilde{\\beta}_j)_{1\\leq j\\leq r}\\) of nonzero numbers such that:</p> \\[  \\begin{equation}    X=\\sum_{1\\leq j\\leq r} \\tilde{\\alpha}_j 1_{C_j},\\quad Y=\\sum_{1\\leq j\\leq r} \\tilde{\\beta}_j 1_{C_j}  \\end{equation} \\] <p>Indeed, let \\(C_{kl}=A_k\\cap B_l\\), which constitutes a finite family of pairwise disjoint elements in \\(\\mathcal{F}\\). It follows that:</p> \\[  X=\\sum_{1\\leq k\\leq n}\\sum_{1\\leq l\\leq m} \\alpha_k 1_{C_{kl}},\\quad Y=\\sum_{1\\leq k\\leq n}\\sum_{1\\leq l\\leq m} \\beta_{l} 1_{C_{kl}} \\] <p>In other words, any two simple random variables can be decomposed on the same common family of pairwise disjoint elements.</p> <p>Definition: Expectation 1.0</p> <p>We define the expectation of a simple random variable \\(X=\\sum_{1\\leq k\\leq n}\\alpha_k 1_{A_k}\\) with respect to \\(P\\) as:</p> \\[   \\hat{E}[X]:=\\sum_{k\\leq n} \\alpha_k P[A_k] \\] <p> </p> <p>Exercise</p> <p>Show that the definition of expectation is a well-defined operator on \\(\\mathcal{L}^{0,step}\\).   Indeed, the decomposition of \\(X\\) is not unique.</p> <p>Proposition</p> <p>On \\(\\mathcal{L}^{0,step}\\), the following properties hold:</p> <ul> <li>Monotonicity: \\(\\hat{E}[X]\\leq \\hat{E}[Y]\\) whenever \\(X\\leq Y\\).</li> <li>Linearity: \\(\\hat{E}\\) is a linear operator on \\(\\mathcal{L}^{0,step}\\).</li> </ul> <p>Proof</p> <p>Let \\(X\\) and \\(Y\\) be two simple random variables.   Since those two simple random variables can be decomposed on a common set of events we can write:</p> \\[   X=\\sum_{k\\leq n} \\alpha_k 1_{A_k}, \\quad Y=\\sum_{k\\leq m}\\beta_k 1_{A_k} \\] <p>If \\(X\\leq Y\\), it follows that \\(\\alpha_k=X(\\omega)\\leq Y(\\omega)=\\beta_k\\) for every state \\(\\omega\\) in \\(A_{k}\\).   Hence:</p> \\[   \\hat{E}[X]=\\sum_{k\\leq n}\\alpha_k P[A_{k}]\\leq \\sum_{k\\leq n} \\beta_k P[A_{k}]=\\hat{E}[Y] \\] <p>For real numbers \\(a\\) and \\(b\\), it holds:</p> \\[   \\hat E[a X+bY]=\\sum_{k\\leq n} (a\\alpha_k+b\\beta_k)P[A_k]=a\\sum_{k\\leq n} \\alpha_k P[A_k]+b\\sum_{k\\leq n} \\beta_k P[A_k]=a\\hat E[X]+b\\hat E[Y] \\] <p>This proposition is important in so far that ir shows that the expectation is a linear operator which is monotone. This monotonicity property allows to extends naturally from below the expectation to the class of positive random variables.</p> <ul> <li> <p>First Approximation</p> <p> </p> </li> <li> <p>Second Approximation</p> <p> </p> </li> </ul> <p>Definition: Expectation 2.0</p> <p>For any positive extended random variable \\(X\\) in \\(\\bar{\\mathcal{L}}^0_+\\), we define the expectation of which as</p> \\[     E[X]:=\\sup \\left\\{ \\hat E[Y]\\colon Y\\leq X,Y \\in \\mathcal{L}^{0,step}_+ \\right\\} \\] <p>A random variable \\(X\\) is called integrable if \\(E[X^+]&lt;\\infty\\) and \\(E[X^-]&lt;\\infty\\). The set of integrable random variables is denoted by \\(\\mathcal{L}^1\\) and the expectation of which is defined as</p> \\[   E[X]:= E[X^+] - E[X^-] \\] <p>Remark</p> <ul> <li>Show as an exercise that for a positive extended random variable \\(X\\) where \\(P[X=\\infty]&gt;0\\), then it follows that \\(E[X]=\\infty\\);</li> <li>Clearly \\(\\mathcal{L}^{0,step}\\subseteq \\mathcal{L}^1\\);</li> <li>Also, by definition and monotonicity of \\(\\hat{E}\\), for every \\(X \\in \\mathcal{L}^{0,step}\\), it holds that \\(E[X]=\\hat{E}[X]\\).   In other terms, \\(E\\) is an extension of \\(\\hat{E}\\) to the space \\(\\bar{\\mathcal{L}}^0_+\\).   We therefore remove the hat on the top of the expectation symbol everywhere.</li> </ul> <p>Lemma</p> <p>For every \\(X\\) and \\(Y\\) in \\(\\bar{\\mathcal{L}}^0_+\\) and \\(a,b\\) positive numbers, it holds:</p> <ul> <li>\\(E[X]\\leq E[Y]\\) whenever \\(X\\leq Y\\).</li> <li>\\(E\\left[ aX+b Y \\right]=a E\\left[ X \\right]+b E\\left[ Y \\right]\\).</li> </ul> <p>Proof</p> <p>The proof is left as an exercise.</p> <p>Theorem: Lebegue's Monotone Convergence Theorem</p> <p>Let \\((X_n)\\) be an increasing sequence of positive random variables. Denote by \\(X = \\lim X_n = \\sup X_n\\) the resulting extended positive random variable limit of the sequence. Then it holds</p> \\[     E[X] = E[\\lim X_n] = \\lim E[X_n] \\] <p>Proof</p> <p>By monotonicity, we clearly have \\(E[X_n]\\leq E[X]\\) for every \\(n\\), therefore \\(\\sup E[X_n]\\leq E[X]\\). Reciprocally, suppose that \\(E[X]&lt;\\infty\\) and pick \\(\\varepsilon&gt;0\\) and some simple positive random variable \\(Y\\) such that \\(Y\\leq X\\) and \\(E[X]-\\varepsilon\\leq E[Y]\\). For \\(0&lt;c&lt;1\\) define the sets \\(A_n=\\{X_n\\geq cY\\}\\). Since \\(X^n\\) is increasing to \\(X\\), it follows that \\(A_n\\) is an increasing sequence of events. Furthermore, since \\(cY\\leq Y\\leq X\\) and \\(cY&lt;X\\) on \\(\\{X&gt;0\\}\\), it follows that \\(\\cup A_n=\\Omega\\).  By non-negativity of \\(X_n\\) and monotonicity, it follows that</p> \\[ cE[1_{A_n}Y]\\leq E[1_{A_n}X_n]\\leq E[X_n] \\] <p>and so</p> \\[ c\\sup E[1_{A_n}Y]\\leq \\sup E[X_n] \\] <p>Since \\(Y=\\sum_{l\\leq k} \\alpha_l 1_{B_l}\\) for \\(\\alpha_1,\\ldots,\\alpha_k \\in \\mathbb{R}_+\\) and \\(B_1,\\ldots, B_k\\in \\mathcal{F}\\), it follows that</p> \\[ E\\left[ 1_{A_n}Y \\right]=\\sum_{l\\leq k}\\alpha_l P[A_n\\cap B_l]. \\] <p>However, since \\(P\\) is a probability measure, and \\(A_n\\) is increasing to \\(\\Omega\\), it follows from the lower semi-continuity of probability measures that \\(P[A_n\\cap B_l]\\nearrow P[\\Omega\\cap B_l]=P[B_l]\\), and so</p> \\[ \\sup E[1_{A_n}Y]=\\sum_{l\\leq k}\\alpha_l \\sup P[A_n\\cap B_l]=\\sum \\alpha_l P[B_l]=E[Y]. \\] <p>Consequently</p> \\[ E[X]\\geq \\lim E[X_n]=\\sup E[X_n]\\geq cE[Y] \\geq cE[X]-c\\varepsilon \\] <p>which by letting \\(c\\) converging to \\(1\\) and \\(\\varepsilon\\) to \\(0\\) yields the result. The case where \\(E[X]=\\infty\\) is similar and left to the reader.</p> <p>As the previous figure suggests, it is actually possible to construct by hand a sequential approximation of positive random variables by simple ones.</p> <p>Proposition: Approximation by Simple Random Variables</p> <p>For any positive random variabel \\(X\\), there exists an increasing sequence of simple positive random variables \\((X_n)\\) such that \\(X_n(\\omega)\\nearrow X(\\omega)\\) and uniformly on each set \\(\\{X\\leq M\\}\\) where \\(M\\in \\mathbb{R}\\).</p> <p>Proof</p> <p>Let \\(A_k^n=\\{(k-1)/2^n\\leq X&lt;k/2^n\\}\\) for \\(k=1,\\ldots, n2^n\\) and define</p> \\[ X_n:=\\sum_{k=1}^{n2^n} \\frac{k-1}{2^n}1_{A_k^n}+n1_{\\{X&gt;n\\}} \\] <p>From the definition, it follows that \\(X_n\\leq X\\) for every \\(n\\) and \\(X(\\omega)-2^{-n}\\leq X_n(\\omega)\\) for every \\(\\omega \\in \\{X\\leq n\\}\\). This, along with the monotonicity, concludes the proof.</p> <p>Proposition</p> <p>For \\(X\\) and \\(Y\\) in \\(\\mathcal{L}^1\\), a real number \\(a\\) and two disjoint events \\(A,B\\) in \\(\\mathcal{F}\\). The following assertions hold:</p> <ol> <li>\\(1_A X\\), \\(X+Y\\), \\(aX\\) and \\(|X+Y|\\) are integrable.</li> <li>\\(E[(1_A+1_B)X]=E[1_AX]+E[1_B Y]\\).</li> <li>\\(E[X+Y]=E[X]+E[Y]\\) and \\(E[aX]=aE[X]\\).</li> <li>\\(E[X]\\leq E[Y]\\) whenever \\(X\\leq Y\\).</li> <li>If \\(X\\geq 0\\) and \\(E[X]=0\\), then \\(P[X=0]=1\\).</li> <li>If \\(P[X\\neq Y]=0\\), then \\(E[X]=E[Y]\\).</li> <li>If \\(Z\\) is a random variable such that \\(|Z|\\leq X\\), then \\(Z\\) is integrable.</li> </ol> <p>Remark</p> <p>In particular, \\(\\mathcal{L}^1\\) is a vector space and the expectation operator \\(E:\\mathcal{L}^1\\to \\mathbb{R}\\) is a monotone, positive, and linear functional.</p> <p>Proof</p> <ol> <li> <p>It holds \\(|X+Y|\\leq |X|+|Y|\\).     According to Lemma \\ref{lem:linearityL0+}, it follows that \\(E[|X+Y|]\\leq E[|X|+|Y|]=E[|X|]+E[|Y|]&lt;\\infty\\), showing that \\(X+Y\\) and \\(|X+Y|\\) are integrable.     The argumentation for \\(1_AX\\) and \\(aX\\) follows the same line.</p> </li> <li> <p>It holds \\(\\left( 1_A+1_B \\right)X=(1_A+1_B)X^+-(1_A+1_B)X^-\\).     From the linearity on \\(\\mathcal{L}^0_+\\), it follows that \\(E[(1_A+1_B)X^\\pm]=E[1_AX^\\pm]+E[1_BX^\\pm]\\), showing that \\(E[(1_A+1_B)X]=E[1_AX]+E[1_BX]\\).</p> </li> <li> <p>Without loss of generality, assume that \\(a\\geq 0\\).     Here again, it follows from \\(aX=aX^+-aX^-\\) and from the linearity on \\(\\mathcal{L}_+^0\\) that \\(E[aX^\\pm]=aE[X^{\\pm}]\\).     Also, since \\(X+Y=(X^++Y^+)-(X^-+Y^-)=(X+Y)^+-(X+Y)^-\\), it follows that \\((X^++Y^+)+(X+Y)^-=(X^-+Y^-)+(X+Y)^+\\).     However, again from the linearity on \\(\\mathcal{L}_0^+\\), it holds \\(E[(X^++Y^+)+(X+Y)^-]=E[X^+]+E[Y^+]+E[(X+Y)^-]\\) and \\(E[(X^-+Y^-)+(X+Y)^+]=E[X^-]+E[Y^-]+E[(X+Y)^+]\\), showing that \\(E[X+Y]=E[(X+Y)^+]-E[(X+Y)^-]=E[X^+]-E[X^-]+E[Y^+]-E[Y^-]=E[X]+E[Y]\\).</p> </li> <li> <p>If \\(X\\leq Y\\), it follows that \\(0\\leq Y-X\\).     According to the proposition stating the approximation from below, let \\((Z_n)\\) be an increasing sequence of positive simple random variables such that \\(Z_n\\nearrow Y-X\\).     It follows from the monotone convergence Theorem that \\(0\\leq E[Z^n]\\leq \\sup E[Z_n]=E[Y-X]\\).     Applying the previous point, we get \\(E[Y-X]=E[Y]-E[X]\\), yielding the assertion.</p> </li> <li> <p>Let \\(A_n=\\{X\\geq 1/n\\}\\) which is an increasing sequence of events such that \\(\\cup A_n=\\{X&gt;0\\}\\).     It follows that \\(1_{A_n}1/n\\leq 1_{A_n}X\\leq X\\) since \\(X\\) is positive.     Monotonicity from the previous point yields \\(P[A_n]/n\\leq E[1_{A_n}X]\\leq E[X]=0\\), showing that \\(P[A_n]=0\\) for every \\(n\\).     By the lower semi-continuity property of measures, it follows that \\(P[A]=\\sup P[A_n]=0\\), showing that \\(P[X&gt;0]=0\\).</p> </li> <li> <p>Suppose that \\(P[X\\neq 0]=0\\) and define \\(X_n=|X|1_{\\{X=0\\}}+(|X|\\wedge n)1_{A_n}\\) where \\(A_n=\\{|X|\\geq 1/n\\}\\).     On the one hand, by definition, \\(A_n\\) is an increasing sequence such that \\(\\cup A_n=\\{|X|\\neq 0\\}\\).     Hence, \\(0\\leq X_n\\nearrow |X|\\), which by the monotone convergence Theorem implies that \\(E[X_n]\\nearrow E[|X|]\\).     On the other hand, \\(A_n\\subseteq \\{X\\neq 0\\}\\), which by monotonicity of the measure implies that \\(P[A_n]=0\\) for every \\(n\\).     Hence, \\(E[X_n]\\leq nP[A_n]=0\\) for every \\(n\\).     We conclude that \\(E[|X|]=0\\), which implies that \\(E[X]=0\\).</p> </li> <li> <p>Follows directly from the linearity on \\(\\mathcal{L}_+^0\\).</p> </li> </ol> <p>Remark</p> <p>Note that for \\(X \\in \\bar{\\mathcal{L}}^{0}\\) with \\(X^- \\in \\mathcal{L}^1\\), and \\(Y \\in \\mathcal{L}^1\\), the same argumentation as above yields that:</p> \\[   \\begin{equation}         -\\infty&lt;E[X-Y]=E[X^+-X^--Y]=E[X^+]-E[X^-]-E[Y]=E[X]-E[Y]       \\end{equation} \\] <p>We finish this section with two of the most important assertions of integration theory.</p> <p>Theorem: Fatou's Lemma and Lebegue's Dominated Convergence</p> <p>Let \\((X_n)\\) be a sequence in \\(\\mathcal{L}^0\\).</p> <ul> <li> <p>Fatou's Lemma: Suppose that \\(X_n\\geq Y\\) for some \\(Y \\in \\mathcal{L}^1\\).     Then it holds</p> \\[   \\begin{equation}   E\\left[ \\liminf X_n \\right]\\leq \\liminf E\\left[ X_n \\right]. \\end{equation} \\] </li> <li> <p>Dominated Convergence Theorem: Suppose that \\(|X_n|\\leq Y\\) for some \\(Y \\in \\mathcal{L}^1\\) and \\(X_n(\\omega)\\to X(\\omega)\\) for any state \\(\\omega\\).     Then it holds</p> \\[ \\begin{equation}   E\\left[ X \\right]=\\lim E\\left[ X_n \\right]. \\end{equation} \\] </li> </ul> <p>Proof</p> <p>By linearity, up to the variable change \\(X_n-Y\\), we can assume that \\(X_n\\) is positive since \\(E[\\liminf X_n-Y]=E[\\liminf X_n]-Y\\) and \\(E[X_n-Y]=E[X_n]-Y\\) for every \\(n\\). Let \\(Y_n=\\inf_{k\\geq n}X_n\\), which is an increasing sequence of positive random variables that converges to \\(\\liminf X_n=\\sup_n \\inf_{k\\geq n}X_k\\). Notice also that \\(Y_n\\leq X_k\\) for every \\(k\\geq n\\), and therefore by monotonicity of the expectation \\(E[Y_n]\\leq \\inf_{k\\geq n}E[X_k]\\). We conclude Fatou's lemma with the monotone convergence theorem as follows:</p> \\[ \\begin{equation}     E\\left[ \\liminf X_n \\right]=\\lim E\\left[ Y_n \\right]=\\sup E\\left[ Y_n \\right]\\leq \\sup_n\\inf_{k\\geq n}E[X_k]=\\liminf E[X_n]. \\end{equation} \\] <p>A simple sign change shows that Fatou's lemma holds in the other direction. That is, if \\(X_n\\leq Y\\) for some \\(Y \\in \\mathcal{L}^1\\), then it holds</p> \\[ \\begin{equation}     \\limsup E\\left[ X_n \\right]\\leq E\\left[ \\limsup X_n \\right]. \\end{equation} \\] <p>Now the dominated convergence theorem assumptions yield that \\(-Y\\leq X_n\\leq Y\\) for some \\(Y \\in \\mathcal{L}^1\\). Hence, since \\(X=\\lim X_n=\\liminf X_n=\\limsup X_n\\), it follows that</p> \\[ \\begin{equation}     \\limsup E\\left[ X_n \\right]\\leq E\\left[ \\limsup X_n \\right]=E\\left[ X \\right]=E\\left[ \\liminf X_n \\right]\\leq \\liminf E\\left[ X_n \\right]. \\end{equation} \\] <p>However, \\(\\liminf E\\left[ X_n \\right]\\leq \\limsup E[X_n]\\), showing that \\(E[X_n]\\) converges, and</p> \\[ \\begin{equation}     E[X]=\\liminf E[X_n]=\\limsup E[X_n]=\\lim E[X_n]. \\end{equation} \\] <p>which ends the proof.</p> <p>Example: Defining a Probability Measure from a Density</p> <p>The concept of density is quite often used in statistics as it defines new measures. Let us formalize it using dominated convergence.</p> <p>On a probability space \\((\\Omega, \\mathcal{F}, P)\\), consider a positive integrable random variable \\(Z\\) such that \\(E[Z]=1\\). We define the set function </p> \\[     \\begin{split}         Q\\colon \\mathcal{F} &amp; \\longmapsto [0,1]\\\\                 A &amp; \\longmapsto Q[A] = E[Z1_A]     \\end{split} \\] <p>which is clearly well defined and mapping to \\([0,1]\\) since \\(Z\\) is positive and \\(E[Z 1_A]\\leq E[Z]=1\\).</p> <p>It follows that \\(Q\\) defined as such is a new probability measure. Indeed</p> <ul> <li>\\(Q[\\emptyset] = E[Z1_{\\emptyset}]=E[0] =0\\), \\(Q[\\Omega] = E[Z1_{\\Omega}] = E[Z] = 1\\);</li> <li> <p>\\(\\sigma\\)-additivity: Let \\((A_n)\\) be a sequence of disjoint events.     It follows that</p> \\[     1_{\\cup_{k\\leq n}A_k} = \\sum_{k\\leq n} 1_{A_k} \\nearrow \\sum 1_{A_n} = 1_{\\cup A_n} \\] <p>By monotone convergence</p> \\[     \\begin{align*}         \\sum Q[A_n] &amp; = \\lim \\sum_{k\\leq n} Q[A_k]\\\\                     &amp; = \\lim \\sum_{k\\leq n}E[Z 1_{A_k}]\\\\                     &amp; = \\lim E\\left[ Z \\sum_{k\\leq n} 1_{A_k} \\right]\\\\                     &amp; = E\\left[ Z \\sum 1_{A_n} \\right]\\\\                     &amp; = E[Z 1_{\\cup A_n}] = Q[\\cup A_n]     \\end{align*} \\] </li> </ul> <p>It can be shown using step functions that integration under \\(P\\) and \\(Q\\) are related through the formula</p> \\[     E^Q[X] = E^P\\left[ Z X \\right] \\] <p>for any bounded random variable \\(X\\) or any \\(X\\) with sufficient integrability under \\(Q\\).</p> <p>Another particular property of the probability measure \\(Q\\) so defined is that it is absolutely continuous with respect to \\(P\\) in the sense that</p> \\[     P[A] = 0 \\quad \\text{implies} \\quad Q[A] = 0 \\]"},{"location":"lecture/03-Integration/032-lp-spaces-inequalities/","title":"\\(L^p\\)-Spaces and Classical Inequalities","text":""},{"location":"lecture/03-Integration/032-lp-spaces-inequalities/#lp-spaces","title":"\\(L^p\\)-Spaces","text":"<p>One important property of the Lebesgue integral is that it is independent of the null sets on which functions may differ.</p> <p>Proposition</p> <p>Let \\(X\\) and \\(Y\\) be two integrable random variables or two elements of \\(\\mathcal{L}_+^0\\). Suppose that \\(X\\geq Y\\) \\(P\\)-almost surely, that is \\(P[X\\geq Y]=1\\), then it follows that \\(E[X]\\geq E[Y]\\).</p> <p>In particular, if \\(X=Y\\) \\(P\\)-almost surely, then it holds \\(E[X]=E[Y]\\). Also, if \\(X\\geq 0\\) \\(P\\)-almost surely and \\(E[X]=0\\), then it follows that \\(X=0\\) \\(P\\)-almost surely.</p> <p>Proof</p> <p>Suppose that \\(X\\geq Y\\) \\(P\\)-almost surely and define \\(A=\\{X&lt;Y\\}\\), which is a negligible set, that is, an event of \\(0\\) probability.   It follows that \\((X-Y)1_{A^c} \\in \\mathcal{L}^0_+\\), and so \\(E[(X-Y)1_{A^c}]=E[X1_{A^c}]-E[Y1_{A^c}]\\geq 0\\) by monotonicity.</p> <p>On the other hand, \\((Y-X)1_{A} \\in \\mathcal{L}^0_+\\), and let \\(Z^n=\\sum \\alpha_k 1_{B_k^n}\\) be an increasing sequence of step random variables that converges to \\((Y-X)1_{A}\\). Since \\((Y-X)1_{A}=0\\) on \\(A^c\\), it follows that \\(B_k^n\\subseteq A\\) for every \\(k,n\\) and therefore \\(P[B_k^n]\\leq P[A]=0\\) for every \\(k,n\\).   We deduce that \\(E[Z^n]=0\\) for every \\(n\\) and by Lebesgue monotone convergence, it follows that \\(E[(Y-X)1_{A}]=0\\).</p> <p>We conclude by noticing that \\((X-Y)=(X-Y)1_{A^c}-(Y-X)1_{A}\\).</p> <p>This proposition allows in the monotone convergence theorem, Fatou's lemma, as well as dominated convergence, to replace convergence of random variables and inequalities by \\(P\\)-almost sure convergence and \\(P\\)-almost sure inequalities.</p> <p>On \\(\\mathcal{L}^1\\), we can define the operator \\(X\\mapsto \\|X\\|_1=E[|X|]\\). Verify that:</p> <ul> <li>\\(X=0\\) implies \\(\\|X\\|_1=0\\).</li> <li>\\(\\|X+Y\\|_1\\leq \\|X\\|_1+\\|Y|_1\\).</li> <li>\\(\\|\\lambda X\\|_1=|\\lambda|\\|X\\|_1\\).</li> </ul> <p>In other words, \\(\\|\\cdot\\|\\) is \"almost\" a norm if in the first point we had equivalence and not only implication. However, as the previous proposition shows, it actually holds:</p> <ul> <li>\\(\\|X\\|_1=0\\) if and only if \\(X=0\\) \\(P\\)-almost surely.</li> </ul> <p>We therefore proceed as in Algebra. Inspection shows that \\(X\\sim Y\\) on \\(\\mathcal{L}^0\\) if and only if \\(X=Y\\) \\(P\\)-almost surely is an equivalence relation.<sup>1</sup> We can therefore define the quotient of equivalence classes \\(L^0=\\mathcal{L}^0/\\sim\\). We can work there just as in \\(\\mathcal{L}^0\\) in the \\(P\\)-almost sure sense, that is, \\(X=Y\\) means \\(X=Y\\) \\(P\\)-almost surely, even if \\(X\\) is actually just a representative of its equivalence class. Inequality is also compatible with the equivalence relation, and therefore \\(X\\geq Y\\) means \\(X\\geq Y\\) \\(P\\)-almost surely. Every operation that is blind with respect to null measure sets can be carried over to \\(L^0\\). This is the case for the expectation on \\(L^0_+\\). Similarly, we can define \\(L^1\\) as the set of equivalence classes of integrable random variables that coincide \\(P\\)-almost surely.</p> <p>Also, since the operator \\(\\left\\Vert\\cdot\\right\\Vert_1\\) does not take into account objects defined on negligible sets, it carries over to \\(L^1\\) as a true norm, making \\((L^1,\\left\\| \\cdot \\right\\| )\\) a normed space. We can further define, for \\(1\\leq p\\leq \\infty\\), the following operators on \\(L^0\\):</p> \\[ \\begin{equation}     \\left\\Vert X\\right\\Vert_p=     \\begin{cases}         E\\left[ \\left\\vert X\\right\\vert^p \\right]^{1/p}, &amp; \\text{if } p&lt;\\infty, \\\\         \\inf\\left\\{ m\\colon P\\left[ \\left\\vert X\\right\\vert\\leq m \\right] =1\\right\\}, &amp; \\text{if } p=\\infty.     \\end{cases} \\end{equation} \\] <p>that give rise to the spaces</p> \\[ \\begin{equation}     L^p :=\\left\\{ X \\in L^0\\colon \\left\\Vert X\\right\\Vert_p&lt;\\infty \\right\\}. \\end{equation} \\] <p>Even if \\(L^1\\) is a normed space for \\(\\|\\cdot \\|\\) it is not clear that \\(L^p\\) is a normed space for \\(\\|\\cdot \\|_p\\) as we do not know if it is subadditive.</p>"},{"location":"lecture/03-Integration/032-lp-spaces-inequalities/#jensen-holder-minkowsky-and-markov-inequalities","title":"Jensen, H\u00f6lder, Minkowsky and Markov Inequalities","text":"<p>Due to the linearity and monotonicity of the expectation several central inequalities can be derived.</p> <p>Theorem: Jensen's inequality</p> <p>Let \\(\\varphi:\\mathbb{R}\\to \\mathbb{R}\\) be a convex function and \\(X\\) be an integrable random variable. It holds</p> \\[ \\begin{equation}     \\varphi\\left( E\\left[ X \\right] \\right)\\leq E\\left[ \\varphi(X) \\right]. \\end{equation} \\] <p>Proof</p> <p>Let \\(x_0=E[X]\\). Since \\(\\varphi\\) is a convex real-valued function, the existence of a sub-derivative for convex functions implies the existence of \\(a\\) and \\(b\\) in \\(\\mathbb{R}\\) such that</p> \\[ \\begin{equation}     \\varphi(x)\\geq ax +b, \\text{ for all any }x \\quad \\text{and} \\quad \\varphi(x_0)=ax_0+b \\end{equation} \\] <p>Hence,</p> \\[ \\begin{equation}     E\\left[ \\varphi(X) \\right]\\geq aE[X]+b=ax_0+b=\\varphi\\left( E[X] \\right). \\end{equation} \\] <p>which ends the proof.</p> <p>Exercise</p> <p>Using Jensen's inequality, prove that \\((\\prod a_i)^{1/n}\\leq \\frac{1}{n} \\sum a_i\\) where \\(a_1, \\dots, a_n&gt;0\\).</p> <p>Theorem: H\u00f6lder and Minkowski Inequalities</p> <p>Let \\(1\\leq p,q \\leq \\infty\\) be convex conjugate, that is, such that \\(1/p+1/q=1\\).</p> <p>For every \\(X\\) in \\(L^p\\) and \\(Y\\) in \\(L^q\\), the H\u00f6lder inequality reads as follows:</p> \\[ \\begin{equation}     \\left\\Vert XY\\right\\Vert_1=E\\left[ \\left\\vert XY\\right\\vert \\right]\\leq  E\\left[ \\left\\vert X\\right\\vert^p \\right]^{1/p}E\\left[ \\left\\vert Y\\right\\vert^q \\right]^{1/q}=\\left\\Vert X\\right\\Vert_p\\left\\Vert Y\\right\\Vert_q. \\end{equation} \\] <p>For every \\(X\\) and \\(Y\\) in \\(L^p\\), the Minkowski inequality reads as follows:</p> \\[ \\begin{equation}     \\left\\Vert X+Y\\right\\Vert_p=E\\left[ \\left\\vert X+Y\\right\\vert^p \\right]^{1/p}\\leq  E\\left[ \\left\\vert X\\right\\vert^p \\right]^{1/p}+E\\left[ \\left\\vert Y\\right\\vert^p \\right]^{1/p}=\\left\\Vert X\\right\\Vert_p+\\left\\Vert Y\\right\\Vert_p. \\end{equation} \\] <p>Proof</p> <p>As for the H\u00f6lder inequality, in the case where \\(p=1\\) and \\(q=\\infty\\), the inequality follows from \\(\\left\\vert XY\\right\\vert\\leq \\left\\vert X\\right\\vert\\left\\Vert Y\\right\\Vert_\\infty\\). Suppose therefore that \\(p,q\\) are conjugate with values in \\((1,\\infty)\\). Without loss of generality, we may assume that \\(X\\) and \\(Y\\) are positive. It holds</p> \\[ \\begin{equation}     E[XY]=E[Y^q]\\int XY^{1-q}\\frac{Y^qdP}{E[Y^q]}=E[Y^q]E_Q\\left[XY^{1-q} \\right] \\end{equation} \\] <p>where \\(E_Q\\) is the expectation operator under the measure \\(Q\\) where the density is given by \\(dQ:=Y^qdP/E[Y^q]\\).<sup>2</sup> Defining the convex function \\(x\\mapsto \\varphi(x)=x^p\\), Jensen's inequality together with the fact that \\(p(1-q)+q=0\\) and \\(1-1/p=1/q\\) yields</p> \\[ \\begin{align}     E[XY]&amp;=E[Y^q]E_Q[X Y^{1-q}] \\\\     &amp;=E[Y^q]\\varphi(E_Q[XY^{1-q}])^{1/p} \\\\     &amp;\\leq E[Y^q]E_Q\\left[ \\varphi(XY^{1-q}) \\right]^{1/p} \\\\     &amp;=E[Y^q]E_Q\\left[X^p Y^{p(1-q)}\\right]^{1/p} \\\\     &amp;=E[Y^q]E\\left[X^p Y^{p(1-q)}Y^q/E[Y^q] \\right]^{1/p} \\\\     &amp;=E[X^p]^{1/p}E[Y^q]^{1-1/p} \\\\     &amp;=E[X^p]^{1/p}E[Y^q]^{1/q}. \\end{align} \\] <p>As for the Minkowski inequality, in the case where \\(p=1\\), it follows from \\(\\left\\vert x+y\\right\\vert\\leq \\left\\vert x\\right\\vert+\\left\\vert y\\right\\vert\\). The case where \\(p=\\infty\\) is also easy. Suppose therefore that \\(1&lt;p&lt;\\infty\\). First, notice that by convexity it holds \\(\\left\\vert x+y\\right\\vert^p\\leq \\frac{1}{2} \\left\\vert2x\\right\\vert^p+\\frac{1}{2}\\left\\vert2y\\right\\vert^p=2^{p-1}\\left(\\left\\vert x\\right\\vert^p+\\left\\vert y\\right\\vert^p\\right)\\). This inequality ensures that \\(L^p\\) is a vector space. Now, using the triangle inequality and H\u00f6lder's inequality for \\(q=p/(p-1)\\), we get</p> \\[ \\begin{align}     \\left\\Vert X+Y\\right\\Vert_p^p &amp;=E\\left[ \\left\\vert X+Y\\right\\vert^p \\right] \\\\     &amp; \\leq E\\left[ \\left\\vert X\\right\\vert\\left\\vert X+Y\\right\\vert^{p-1} \\right]+E\\left[ \\left\\vert Y\\right\\vert\\left\\vert X+Y\\right\\vert^{p-1} \\right] \\\\     &amp;\\leq \\left(E\\left[ \\left\\vert X\\right\\vert^p \\right]^{1/p}+E\\left[ \\left\\vert Y\\right\\vert^p \\right]^{1/p}\\right)E\\left[ \\left\\vert X+Y\\right\\vert^{p} \\right]^{1-1/p} \\\\     &amp;= \\left( \\left\\Vert X\\right\\Vert_p+\\left\\Vert Y\\right\\Vert_p \\right)\\left\\Vert X+Y\\right\\Vert_p^{p-1}. \\end{align} \\] <p>If \\(\\left\\Vert X+Y\\right\\Vert_p=0\\), the inequality is trivial. Otherwise, divide both sides by \\(\\left\\Vert X+Y\\right\\Vert^{p-1}\\).</p> <p>It follows in particular that \\(L^p\\) is a vector space and that \\(\\left\\Vert\\cdot\\right\\Vert_p\\) is a norm on \\(L^p\\). We say that \\(X_n \\to X\\) in \\(L^p\\) for \\((X_n),X\\) in \\(L^p\\) if \\(\\left\\Vert X_n-X\\right\\Vert_p\\to 0\\). The \\(L^p\\) spaces are not only normed space but also Banach spaces (that is complete).</p> <p>Theorem: The \\(L^p\\) Spaces are Banach Spaces</p> <p>Let \\(1\\leq p\\leq \\infty\\) and \\((X_n)\\) be a Cauchy sequence in \\(L^p\\). Then it follows that \\(X_n \\to X\\) in \\(L^p\\) for some \\(X\\) in \\(L^p\\).</p> <p>In particular \\((L^p, \\|\\cdot \\|_p)\\) is a Banach space.</p> <p>Proof</p> <p>We do the proof for \\(p&lt;\\infty\\). Let \\((X_n)\\) be a Cauchy sequence. By Cauchy property, we can take a subsequence \\((Y_n)\\) of \\((X_n)\\) such that \\(\\left\\vert Y_{n+1}-Y_n\\right\\vert\\leq 2^{-n}\\) and define \\(Z_n=\\left\\vert Y_1\\right\\vert+\\sum_{k\\leq n-1} \\left\\vert Y_{k+1}-Y_k\\right\\vert\\) which is an increasing sequence of positive random variables converging to \\(Z=\\sup Z_n\\). Hence, the monotone convergence theorem shows that \\(E[Z^p]=\\lim E[Z_n^p]\\). By Minkowski inequality it holds</p> \\[ \\begin{equation*}     E\\left[ Z_n^p \\right]=\\left\\Vert Z_n\\right\\Vert_p^p\\leq \\left( \\left\\Vert Y_1\\right\\Vert_p+\\sum_{k\\leq n-1}\\left\\Vert Y_{k+1}-Y_k\\right\\Vert_p \\right)^p\\leq \\left( \\left\\Vert Y_1\\right\\Vert_p+1 \\right)^p \\end{equation*} \\] <p>The right-hand side being independent of \\(n\\), it follows by passing to the limit that \\(Z \\in L^p\\) and therefore \\(Z&lt;\\infty\\) \\(P\\)-almost surely.</p> <p>On the other hand, since the absolute series, \\(\\sum \\left\\vert Z_{k+1}-Z_k\\right\\vert\\) converges, it follows that \\(Y_n=Y-1+\\sum_{k\\leq n-1}Y_{k+1}-Y_k\\) converges \\(P\\)-almost surely to some \\(Y\\). Hence, \\(Y=\\lim Y_n\\) is in \\(L^p\\) since \\(\\left\\vert Y\\right\\vert=\\lim \\left\\vert Y_n\\right\\vert\\leq Z \\in L^p\\). We make use of dominated convergence on \\((Y_n)\\) since \\(Y_n^p\\to Y^p\\) \\(P\\)-almost surely and \\(\\left\\vert Y_n\\right\\vert^p\\leq Z^p\\in L^p\\), which implies that \\(E[\\left\\vert Y_n-Y\\right\\vert^p]\\to 0\\). It shows that a subsequence \\((Y_n)\\) of \\((X_n)\\) converges in \\(L^p\\) to some \\(Y\\).</p> <p>As an exercise, using the Cauchy property, show that \\(X_n\\to Y\\) in \\(L^p\\).</p> <p>Definition</p> <p>Let \\((X_n)\\) be a sequence of random variables and \\(X\\) a random variable.</p> <p>We say that</p> <ul> <li>\\(X_n\\to X\\) \\(P\\)-almost surely if \\(P[\\limsup X_n=\\liminf X_n]=1\\).</li> <li>\\(X_n\\to X\\) in probability if \\(\\lim P[\\left\\vert X_n-X\\right\\vert&gt;\\varepsilon]=0\\) for every \\(\\varepsilon&gt;0\\).</li> <li>\\(X_n\\to X\\) in \\(L^p\\) if \\(\\left\\Vert X_n-X\\right\\Vert_p\\to 0\\).</li> </ul> <p>Proposition</p> <p>Let \\((X_n)\\) be a sequence of random variables and \\(X\\) a random variable.   The following assertions hold:</p> <ol> <li>\\(X_n\\to X\\) \\(P\\)-almost surely implies \\(X_n\\to X\\) in probability.</li> <li>\\(X_n\\to X\\) in probability implies that \\(Y_n\\to X\\) \\(P\\)-almost surely for some subsequence \\((Y_n)\\) of \\((X_n)\\).</li> <li>\\(X_n\\to X\\) in \\(L^p\\) implies that \\(Y_n\\to X\\) \\(P\\)-almost surely for some subsequence \\((Y_n)\\) of \\((X_n)\\).</li> <li>\\(X_n\\to X\\) in probability and \\(\\left\\vert X_n\\right\\vert\\leq Y\\) for some \\(Y \\in L^1\\) implies \\(X_n\\to X\\) in \\(L^1\\).</li> </ol> <p>Proof</p> <p>Homework sheet.</p> <p>Theorem: Chebyshev/Markov Inequality</p> <p>Let \\(X\\) be a random variable and \\(\\varepsilon&gt;0\\). For every \\(0&lt;p&lt;\\infty\\), the Chebyshev inequality reads</p> \\[ \\begin{equation*}     P\\left[ \\left\\vert X\\right\\vert\\geq \\varepsilon \\right]\\leq \\frac{1}{\\varepsilon^p}E\\left[ \\left\\vert X\\right\\vert^p \\right]. \\end{equation*} \\] <p>In the case where \\(p=1\\), the inequality is due to Markov.</p> <p>Proof</p> <p>Define \\(A_{t}=\\{\\left\\vert X\\right\\vert\\geq t\\}\\) and \\(g(x)=x^p\\) which is an increasing function, so that consequently yields \\(0\\leq g(\\varepsilon)1_{A_\\varepsilon}\\leq g(\\left\\vert X\\right\\vert)1_{A_\\varepsilon}\\).</p> <p>Thus, \\(0\\leq g(\\varepsilon)P[A_\\varepsilon]=E[g(\\varepsilon)1_{A_\\varepsilon}]\\leq E[g(\\left\\vert X\\right\\vert)1_{A_\\varepsilon}]\\leq E[g(\\left\\vert X\\right\\vert)]\\) which ends the proof.</p> <p>Remark</p> <p>Note the proof of Markov's inequality shows that the following inequality holds</p> \\[ \\begin{equation*}     P\\left[ \\left\\vert X\\right\\vert\\geq \\varepsilon \\right]\\leq \\frac{1}{g(\\varepsilon)}E\\left[g\\left( \\left\\vert X\\right\\vert \\right)  \\right] \\end{equation*} \\] <p>for every increasing and measurable function \\(g:\\mathbb{R}\\to \\mathbb{R}\\) such that \\(g(\\varepsilon)&gt;0\\).</p> <ol> <li> <p>An equivalence relation \\(\\sim\\) is a binary relation that is symmetric, that is, \\(x\\sim y\\) if and only if \\(y\\sim x\\), reflexive, that is, \\(x\\sim x\\), and transitive, that is, \\(x\\sim y\\) and \\(y\\sim z\\) implies \\(x\\sim z\\).\u00a0\u21a9</p> </li> <li> <p>Verify that defined as such, \\(Q\\) is a probability measure, that is, \\(Q(A)=\\int_A dQ=\\int_A Y^q/E[Y^q]dP=E[1_A Y^q/E[Y^q]]\\) is a \\(\\sigma\\)-additive measure and it holds \\(E_Q[Z]=E[Z Y^q/E[Y^q]]\\). See las exercise of the previous section.\u00a0\u21a9</p> </li> </ol>"},{"location":"lecture/03-Integration/033-radon-nikodym-cond-exp/","title":"Radon-Nykodym and Conditional Expectation","text":""},{"location":"lecture/03-Integration/033-radon-nikodym-cond-exp/#radon-nykodym-change-of-measure","title":"Radon-Nykodym Change of Measure","text":"<p>In this section, we will make use of a central theorem of Functional Analysis applied in the special case of Hilbert spaces.</p> <p>Note</p> <p>A (real) Hilbert space is a vector space with a bilinear form \\(\u27e8\\cdot,\\cdot\u27e9:H\\times H \\to \\mathbb{R}\\), that is linear in the first as well as in the second argument, such that \\(\u27e8x,y\u27e9 =\u27e8y,x\u27e9\\) and \\(\u27e8x,x\u27e9 \\geq 0\\) with equality if and only if \\(x=0\\).</p> <p>Hence, \\(\\left\\Vert x\\right\\Vert=\\left\\vert\\langle x,y\\rangle\\right\\vert^{1/2}\\) defines a norm on \\(H\\) due to the Cauchy-Schwarz inequality that states that \\(\\left\\vert\\langle x,y\\rangle\\right\\vert\\leq \\left\\Vert x\\right\\Vert\\left\\Vert y\\right\\Vert\\).</p> <p>The finite-dimensional vector space \\(\\mathbb{R}^d\\) is a Hilbert space for the dot product \\(\\langle x,y\\rangle=\\sum_{k\\leq d} x_ky_k\\) and defines the Euclidean norm \\(\\left\\Vert x\\right\\Vert=\\sqrt{\\sum_{k\\leq d}x_k^2}\\).</p> <p>More importantly in our case, given a finite measure \\(P\\), the space \\(L^2\\) with the bilinear form \\(\u27e8X,Y\u27e9=\\int XY dP\\) is, due to H\u00f6lder's inequality</p> \\[ \\int \\left\\vert XY\\right\\vert dP\\leq \\left( \\int \\left\\vert X\\right\\vert^2dP \\right)^{1/2} \\left( \\int \\left\\vert Y\\right\\vert^2dP \\right)^{1/2}, \\] <p>a Hilbert space with resulting norm \\(\\left\\Vert X\\right\\Vert_{L^2(P)}=\\left(\\int X^2 dP\\right)^{1/2}\\), which is the \\(L^2\\) norm as defined before.</p> <p>Recall that a linear functional \\(T:H\\to \\mathbb{R}\\) where \\((H,\\langle \\cdot,\\cdot\\rangle)\\) is a Hilbert space is continuous if and only if</p> \\[ \\sup_{\\left\\Vert x\\right\\Vert=\\langle x,x\\rangle \\leq 1}\\left\\vert T(x)\\right\\vert&lt;\\infty \\] <p>Riesz Representation Theorem</p> <p>Let \\(H\\) be a Hilbert space, and \\(T:H\\to \\mathbb{R}\\) be a continuous linear functional. Then there exists \\(y\\) in \\(H\\) such that \\(T(x)=\\langle y,x\\rangle\\) for every \\(x\\) in \\(H\\).</p> <p>This theorem allows us to treat the following central theorem of measure theory in a rather simple way.</p> <p>Radon-Nikodym Theorem</p> <p>Let \\((\\Omega, \\mathcal{F})\\) be a measurable space and \\(P, Q\\) two finite measures on \\(\\mathcal{F}\\) such that \\(Q \\ll P\\). Then there exists a \\(P\\)-almost surely unique and positive random variable \\(Z\\) in \\(L^1(P)\\) such that</p> \\[ Q\\left( A \\right)=\\int_{A}Z dP, \\quad \\text{for every }A \\in \\mathcal{F} \\] <p>The random variable \\(Z\\) is called the Radon-Nikodym derivative of \\(Q\\) with respect to \\(P\\) and denoted by \\(dQ/dP\\).</p> <p>In particular, it holds</p> \\[ \\int X dQ = \\int X \\frac{dQ}{dP} dP, \\quad \\text{for every }X \\text{ in }L^1(Q). \\] <p>Proof</p> <p>The proof is based on the argumentation of John von Neumann. The measure</p> \\[ \\tilde{Q}[A]=P[A]+Q[A] \\quad \\text{for } A \\in \\mathcal{F} \\] <p>is such that \\(\\tilde{Q}\\) is equivalent to \\(P\\). Indeed, for any event \\(A\\), on the one hand, \\(P[A]=0\\) implies that \\(Q[A]=0\\) and therefore \\(\\tilde{Q}[A]=P[A]+Q[A]=0\\). On the other hand, the positivity of \\(Q\\) and \\(P\\) implies \\(P[A]=0\\) whenever \\(\\tilde{Q}[A]=0\\), showing \\(P\\sim \\tilde{Q}\\). The equivalence between \\(P\\) and \\(\\tilde{Q}\\) implies in particular that two random variables \\(X\\) and \\(Y\\) agree \\(P\\)-almost surely if and only if they agree \\(Q\\)-almost surely. Hence, \\(L^0(P)=L^0(\\tilde{Q})\\). For any random variable \\(X\\) in \\(L^0(P)=L^0(\\tilde{Q})\\), it holds</p> \\[ \\left\\Vert X\\right\\Vert_{L^2(P)}^2= \\int X^2 dP\\leq \\int X^2 d(P+Q)=\\left\\Vert X\\right\\Vert_{L^2(\\tilde{Q})}^2 \\] <p>showing that \\(L^2(\\tilde{Q})\\subseteq L^2(P)\\).</p> <p>Define now the linear functional</p> \\[ T\\colon L^2(\\tilde{Q})\\to \\mathbb{R}, \\quad X \\mapsto T(X)=\\int X dP \\] <p>which is well-defined since \\(L^2(\\tilde{Q})\\subseteq L^2(P)\\). Using Jensen's inequality and the fact that \\(\\tilde{Q}/\\tilde{Q}(\\Omega)\\) is a probability measure, it holds</p> \\[ \\left| T(X) \\right|\\leq \\sqrt{\\tilde{Q}(\\Omega)}\\left\\Vert X\\right\\Vert_{L^2(\\tilde{Q})}. \\] <p>As a consequence,</p> \\[ \\sup_{X \\in L^2(\\tilde{Q}), \\left\\Vert X\\right\\Vert_{L^2(\\tilde{Q})}\\leq 1} \\left\\vert T(X)\\right\\vert\\leq \\sqrt{\\tilde{Q}(\\Omega)}&lt;\\infty, \\] <p>showing that \\(T\\) is a continuous linear functional on the Hilbert space \\(L^2(\\tilde{Q})\\). Applying the Riesz Representation Theorem, there exists \\(Y\\) in \\(L^2(\\tilde{Q})\\) such that</p> \\[ T(X)=\\langle X, Y\\rangle = \\int XY d\\tilde{Q},\\quad \\text{for every }X\\in L^2(\\tilde{Q}). \\] <p>Taking \\(X=1_A\\) for \\(A=\\{Y\\leq 0\\}\\), it follows that \\(P[A]=0\\), showing that \\(Y&gt;0\\) \\(P\\)-almost surely. Similarly, we obtain \\(0&lt;Y\\leq 1\\) \\(\\tilde{Q}\\)- and \\(P\\)-almost surely. Defining \\(Z=1/Y-1\\), which is a positive measurable function in \\(L^1(P)\\), it follows that</p> \\[ Q\\left( A \\right)=\\tilde{Q}(A)-Q(A)=\\int_A Z dP, \\quad \\text{for every } A \\in \\mathcal{F} \\] <p>which ends the proof of existence.</p> <p>Uniqueness is left as an exercise.</p>"},{"location":"lecture/03-Integration/033-radon-nikodym-cond-exp/#conditional-expectation","title":"Conditional Expectation","text":"<p>The Radon-Nikodym Theorem allows us to prove easily the existence of conditional expectations.</p> <p>Theorem: Conditional Expectation</p> <p>Let \\((\\Omega,\\mathcal{F},P)\\) be a probability space and \\(\\mathcal{G}\\subseteq \\mathcal{F}\\) be a sub-\\(\\sigma\\)-algebra. For every integrable random variable \\(X\\), there exists a \\(P\\)-almost surely unique \\(\\mathcal{G}\\)-measurable and integrable random variable \\(Y\\) such that</p> \\[ E\\left[ 1_A X \\right]=E\\left[ 1_A Y \\right],\\quad \\text{for every }A \\in \\mathcal{G} \\] <p>Denoting \\(E[X|\\mathcal{G}]:=Y\\), provided all the following random variables are all in \\(L^1\\), it holds:</p> <ol> <li>\\(X\\mapsto E[X | \\mathcal{G}]\\) is linear, monotone, and \\(E[c|\\mathcal{G}]=c\\) for every constant.</li> <li>\\(E[X|\\mathcal{G}]=E[X]\\) if \\(\\mathcal{G}=\\{\\emptyset, \\Omega\\}\\).</li> <li>\\(E[X_n|\\mathcal{G}]\\nearrow E[X|\\mathcal{G}]\\) whenever \\(0\\leq X_n\\nearrow X\\).</li> <li>\\(E[YX|\\mathcal{G}]=YE[X|\\mathcal{G}]\\) and \\(E[Y|\\mathcal{G}]=Y\\) whenever \\(Y\\) is \\(\\mathcal{G}\\)-measurable.</li> <li>\\(E[E[X|\\mathcal{G}_2]|\\mathcal{G}_1]=E[X|\\mathcal{G}_1]\\) for every two \\(\\sigma\\)-algebras \\(\\mathcal{G}_1\\subseteq \\mathcal{G}_2\\subseteq \\mathcal{F}\\).</li> <li>\\(\\varphi(E[X|\\mathcal{G}])\\leq E[\\varphi(X)|\\mathcal{G}]\\) if \\(\\varphi\\) is convex with \\(\\varphi(X)\\in L^1\\).</li> <li>\\(E[\\left\\vert E[X|\\mathcal{G}]\\right\\vert]\\leq E[\\left\\vert X\\right\\vert]\\).</li> <li>\\(E[\\liminf X_n|\\mathcal{G}]\\leq \\liminf E[X_n|\\mathcal{G}]\\) if \\(X_n\\geq Y \\in L^1\\) and \\(\\liminf X_n \\in L^1\\).</li> <li>\\(E[X|\\mathcal{G}]=\\lim E[X_n |\\mathcal{G}]\\) if \\(X_n \\to X\\) almost surely and \\(|X_n|\\leq Y\\in L^1\\).</li> <li>\\(E[XE[Y|\\mathcal{G}]]=E[E[X|\\mathcal{G}]Y]=E[E[X|\\mathcal{G}]E[Y|\\mathcal{G}]]\\).</li> </ol> <p>This unique random variable is called the \\(\\mathcal{G}\\)-conditional expectation of \\(X\\), and is denoted by \\(E[X|\\mathcal{G}]\\).</p> <p>Proof</p> <p>For \\(X\\) in \\(L^1\\), it defines two finite measures on \\(\\mathcal{G}\\) given by</p> \\[ Q^{\\pm}(A)=E\\left[ 1_A X^{\\pm}\\right], \\quad A \\in \\mathcal{G} \\] <p>which are by definition both absolutely continuous with respect to \\(P\\). It follows from the Radon-Nikodym Theorem {#thm:radonnikodym} that there exist two \\(P\\)-almost surely unique positive \\(\\mathcal{G}\\)-measurable random variables \\(Z^{\\pm}\\in L^1(\\mathcal{G})\\) such that</p> \\[ Q^{\\pm}(A)=E[1_A Z^{\\pm}]. \\] <p>Defining \\(E\\left[ X|\\mathcal{G} \\right]=Z^+-Z^-\\in L^1(\\mathcal G)\\) as the conditional expectation ends the proof of existence and uniqueness.</p> <p>In the following, we assume that the integrability conditions are sufficient for the assertions.</p> <ul> <li>Point 1: Follows immediately from the linearity and monotonicity of the expectation.</li> <li>Point 2: Let \\(Y=E[X|\\mathcal{G}]\\). Since \\(\\mathcal{G}\\) is the trivial \\(\\sigma\\)-algebra, it follows that \\(Y\\) is constant. Furthermore, \\(E[Y1_\\Omega]=E[X1_{\\Omega}]=E[X]\\), hence \\(Y=E[X]\\) per definition of the conditional expectation.</li> <li> <p>Point 3: For \\(X_n \\nearrow X\\) almost surely, by monotonicity of the conditional expectation, it follows that \\(E[X_n|\\mathcal{G}]\\leq E[X|\\mathcal{G}]\\) and is an increasing sequence whose limit is denoted by \\(Y\\).     For \\(A=\\{Y &lt; E[X|\\mathcal{G}]\\}\\), which is in \\(\\mathcal{G}\\), we have by monotone convergence</p> \\[ E[ 1_A (E[ X|\\mathcal{G} ]-Y) ]=\\lim E[ 1_A E[X-X_n|\\mathcal{G}] ]=\\lim E[ X-X_n]=0 \\] <p>showing that \\(P[A]=0\\) since \\(E[X|\\mathcal{G}]\\geq Y\\).</p> </li> <li> <p>Point 4: For \\(Y=1_A\\) with \\(A \\in \\mathcal{G}\\), it holds</p> \\[ E[ 1_BE[ YX|\\mathcal{G} ]]=E[ 1_B YX ]=E[ 1_{B\\cap A}X ]=E[ 1_{B\\cap A}E[ X|\\mathcal{G} ]]=E[1_B YE[ X|\\mathcal{G} ]]. \\] <p>Since \\(YE[X|\\mathcal{G}]\\) is \\(\\mathcal{G}\\)-measurable, by uniqueness of the conditional expectation, we get \\(YE[X|\\mathcal{G}]=E[YX|\\mathcal{G}]\\) for every \\(Y\\) of the form \\(Y=1_A\\) with \\(A\\) in \\(\\mathcal{G}\\). By linearity of the conditional expectation, it holds for every simple step \\(\\mathcal{G}\\)-measurable \\(Y\\). By approximating from below by simple step functions and using Point 3, it holds for any positive \\(\\mathcal{G}\\)-measurable random variable \\(Y\\).</p> <p>The general case follows from the linearity of the conditional expectation and the decomposition \\(Y=Y^+-Y^-\\).</p> </li> <li> <p>Point 5: Let \\(A\\) in \\(\\mathcal{G}_1\\subseteq \\mathcal{G}_2\\).     By the definition of the conditional expectation applied twice, it follows that</p> \\[ E[1_A E[E[X|\\mathcal{G}_2]|\\mathcal{G}_1]]=E[1_A E[ X|\\mathcal{G}_2]]=E[1_AX] \\] <p>showing that \\(E[E[X|\\mathcal{G}_2]|\\mathcal{G}_1]=E[X|\\mathcal{G}_1]\\).</p> </li> <li> <p>Point 6: This follows from Jensen's inequality using the fact that the conditional expectation is linear and monotone, plus monotone convergence from Point 3.</p> </li> <li>Point 7: Applying Point 6 for \\(\\varphi(x)=|x|\\).</li> <li>Point 8: For the Fatou assertion, it follows from Point 3 by considering the increasing sequence \\(Z_n=\\sup_{k\\geq n}X_k\\).</li> <li>Point 9: Follows from the dominated convergence theorem.</li> </ul> <p>For your interest, here is the proof of the existence of conditional expectation using Hilbert projections.</p> <p>Proof</p> <p>Suppose first that \\(X \\in L^2(\\mathcal{F})\\). Note that \\(L^2(\\mathcal{F})\\) is a Hilbert space for the norm \\(\\left\\Vert\\cdot\\right\\Vert_2\\) and \\(L^2(\\mathcal{G})\\) is a closed linear subspace of \\(L^2(\\mathcal{F})\\). Hence, by Hilbert's projection theorem, there exists a unique \\(Y \\in L^2(\\mathcal{G})\\) such that \\(X-Y\\) is orthogonal to \\(L^2(\\mathcal{G})\\). Since \\(1_A \\in L^2(\\mathcal{G})\\) for every \\(A \\in \\mathcal{G}\\), it follows that</p> \\[ E\\left[ (X-Y)1_A \\right]=\\langle X-Y,1_A\\rangle=0, \\quad A \\in \\mathcal{G} \\] <p>showing the main assertion.</p>"},{"location":"lecture/03-Integration/034-independence/","title":"Independence","text":"<p>Unlike the previous section that do hold in large part for measures without considering the special case of probability, independence is a concept that is genuinly cored into probability.</p> <p>Throughout we fix a probability space \\((\\Omega, \\mathcal{F}, P)\\).</p> <p>Definition: Independence</p> <ul> <li> <p>Independence of families:     A family \\((\\mathcal{C}^i)\\) of collections of events is called independent if for every finite collection \\((A_{i_k})_{k\\leq n}\\) with \\(A_{i_k}\\) event in \\(\\mathcal{C}^{i_k}\\) for every \\(k=1,\\ldots, n\\), it holds</p> \\[ P\\left[ A_{i_1}\\cap \\cdots\\cap A_{i_n} \\right]=\\prod_{k\\leq n} P\\left[ A_{i_k} \\right]. \\] </li> <li> <p>Independence of events:     A family of events \\((A_i)\\) is called independent if the family \\((\\{A_i\\})\\) is independent.</p> </li> <li> <p>Independence of random variables:     A family of random variables \\((X_i)\\) is called independent if \\((\\sigma(X_i))\\) are independent.</p> </li> </ul> <p>Remark</p> <p>A family \\((\\mathcal{C}^i)\\) is called pairwise independent if</p> \\[ P[A_{i_1}\\cap A_{i_2}]=P[A_{i_1}]P[A_{i_2}] \\] <p>for every \\(A_{i_1}\\in \\mathcal{C}^{i_1}\\), \\(A_{i_2}\\in \\mathcal{C}^{i_2}\\), which is a weaker version of independence. As an exercise, find three sets \\(A,B\\) and \\(C\\) on some probability space which are pairwise independent but not independent.</p> <p>Proposition</p> <p>The following assertions hold:</p> <ol> <li>Let \\(\\mathcal{P}_1, \\ldots, \\mathcal{P}_n\\) be a finite family of independent \\(\\pi\\)-systems.    Then \\(\\sigma(\\mathcal{P}_1), \\ldots,\\sigma(\\mathcal{P}_n)\\) are also independent.</li> <li> <p>Let \\(X,Y\\) be two independent random variables, either positive or such that \\(X,Y, XY \\in L^1\\), then it follows that</p> \\[ E[XY]=E[X]E[Y] \\] </li> <li> <p>Let \\(X\\) be a random variable independent of a sigma-algebra \\(\\mathcal{G}\\).     Then it holds</p> \\[   E\\left[ X|\\mathcal{G} \\right]=E\\left[ X \\right] \\] </li> </ol> <p>Proof</p> <ol> <li> <p>We show the case \\(n=2\\), the general one is done by induction.     Let \\(\\mathcal{C}_1\\) be the collection of elements \\(A_1\\) in \\(\\sigma(\\mathcal{P}_1)\\) for which it holds</p> \\[ P\\left[ A_1\\cap A_2 \\right]=P[A_1]P[A_2] \\quad \\text{for every }A_2 \\in \\mathcal{P}_2. \\] <p>Let us show that \\(\\mathcal{C}_1\\) is a \\(\\lambda\\)-system. Clearly, \\(\\Omega \\in \\mathcal{C}_1\\). Let \\(A_1 \\in \\mathcal{C}_1\\) and \\(A_2\\in \\mathcal{P}_2\\).</p> <p>It follows that</p> \\[ P[A_1^c\\cap A_2]=P\\left[ A_2\\right]-P[A_1\\cap A_2]=P[A_2]-P[A_1]P[A_2]=(1-P[A_1])P[A_2]=P[A_1^c]P[A_2], \\] <p>showing that \\(A_1^c\\in \\mathcal{C}_1\\). Finally, let \\((A_1^n)\\) be a sequence of pairwise disjoint elements in \\(\\mathcal{C}_1\\) and \\(A_2\\) in \\(\\mathcal{P}_2\\). By \\(\\sigma\\)-additivity of probability measures, it holds</p> \\[   P\\left[ \\left( \\cup A_1^n \\right)\\cap A_2 \\right]=\\sum P\\left[ A_1^n\\cap A_2 \\right]=\\sum P[A_1^n]P[A_2]=\\left(\\sum P[A_1^n]\\right)P[A_2]=P\\left[ \\cup A_1^n \\right]P[A_2], \\] <p>showing that \\(\\cup A_1^n\\in \\mathcal{C}_1\\). We deduce that \\(\\mathcal{C}_1\\) is a \\(\\lambda\\)-system containing the \\(\\pi\\)-system \\(\\mathcal{P}_1\\). Since \\(\\sigma(\\mathcal{P}_1)\\subseteq \\sigma(\\mathcal{C}_1)=\\mathcal{C}_1\\subseteq \\sigma(\\mathcal{P}_1)\\), it follows that \\(\\mathcal{C}_1=\\sigma(\\mathcal{P}_1)\\). Now let \\(\\mathcal{C}_2\\) be the set of those \\(A_2\\) in \\(\\sigma(\\mathcal{P}_2)\\) such that</p> \\[   P\\left[ A_1\\cap A_2 \\right]=P[A_1]P[A_2] \\quad \\text{for every }A_1 \\in \\sigma(\\mathcal{P}_1). \\] <p>Since \\(\\sigma(\\mathcal{P}_1)\\) is independent of \\(\\mathcal{P}_2\\), the same argumentation as above shows that \\(\\mathcal{C}_2\\) is a \\(\\lambda\\)-system.</p> <p>Therefore, \\(\\mathcal{C}_2=\\sigma(\\mathcal{P}_2)\\) showing that \\(\\sigma(\\mathcal{P}_1)\\) is independent of \\(\\sigma(\\mathcal{P}_2)\\).</p> </li> <li> <p>By assumption, \\(\\sigma(X)\\) is independent of \\(\\sigma(Y)\\).     Assume that \\(X\\) and \\(Y\\) are positive.     Let \\(\\tilde{X}=\\sum_{k\\leq n} \\alpha_k 1_{A_k}\\) and \\(\\tilde{Y}=\\sum_{l\\leq m}\\beta_l 1_{B_l}\\) for \\(\\alpha_k,\\beta_l\\) positives and \\(A_k\\in \\sigma(X)\\) and \\(B_l\\in \\sigma(Y)\\).     It follows that</p> \\[   E\\left[ \\tilde{X}\\tilde{Y} \\right]=\\sum_{k\\leq n,l\\leq m}\\alpha_k \\beta_l P[A_k\\cap B_l]=\\sum_{k\\leq n,l\\leq m}\\alpha_k\\beta_l P[A_k]P[B_l]. \\] <p>Since there exist sequences \\(\\tilde{X}_n,\\tilde{Y}_n\\) such that \\(\\tilde{X}_n\\nearrow X\\) and \\(\\tilde{Y}_n\\nearrow Y\\), it follows from Lebesgue's monotone convergence that</p> \\[   E\\left[ XY \\right]=\\lim E\\left[ \\tilde{X}_n\\tilde{Y}_n \\right]=\\lim E\\left[ \\tilde{X}_n \\right]E\\left[ \\tilde{Y}_n \\right]=E[X]E[Y]. \\] <p>The case where \\(X,Y,XY\\) are in \\(L^1\\) follows by separating positive and negative parts.</p> </li> <li> <p>Let \\(X\\) be an integrable random variable independent of the \\(\\sigma\\)-algebra \\(\\mathcal{G}\\).     For every \\(A\\in \\mathcal{G}\\), it follows that \\(1_A\\) and \\(X\\) are independent, and therefore, from the previous point, it holds</p> \\[   E\\left[ X1_{A} \\right] = E\\left[ X \\right]E\\left[ 1_A \\right]. \\] <p>But on the other hand,</p> \\[ E\\left[ E\\left[ X \\right]1_{A} \\right]=E\\left[ X \\right]E\\left[ 1_A \\right]. \\] <p>Since \\(E[X]\\) is a constant and \\(\\mathcal{G}\\)-measurable, it follows from the uniqueness of the conditional expectation that</p> \\[   E[X|\\mathcal{G}]=E[X]. \\] </li> </ol>"},{"location":"lecture/03-Integration/035-fubini-tonelli/","title":"Fubini-Tonelli, Stochastic Kernel","text":"<p>The theorem of Fubini-Tonelli is concerned with the definition of sound product measures on finite product spaces and their properties. To do so, we will make use of Carath\u00e9odory's extension Theorem. In the following, we consider the two-dimensional case.</p> <p>Let \\((\\Omega_1,\\mathcal{F}_1)\\) and \\((\\Omega_2, \\mathcal{F}_2)\\) be two measurable spaces and define \\(\\Omega =\\Omega_1 \\times \\Omega_2\\) endowed with the product \\(\\sigma\\)-algebra \\(\\mathcal{F}=\\mathcal{F}_1\\otimes \\mathcal{F}_2\\).</p> <p>Proposition</p> <p>Let \\(A\\) be an event in the product \\(\\sigma\\)-algebra \\(\\mathcal{F}=\\mathcal{F}_1\\otimes \\mathcal{F}_2\\) and \\(X\\colon \\Omega \\to \\mathbb{R}\\) be a measurable function. Define</p> \\[ \\begin{align}     A_{\\omega_1} &amp; :=\\{\\omega_2\\in \\Omega_2\\colon (\\omega_1,\\omega_2)\\in A\\}\\subseteq \\Omega_2,  &amp;\\quad A_{\\omega_2} &amp; :=\\{\\omega_1\\in \\Omega_1\\colon (\\omega_1,\\omega_2)\\in A\\}\\subseteq \\Omega_1,\\\\     X_{\\omega_1} &amp; :=X(\\omega_1,\\cdot):\\Omega_2\\to \\mathbb{R},  &amp;\\quad X_{\\omega_2} &amp; :=X(\\cdot,\\omega_2):\\Omega_1\\to \\mathbb{R}. \\end{align} \\] <p>Then it holds that \\(A_{\\omega_1}\\) and \\(A_{\\omega_2}\\) are events in \\(\\mathcal{F}_2\\) and \\(\\mathcal{F}_1\\) respectively. Furthermore, \\(X_{\\omega_1}\\) is \\(\\mathcal{F}_2\\)-measurable, and \\(X_{\\omega_2}\\) is \\(\\mathcal{F}_1\\)-measurable.</p> <p>Proof</p> <p>Let \\(\\mathcal{C}\\) be the collection of those \\(A\\in\\mathcal{F}\\) such that \\(A_{\\omega_2}\\times A_{\\omega_1}\\) is in \\(\\mathcal{F}_1\\times \\mathcal{F}_2\\). It clearly holds that \\(\\mathcal{F}_1\\times \\mathcal{F}_2\\subseteq \\mathcal{C}\\). Direct inspection shows that \\(\\mathcal{C}\\) is a \\(\\sigma\\)-algebra, and therefore </p> \\[   \\mathcal{F}=\\sigma(\\mathcal{F}_1\\times \\mathcal{F}_2)\\subseteq \\mathcal{C}\\subseteq \\mathcal{F}, \\] <p>showing the first assertion.</p> <p>As for the second point, let \\(B\\) be a Borel set in \\(\\mathbb{R}\\). It follows that \\(\\{X_{\\omega_1}\\in B\\}=\\{X\\in B\\}_{\\omega_1}\\), which is an element of \\(\\mathcal{F}_2\\) by what has just been shown. Hence, \\(X_{\\omega_1}\\) is \\(\\mathcal{F}_2\\)-measurable. The same argumentation holds for \\(X_{\\omega_2}\\).</p> <p>Definition: Stochastic Kernel</p> <p>A stochastic kernel on \\(\\Omega_1\\times \\mathcal{F}_2\\) is a function \\(K\\colon \\Omega_1\\times \\mathcal{F}_2\\to [0,1]\\) such that</p> <ol> <li>\\(\\omega_1\\mapsto K(\\omega_1,A_2)\\) is \\(\\mathcal{F}_1\\)-measurable for every event \\(A_2\\) in \\(\\mathcal{F}_2\\).</li> <li>\\(A_2\\mapsto K(\\omega_1,A_2)\\) is a probability measure on \\(\\mathcal{F}_2\\) for every \\(\\omega_1 \\in \\Omega_1\\).</li> </ol> <p>A stochastic kernel is, in some sense, a measurable family of probability measures on \\(\\mathcal{F}_1\\), one for each state \\(\\omega_1\\) in \\(\\Omega_1\\). A special case of a stochastic kernel is the constant one \\(K(\\omega_1,\\cdot)=P_2\\) for all states \\(\\omega_1\\) in \\(\\Omega_1\\), where \\(P_2\\) is a probability measure on \\(\\mathcal{F}_2\\).</p> <p>Given a probability measure \\(P_1\\) on \\(\\mathcal{F}_1\\), we want to define a probability measure \\(P\\) on the product \\(\\sigma\\)-algebra \\(\\mathcal{F}\\) such that</p> \\[ P[A]=\\int_{\\Omega_1}\\left( \\int_{\\Omega_2}1_A(\\omega_1,\\omega_2)K(\\omega_1,d\\omega_2) \\right)P_1(d\\omega_1). \\] <p>This is the subject of the following theorem.</p> <p>Stochastic variant of Tonelli's Theorem</p> <p>Let \\(P_1\\) be a measure on \\(\\mathcal{F}_1\\) and \\(K\\) a stochastic kernel on \\(\\Omega_1\\times \\mathcal{F}_2\\). Then there exists a unique probability measure \\(P\\) on \\(\\mathcal{F}\\) such that for every positive random variable \\(X\\colon \\Omega \\to \\mathbb{R}\\), it holds</p> \\[   E_P\\left[ X \\right]=\\int_{\\Omega_1}\\left( \\int_{\\Omega_2} X(\\omega_1,\\omega_2)K(\\omega_1,d\\omega_2) \\right)P_1(d\\omega_1) \\] <p>In particular,</p> \\[   P[A]=\\int_{\\Omega_1}K(\\omega_1, A_{\\omega_1})P_1(d\\omega_1) \\] <p>for any event \\(A\\) in \\(\\mathcal{F}\\).</p> <p>Proof</p> <p>Define \\(\\mathcal{R}=\\mathcal{F}_1\\times \\mathcal{F}_2\\) and the set function \\(P\\colon \\mathcal{R}\\to [0,1]\\) given by</p> \\[   P[A]=\\int_{A_1}K(\\omega_1,A_2)P_1(d\\omega_1) \\] <p>For any element \\(A_1 \\times A_2\\) in $\\mathcal{R}. Inspection shows that \\(\\mathcal{R}\\) is a semi-ring that contains \\(\\Omega\\). To apply Carath\u00e9odory\u2019s extension Theorem, we must show that \\(P\\) is a \\(\\sigma\\)-additive pre-measure.</p> <p>It clearly holds that \\(P[\\emptyset]=0\\) and </p> \\[   P[\\Omega]=\\int_{\\Omega_1}K(\\omega_1,\\Omega_2)P_1[d\\omega_1]=\\int_{\\Omega_1}P_1[d\\omega_1]=1. \\] <p>Let \\((A_1^n\\times A_2^n)\\) be a sequence of pairwise disjoint elements of \\(\\mathcal{R}\\) such that \\(\\cup A_1^n\\times A_2^n=A_1\\times A_2\\) in \\(\\mathcal{R}\\) for some \\(A_1\\in \\mathcal{F}_1\\) and \\(A_2 \\in \\mathcal{F}_2\\) and define the functions</p> \\[ \\begin{aligned}     X_n(\\omega_1)&amp;:=1_{A_1^n}(\\omega_1)K(\\omega_1,A_2^n)=\\int_{\\Omega_2}1_{A_1^n\\times A_2^n}(\\omega_1,\\omega_2)K(\\omega_1,d\\omega_2),\\\\     X(\\omega_1)&amp;:=1_{A_1}(\\omega_1)K(\\omega_1,A_2)=\\int_{\\Omega_2}1_{A_1\\times A_{2}}(\\omega_1,\\omega_2)K(\\omega_1,d\\omega_2). \\end{aligned} \\] <p>Furthermore, due to the pairwise disjointness of \\((A_1^n\\times A_2^n)\\), as well as monotone convergence, it follows that</p> \\[ \\sum X_n(\\omega_1)=\\int_{\\Omega_1}\\sum 1_{A_1^n\\times A_2^n}(\\omega_1,\\omega_2)K(\\omega_1,d\\omega_2)=\\int_{\\Omega_2}1_{\\cup A_1^n\\times A_2^n}K(\\omega_1,d\\omega_2)=X(\\omega_1) \\] <p>for any state \\(\\omega_1\\) in \\(\\Omega_1\\).   Hence, once again, monotone convergence yields</p> \\[   P[A_1\\times A_2]=\\int_{\\Omega_1}X(\\omega_1)P_1(d\\omega_1)=\\sum\\int_{\\Omega_1}X_n(\\omega_1)P_1(d\\omega_1)=\\sum P[A_1^n\\times A_2^n]. \\] <p>showing the \\(\\sigma\\)-additivity.</p> <p>It follows that we can apply Carath\u00e9odory's extension Theorem, ensuring the existence of a unique measure \\(P\\) on \\(\\mathcal{F}\\) satisfying</p> \\[   P[A_1\\times A_2]=\\int_{A_1}K(\\omega_1,A_2)P_{1}(d\\omega_1), \\quad A_1 \\times A_2 \\in \\mathcal{F}_1\\times \\mathcal{F}_2. \\] <p>Let us now show that</p> \\[   P[A]=\\int_{\\Omega_1}K(\\omega_1, A_{\\omega_1})P_1(d\\omega_1) \\] <p>holds. Define the collection \\(\\mathcal{C}\\) of those events \\(A\\) in \\(\\mathcal{F}\\) such that this relation holds.   For \\(A=A_1\\times A_2\\), it follows that \\(A_{\\omega_1}=A_2\\) if \\(\\omega_1 \\in A_1\\) and \\(\\emptyset\\) otherwise. It follows that </p> \\[   K(\\omega_1,A_{\\omega_1})=1_{A_1}(\\omega_1)K(\\omega_1,A_2), \\] <p>showing that \\(\\mathcal{F}_1\\times \\mathcal{F}_2\\subseteq \\mathcal{C}\\).   In particular, \\(\\Omega \\in \\mathcal{C}\\). Furthermore, for every pairwise disjoint sequence \\((A^n)\\) of elements in \\(\\mathcal{C}\\), denoting \\(A=\\cup A^n\\), it follows from monotone convergence that </p> \\[ \\begin{align}     P\\left[ A\\right]&amp;=\\sum P[A^n]     =\\int_{\\Omega_1}\\sum K(\\omega_1,A_{\\omega_1}^n)P_1(d\\omega_1)\\\\     &amp;=\\int_{\\Omega_1}K(\\omega_1,\\cup A_{\\omega_1}^n)P(d\\omega_1)     =\\int_{\\Omega_1}K(\\omega_1,A_{\\omega_1})P(d\\omega_1) \\end{align} \\] <p>showing that \\(A \\in \\mathcal{C}\\). Finally, for \\(A \\in \\mathcal{C}\\), it follows that</p> \\[   P[A^c]=1-P[A] =\\int_{\\Omega_1} \\left( 1-K(\\omega_1,A_{\\omega_1}) \\right)P(d\\omega_1) =\\int_{\\Omega_1} K(\\omega_1,A_{\\omega_1}^c)P(d\\omega_1) \\] <p>showing that \\(A^c \\in \\mathcal{C}\\). Hence, \\(\\mathcal{C}\\) is a \\(\\lambda\\)-system that contains the \\(\\pi\\)-system \\(\\mathcal{F}_1\\times \\mathcal{F}_2\\). Hence, by Dynkin's \\(\\pi\\)-\\(\\lambda\\) lemma, it follows that</p> \\[ \\sigma(\\mathcal{F}_1\\times \\mathcal{F}_2)\\subseteq \\mathcal{C}\\subseteq \\mathcal{F}=\\sigma(\\mathcal{F}_1\\times \\mathcal{F}_2) \\] <p>showing that \\(\\mathcal{C}=\\mathcal{F}\\), that is, showing that the second equation of the Theorem holds for any event \\(A\\) in \\(\\mathcal{F}\\).</p> <p>As for expectation equality from the theorem, it follows from the fact that every positive random variable \\(X\\colon\\Omega \\to \\mathbb{R}\\) can be approximated by step functions, ending the proof.</p> <p>Definition: Product Measure</p> <p>With the notations of Theorem the previous theorem, we denote this unique measure by \\(P=P_1\\otimes K\\).</p> <p>In the case where \\(K(\\omega_1,\\cdot)=P_2\\) for all \\(\\omega_1 \\in \\Omega_1\\) for some measure \\(P_2\\) on \\(\\mathcal{F}_2\\), then \\(P\\) is called the product measure of \\(P_1\\) and \\(P_2\\) on the product space and is denoted by \\(P=P_1\\otimes P_2\\).</p> <p>In the case of a product measure, due to symmetry, it holds in particular</p> \\[ \\int_{\\Omega}X(\\omega)P(d\\omega)=\\int_{\\Omega_1}\\left( \\int_{\\Omega_2}X(\\omega_1,\\omega_2) P_2(d\\omega_2)\\right)P_1(d\\omega_1)=\\int_{\\Omega_2}\\left( \\int_{\\Omega_1}X(\\omega_1,\\omega_2) P_1(d\\omega_1)\\right)P_2(d\\omega_2). \\] <p>Corollary</p> <p>Let \\(X\\) be a positive random variable on some probability space \\((\\Omega,\\mathcal{F},P)\\). Then it holds</p> \\[ E[X]=\\int_{0}^\\infty P\\left[ X&gt;x \\right]\\lambda(dx), \\] <p>where \\(\\lambda\\) is the Lebesgue measure on \\(\\mathbb{R}\\).</p> <p>Proof</p> <p>For almost all state \\(\\omega\\) in \\(\\Omega\\), we have \\(X(\\omega)\\geq 0\\), and therefore</p> \\[   X(\\omega)=\\int_{0}^{X(\\omega)}\\lambda(dx)=\\int_{0}^{\\infty}1_{\\{X(\\omega)&gt;x\\}}\\lambda(dx), \\] <p>where \\(\\lambda\\) is the Lebesgue measure on \\(\\mathbb{R}\\). Since \\((\\omega,x)\\mapsto 1_{\\{X(\\omega)&gt;x\\}}\\) is a \\(\\mathcal{F}\\otimes \\mathcal{B}(\\mathbb{R}_+)\\)-measurable function, by Fubini-Tonelli for the product measure \\(P\\otimes \\lambda\\), it holds</p> \\[ \\begin{align}     E\\left[ X \\right] &amp;=\\int_{\\Omega} \\left(\\int_{0}^{\\infty} 1_{\\{X(\\omega)&gt;x\\}}\\lambda(dx)\\right)P(d\\omega) \\\\     &amp;=\\int_{\\Omega\\times \\mathbb{R}_+} 1_{\\{X(\\omega)&gt;x\\}}P\\otimes \\lambda(d\\omega dx) \\\\     &amp;=\\int_{0}^{\\infty} \\left(\\int_{\\Omega} 1_{\\{X(\\omega)&gt;x\\}}P(d\\omega)\\right)\\lambda(dx) \\\\     &amp;=\\int_{0}^{\\infty} E\\left[ 1_{\\{X&gt;x\\}} \\right]\\lambda(dx) \\\\     &amp;=\\int_{0}^{\\infty} P\\left[ X&gt;x \\right]\\lambda(dx). \\end{align} \\] <p>We now address the stochastic variant of Fubini's theorem since we considered a stochastic kernel instead of a simple probability measure. Let \\(X\\) and \\(Y\\) be two random variables on some probability space \\((\\Omega,\\mathcal{F},P)\\). We consider the probability measure \\(P_{(X,Y)}\\) on the product Borel \\(\\sigma\\)-algebra of \\(\\mathbb{R}^2\\) given by</p> \\[   P_{(X,Y)}[B]=P\\left[ (X,Y)\\in B \\right] \\] <p>for any Borel set \\(B\\) on \\(\\mathbb{R}^2\\). We suppose that this joint distribution \\(P_{(X,Y)}\\) can be decomposed into \\(P_1\\otimes K\\) for some probability measure \\(P_1\\) on \\(\\mathcal{B}(\\mathbb{R})\\) and a stochastic kernel \\(K\\) on \\(\\mathbb{R}\\times \\mathcal{B}(\\mathbb{R})\\). We will see later that this is always the case. Note that by Tonelli's Theorem, it holds</p> \\[ \\begin{align}     P_X[B_1]&amp;=P[X\\in B_1]=P\\left[ (X,Y)\\in B_1\\times \\mathbb{R} \\right]=P_{(X,Y)}[B_1\\times \\mathbb{R}]\\\\     &amp;=\\int_{\\mathbb{R}}1_{B_1}(x)K(x,\\mathbb{R})P_1(dx)=\\int_{\\mathbb{R}}1_{B_1}(x)P_1(dx)=P_1[B_1] \\end{align} \\] <p>showing that \\(P_X=P_1\\), justifying therefore the notation \\(P_{(X,Y)}=P_X\\otimes K_{Y|X}\\).</p> <p>Theorem</p> <p>Let \\(X\\) and \\(Y\\) be two random variables whose joint distribution is given by \\(P_X\\otimes K_{Y|X}\\), where \\(P_X\\) is the distribution of \\(X\\) and \\(K_{Y|X}\\) is a stochastic kernel on \\(\\mathbb{R}\\times \\mathcal{B}(\\mathbb{R})\\).</p> <p>For every positive random variable \\(g\\colon \\mathbb{R}^2\\to \\mathbb{R}_+\\) such that \\(g(X,Y)\\) is integrable, it holds</p> \\[   E\\left[ g(X,Y)|\\sigma(X) \\right]=\\int_{\\mathbb{R}}g(X,y)K_{Y|X}(X,dy) \\] <p>\\(P\\)-almost surely.</p> <p>This relation means that for \\(P\\)-almost all \\(\\omega \\in \\Omega\\), it holds</p> \\[ E\\left[ g(X,Y)|\\sigma(X) \\right](\\omega)=\\int_{\\mathbb{R}}g(X(\\omega),y)K_{Y|X}(X(\\omega),dy). \\] <p>Proof</p> <p>From Tonelli's Theorem's proof, the function \\(x\\mapsto h(x):=\\int_{\\mathbb{R}}g(x,y)K_{Y|X}(x,dy)\\) for \\(x\\) in \\(\\mathbb{R}\\), is measurable, and therefore </p> \\[   h(X)=\\int_{\\mathbb{R}}g(X,y)K_{Y|X}(X,dy) \\] <p>is a positive random variable. Let \\(A\\) be an event in \\(\\sigma(X)\\).  It follows that \\(A=X^{-1}(B)\\) for some Borel set \\(B \\in \\mathbb{R}\\). Therefore,</p> \\[ \\begin{align}     E\\left[ 1_{A}g(X,Y) \\right] &amp;= E\\left[ 1_{B}(X)g(X,Y) \\right] \\\\     &amp;=\\int_{\\mathbb{R}^2} 1_{B}(x)g(x,y)P_X\\otimes K_{Y|X}(dx,dy) \\\\     &amp;=\\int_{\\mathbb{R}}\\left( \\int_{\\mathbb{R}}1_B(x)g(x,y) K_{Y|X}(x,dy)\\right)P_X(dx) \\\\     &amp;=\\int_{\\mathbb{R}}1_{B}(x)\\left( \\int_{\\mathbb{R}}g(x,y) K_{Y|X}(x,dy)\\right)P_X(dx) \\\\     &amp;=\\int_{\\mathbb{R}}1_B(x)h(x)P_{X}(dx) \\\\     &amp;=E\\left[ 1_A h(X) \\right]. \\end{align} \\] <p>This concludes the proof.</p> <p>Remark</p> <p>As in the previous theorem, let \\(X\\) and \\(Y\\) be two random variables whose joint distribution is given by \\(P_{(X,Y)}\\). Suppose that \\(P_{(X,Y)}\\) is absolutely continuous with respect to the Lebesgue measure on \\(\\mathbb{R}^2\\). It follows that there exists a Lebesgue-almost surely unique positive function \\(f_{(X,Y)}\\colon \\mathbb{R}^2\\to \\mathbb{R}\\) with expectation \\(1\\) such that</p> \\[ E\\left[ g(X,Y) \\right]=\\int_{\\mathbb{R}^2} g(x,y)f_{(X,Y)}(x,y)dxdy. \\] <p>It follows that the density of \\(X\\) and \\(Y\\) are respectively given by</p> \\[ f_X(x)=\\int_{\\mathbb{R}} f_{(X,Y)}(x,y)dy\\quad \\text{and}\\quad f_{Y}(y)=\\int_{\\mathbb{R}} f_{(X,Y)}(x,y)dx. \\] <p>By defining</p> \\[ f_{Y|X}(x,y)=\\frac{f_{(X,Y)}(x,y)}{f_{X}(x)}1_{\\{f_X(x)&gt;0\\}}+f_{Y}(y)1_{\\{f_X(x)=0\\}}, \\] <p>inspection shows that</p> \\[ K_{Y|X}(x,A):=\\int_{A} f_{Y|X}(x,y)dy \\] <p>defines a kernel. It holds</p> \\[ \\begin{align}     P_X\\otimes K_{Y|X}[A\\times B]&amp;=\\int_{\\mathbb{A}} \\left(\\int_{B} \\frac{f_{(X,Y)}(x,y)}{f_{X}(x)}1_{\\{f_X(x)&gt;0\\}}+f_{Y}(y)1_{\\{f_X(x)=0\\}} dy\\right)f_{X}(x)dx\\\\     &amp;=\\int_{\\mathbb{A}} \\int_{B} f_{(X,Y)}(x,y) dydx=P_{(X,Y)}[A\\times B] \\end{align} \\] <p>for every \\(A\\) and \\(B\\) in \\(\\mathcal{B}(\\mathbb{R})\\). From the uniqueness assumption of Fubini-Tonelli's theorem, it follows that </p> \\[ P_{(X,Y)}=P_X\\otimes K_{Y|X}. \\] <p>And following the theorem, it follows that</p> \\[   E\\left[ g(X,Y) \\right]=\\int_{\\mathbb{R}^2} g(x,y)f_{(X,Y)}(x,y)dxdy=\\int_{\\mathbb{R}} \\left(\\int_{\\mathbb{R}} g(x,y)f_{Y|X}(x,y)dy\\right)f_{X}(x)dx. \\]"},{"location":"lecture/03-Integration/036-uniform-integrability/","title":"Uniform Integrability","text":"<p>In the integration section, we saw the dominated convergence theorem stating that for every sequence \\((X_n)\\) such that \\(X_n \\to X\\) in probability, if \\(|X_n| \\leq Y\\) for \\(Y\\) in \\(L^1\\), then \\(X_n \\to X\\) in \\(L^1\\). The reciprocal is however not true in the sence that under convergence in probability the convergence in \\(L^1\\) does not implies uniform boundedness by an element in \\(L^1\\).</p> <p>This fact is related to a deeper issue with \\(L^1\\) that is not encountered for any other \\(L^p\\) spaces for \\(1&lt;p&lt;\\infty\\). The following concept of uniform integrability is the correct way to describe those sets that are stable under \\(L^1\\) convergence.</p> <p>Note first that if \\(X\\) is in \\(L^1\\), then it holds that \\(E[|X|1_{X&gt;n}] \\to 0\\) as \\(n\\) goes to \\(\\infty\\). Uniform integrability brings this concept to whole families.</p> <p>Definition: Uniformly Integrable Families</p> <p>A subset \\(\\mathcal{H}\\) of \\(L^1\\) is called uniformly integrable if </p> \\[   \\sup_{X \\in \\mathcal{H}}E\\left[ \\left\\vert X\\right\\vert1_{\\{\\left\\vert X\\right\\vert\\geq n \\}}\\right]\\longrightarrow 0. \\] <p>We first state two additional equivalent way to state that a family is uniformly integrable. The second one\u2014Boundedness and Tightness\u2014is sometimes refered to as the \\(\\varepsilon\\)-\\(\\delta\\)-criteria. The third one is refered to as the De la Vallee-Poussin criteria.</p> <p>Proposition</p> <p>For a subsets \\(\\mathcal{H}\\) of \\(L^1\\), the following assertions are equivalent:</p> <ol> <li>Uniform Integrability: \\(\\mathcal{H}\\) is uniformly integrable.</li> <li> <p>Boundedness and Tightness:</p> <ul> <li> <p>\\(\\mathcal{H}\\) is bounded in \\(L^1\\), that is, </p> \\[ \\sup_{X \\in \\mathcal{H}}E[\\left\\vert X\\right\\vert]&lt;\\infty. \\] </li> <li> <p>For every \\(\\varepsilon&gt;0\\), there exists \\(\\delta&gt;0\\) such that</p> \\[ E\\left[ \\left\\vert X\\right\\vert1_A \\right]\\leq \\varepsilon \\] <p>for all \\(X\\) in \\(\\mathcal{H}\\) and event \\(A\\) such that \\(P[A]\\leq \\delta\\).</p> </li> </ul> </li> <li> <p>De la Vallee Poussin:     There exists a Borel measurable function \\(\\varphi\\colon \\mathbb{R}_+\\to \\mathbb{R}_+\\) such that \\(\\varphi(x)/x\\to \\infty\\) as \\(x \\to \\infty\\) for which</p> \\[ \\sup_{X \\in \\mathcal{H}}E\\left[ \\varphi(\\left\\vert X\\right\\vert) \\right]&lt;\\infty. \\] <p>This function \\(\\varphi\\) can be chosen increasing and convex.</p> </li> </ol> <p>Proof</p> <p>Step 1: Uniform integrability implies boundedness and tightness:</p> <p>For sufficiently large \\(n\\), we have \\(E[|X| 1_{\\{|X|\\geq n\\}}]\\leq 1\\) for all \\(X\\) in \\(\\mathcal{H}\\). Hence, \\(E[|X|]\\leq n+1\\) for all \\(X\\) in \\(\\mathcal{H}\\), showing that \\(\\mathcal{H}\\) is bounded in \\(L^1\\). Let further \\(\\varepsilon&gt;0\\) and choose \\(n\\) large enough such that \\(E[| X|1_{\\{|X|\\geq n\\}}]\\leq \\varepsilon/2\\) for every \\(X\\) in \\(\\mathcal{H}\\). Setting \\(\\delta=\\varepsilon/(2n)\\), for every event \\(A\\) in \\(\\mathcal{F}\\) such that \\(P[A]\\leq \\delta\\), it follows that</p> \\[ \\begin{equation}     E\\left[ \\left\\vert X\\right\\vert1_A \\right] =E\\left[ \\left\\vert X\\right\\vert1_A1_{\\{\\left\\vert X\\right\\vert\\geq n\\}} \\right]+E\\left[ \\left\\vert X\\right\\vert1_A1_{\\{\\left\\vert X\\right\\vert&lt; n\\}} \\right]      \\leq nP[A]+\\varepsilon/2\\leq \\varepsilon, \\end{equation} \\] <p>showing that uniform integrability implies boundedness and tightness.</p> <p>Step 2: Boundedness and tightness implies uniform integrability:</p> <p>Denote by \\(M=\\sup_{X\\in \\mathcal{H}} E[|X|]&lt;\\infty\\) and let \\(\\varepsilon &gt;0\\). There exists \\(\\delta&gt;0\\) such that \\(E[|X|1_A]\\leq \\varepsilon\\) for any event \\(A\\) in \\(\\mathcal{F}\\) with \\(P[A]\\leq \\delta\\) and every \\(X\\) in \\(\\mathcal{H}\\). Then for any \\(n\\) greater than \\(M/\\delta\\) and any \\(X\\) in \\(\\mathcal{H}\\), Markov's inequality yields</p> \\[   P\\left[ \\left\\vert X\\right\\vert\\geq n \\right]\\leq \\frac{E[\\left\\vert X\\right\\vert]}{n}\\leq \\frac{M}{n}\\leq \\delta. \\] <p>Hence, \\(\\sup_{X\\in \\mathcal{H}}E\\left[ \\left\\vert X\\right\\vert1_{\\{\\left\\vert X\\right\\vert\\geq n\\}} \\right]\\leq \\varepsilon\\) showing the uniform integrability of \\(\\mathcal{H}\\).</p> <p>Step 3: De la Vallee-Poussin criteria implies uniform integrability:</p> <p>Denote by \\(M=\\sup_{X \\in \\mathcal{H}}E[\\varphi(|X|)]\\). For \\(\\varepsilon&gt;0\\), there exists \\(n_\\varepsilon\\) such that \\(\\varphi(x)\\geq x M/\\varepsilon\\) for every \\(x\\geq n_\\varepsilon\\). Hence,</p> \\[ \\begin{align}     M &amp;\\geq \\sup_{X \\in \\mathcal{H}}E\\left[ \\varphi(|X|) \\right]      \\geq \\sup_{X \\in \\mathcal{H}}E\\left[ \\varphi(\\left\\vert X\\right\\vert)1_{\\{\\left\\vert X\\right\\vert\\geq n_\\varepsilon\\}} \\right]      \\geq \\frac{M}{\\varepsilon}\\sup_{X \\in \\mathcal{H}}E \\left[ \\left\\vert X\\right\\vert1_{\\{\\left\\vert X\\right\\vert\\geq n_\\varepsilon\\}} \\right] \\end{align} \\] <p>showing that</p> \\[ \\sup_n \\sup_{X \\in \\mathcal{H}}E\\left[ \\left\\vert X\\right\\vert1_{\\{\\left\\vert X\\right\\vert \\geq n\\}} \\right]\\leq \\sup_{X \\in \\mathcal{H}}E\\left[ \\left\\vert X\\right\\vert1_{\\{\\left\\vert X\\right\\vert \\geq n_\\varepsilon\\}} \\right]\\leq \\varepsilon \\] <p>and so the uniform integrability of \\(\\mathcal{H}\\).</p> <p>Step 4: Uniform integrability implies de la Valle-Poussin criteria: Choose a sequence \\((c_n)\\), which can always be chosen increasing, such that </p> \\[   \\sup_{X \\in \\mathcal{H}}E[\\left\\vert X\\right\\vert1_{\\{\\left\\vert X\\right\\vert\\geq c_n\\}}]\\leq 1/n^3. \\] <p>Define the function \\(\\varphi:\\mathbb{R}_+\\) as a piecewise linear function, equal to \\(0\\) on \\([0,c_1]\\) and with derivative equal to \\(n\\) on \\([c_{n},c_{n+1}]\\), which implies that \\(\\varphi(x)/x \\to \\infty\\) as \\(x\\to \\infty\\). Note that this function is convex and increasing. It follows that</p> \\[ \\begin{equation}     E[\\varphi(\\left\\vert X\\right\\vert)] =\\sum E\\left[\\varphi(\\left\\vert X\\right\\vert)1_{\\{c_n\\leq \\left\\vert X\\right\\vert\\leq c_{n+1}\\}}\\right]      =\\sum n\\left( E\\left[\\left\\vert X\\right\\vert\\wedge c_{n+1}\\right]-E\\left[\\left\\vert X\\right\\vert\\wedge c_n\\right] \\right). \\end{equation} \\] <p>However</p> \\[ \\begin{align}     E\\left[\\left\\vert X\\right\\vert\\wedge c_{n+1}\\right]-E\\left[\\left\\vert X\\right\\vert\\wedge c_n\\right]     &amp; =E\\left[\\left\\vert X\\right\\vert1_{\\{c_{n}\\leq \\left\\vert X\\right\\vert&lt; c_{n+1}\\}}\\right]+E\\left[c_{n+1}1_{\\{\\left\\vert X\\right\\vert\\geq c_{n+1}\\}}\\right] -E\\left[c_n1_{\\{\\left\\vert X\\right\\vert\\geq c_n\\}}\\right]\\\\     &amp; \\leq E\\left[\\left\\vert X\\right\\vert1_{\\{\\left\\vert X\\right\\vert\\geq c_n\\}}\\right]+E\\left[\\left\\vert X\\right\\vert1_{\\{\\left\\vert X\\right\\vert\\geq c_{n+1}\\}}\\right]\\\\     &amp; \\leq 2/n^3  \\end{align} \\] <p>showing that \\(\\sup_{X \\in \\mathcal{H}}E[\\varphi(|X|)]\\leq \\sum 2n/n^3&lt;\\infty\\).</p> <p>Theorem</p> <p>Let \\((X_n)\\) be a sequence of integrable random variables such that \\(X_n\\to X\\) in probability.(1)</p> <ol> <li>That is, \\(P[\\left\\vert X_n-X\\right\\vert\\geq \\varepsilon]\\to 0\\) for every \\(\\varepsilon\\).</li> </ol> <p>Then, the following assertions are equivalent:</p> <ol> <li>The sequence is uniformly integrable;</li> <li>\\(X_n\\to X\\) in \\(L^1\\);</li> <li>\\(\\|X_n\\|_1\\to \\|X\\|_1\\).</li> </ol> <p>Proof</p> <p>Step1: Uniform integrability implies \\(L^1\\) convergence: We know that we can find a subsequence \\((Y_n)\\) of \\((X_n)\\) that converges \\(P\\)-almost surely to \\(X\\). In particular, \\((Y_n)\\) is uniformly integrable. Using Fatou's lemma and the \\(L^1\\) boundedness of the family \\((X_n)\\), it follows that </p> \\[   E[\\left\\vert X\\right\\vert]\\leq \\liminf E[\\left\\vert Y_n\\right\\vert]\\leq \\sup_n E[\\left\\vert Y_n\\right\\vert]&lt;\\infty, \\] <p>showing that \\(X \\in L^1\\). It follows that the sequence \\((X_n-X)\\) is uniformly integrable, and therefore, without loss of generality, we can assume that \\((X_n)\\) is a uniformly integrable family converging in probability to \\(0\\). For \\(\\varepsilon&gt;0\\), it holds</p> \\[   E\\left[\\left\\vert X_n\\right\\vert\\right]=E\\left[\\left\\vert X_n\\right\\vert1_{\\{\\left\\vert X_n\\right\\vert\\leq \\varepsilon/2\\}}\\right]+E\\left[\\left\\vert X_n\\right\\vert1_{\\{\\left\\vert X_n\\right\\vert&gt; \\varepsilon/2\\}}\\right]\\leq \\varepsilon/2+E\\left[\\left\\vert X_n\\right\\vert1_{\\{\\left\\vert X_n\\right\\vert&gt; \\varepsilon/2\\}}\\right]. \\] <p>By uniform integrability of the family \\((X_n)\\) and making use of the \\(\\varepsilon\\)-\\(\\delta\\) criteria, let \\(\\delta&gt;0\\) such that </p> \\[   \\sup_n E[\\left\\vert X_n\\right\\vert1_A]\\leq \\varepsilon/2 \\] <p>for every \\(A\\in \\mathcal{F}\\) with \\(P[A]\\leq \\delta\\). Further, by the convergence of \\((X_n)\\) in probability to \\(0\\), there exists \\(n_0\\) such that </p> \\[   P[\\left\\vert X_n\\right\\vert&gt; \\varepsilon/2]\\leq \\delta \\] <p>for every \\(n\\geq n_0\\). Thus, for every \\(n\\geq n_0\\), it holds </p> \\[ E[\\left\\vert X_n\\right\\vert]\\leq \\varepsilon/2+\\sup_{k\\geq n_0} E[\\left\\vert X_n\\right\\vert1_{\\{ \\left\\vert X_n\\right\\vert&gt;\\varepsilon/2\\}}]\\leq \\varepsilon, \\] <p>showing that \\(X_n\\) converges to \\(0\\) in \\(L^1\\).</p> <p>Step 2: Convergence in \\(L^1\\) implies the convergence of the norms: This step is trivial from </p> \\[   \\left\\vert\\left\\vert x\\right\\vert-\\left\\vert y\\right\\vert\\right\\vert\\leq \\left\\vert x-y\\right\\vert. \\] <p>Step 3: Convergence of the norms implies uniform integrability: For \\(M&gt;0\\), define \\(\\varphi_M\\) as the identity on \\([0,M-1]\\), \\(0\\) on \\([M,\\infty)\\), and linearly interpolated elsewhere. Let \\(\\varepsilon&gt;0\\). Using the dominated convergence theorem, choose \\(M\\) such that </p> \\[   E[\\left\\vert X\\right\\vert]-E[\\varphi_M(\\left\\vert X\\right\\vert)]\\leq \\varepsilon/2, \\] <p>since \\(\\varphi_M(\\left\\vert X\\right\\vert)\\) converges to \\(|X|\\) and is dominated by \\(\\left\\vert X\\right\\vert \\in L^1\\). By continuity of \\(\\varphi_M\\), it follows that </p> <p>[   \\varphi_M(\\left\\vert X_n\\right\\vert)\\to \\varphi_M(\\left\\vert X\\right\\vert) ] in probability. Since \\(\\varphi_M(\\left\\vert X_n\\right\\vert)\\leq M\\) for every \\(n\\), the dominated convergence theorem yields </p> \\[   E[\\varphi_M(\\left\\vert X_n\\right\\vert)]\\to E[\\varphi_M(\\left\\vert X\\right\\vert)]. \\] <p>Hence, together with \\(E[\\left\\vert X_n\\right\\vert]\\to E[\\left\\vert X\\right\\vert]\\), there exists some integer \\(n_0\\) such that</p> \\[   E[\\left\\vert X_n\\right\\vert]-E[\\left\\vert X\\right\\vert]\\leq \\varepsilon/4\\quad \\text{and}\\quad E[\\varphi_M(\\left\\vert X\\right\\vert)]-E[\\varphi_M(\\left\\vert X_n\\right\\vert)]\\leq \\varepsilon/4 \\] <p>for every \\(n\\geq n_0\\). Henceforth,</p> \\[ E\\left[\\left\\vert X_n\\right\\vert1_{\\{\\left\\vert X_n\\right\\vert\\geq M\\}}\\right]\\leq E[\\left\\vert X_n\\right\\vert]-E[\\varphi_M(\\left\\vert X_n\\right\\vert)]\\leq \\varepsilon/2+E[\\left\\vert X\\right\\vert]-E[\\varphi_M(\\left\\vert X\\right\\vert)]\\leq \\varepsilon \\] <p>for every \\(n\\geq n_0\\). Increasing the value of \\(M\\) ensures this inequality remains true for the remaining \\(n\\geq n_0\\), concluding the uniform integrability of \\((X_n)\\).</p> <p>Theorem</p> <ul> <li> <p>Let \\(X\\) be an integrable random variable and \\((\\mathcal{F}_i)\\) an arbitrary family of \\(\\sigma\\)-algebras \\(\\mathcal{F}_i\\subseteq \\mathcal{F}\\).     Then, \\((E[X|\\mathcal{F}_i])\\) is uniformly integrable.</p> </li> <li> <p>Let \\((X_i)\\) be a family of random variables bounded in \\(L^p\\) for \\(1&lt;p\\leq \\infty\\).     Then, \\((X_i)\\) is uniformly integrable.</p> </li> </ul> <p>Proof</p> <p>Since \\(X\\) is integrable, it is in particular uniformly integrable. Therefore, there exists a convex function \\(\\varphi\\) with \\(\\varphi(x)/x\\to \\infty\\) such that \\(E[\\varphi(|X|)]&lt;\\infty\\). By the conditional version of Jensen's inequality and the tower property, it follows that</p> \\[ E\\left[ \\varphi\\left( \\left| E[X|\\mathcal{F}_i] \\right| \\right) \\right]\\leq E\\left[ \\varphi\\left( E\\left[ |X||\\mathcal{F}_i \\right] \\right) \\right]\\leq E\\left[ E\\left[ \\varphi(|X|)|\\mathcal{F}_i \\right] \\right]=E\\left[ \\varphi\\left( |X| \\right) \\right], \\] <p>showing by de la Vall\u00e9e Poussin's criterion that \\((E[X|\\mathcal{F}_i])\\) is uniformly integrable.</p> <p>If \\((X_i)\\) is bounded in \\(L^p\\), then \\(\\sup E[|X_i|^p]&lt;\\infty\\) which, for \\(\\varphi(x)=x^p\\) satisfying \\(\\varphi(x)/x\\to \\infty\\), satisfies de la Vall\u00e9e Poussin's criterion. Hence, \\((X_i)\\) is uniformly integrable.</p> <p>We finish this section with an extension of Fatou's lemma for conditional expectation. While in the classical case the sequence must be bounded from below by an integrable random variable, in the conditional case, the negative part of the sequence of conditional expectation must be uniformly integrable.</p> <p>Conditional Fatou's Lemma for Uniformly Integrable Lower Bound</p> <p>Let \\((X_n)\\) be a sequence of random variables and \\(\\mathcal{G}\\subseteq \\mathcal{F}\\) be a sub-\\(\\sigma\\)-algebra. Suppose that \\((X_n^-)\\) is uniformly integrable conditionally with respect to \\(\\mathcal{G}\\), in the sense that for every \\(\\varepsilon&gt;0\\), there exists \\(M&gt;0\\) such that</p> \\[ E\\left[ X_n^- 1_{\\{X_n^-&gt;M\\}} \\big | \\mathcal{G}\\right]\\leq \\varepsilon \\quad \\text{ for all }n. \\] <p>Then it holds</p> \\[ E\\left[ \\liminf X_n |\\mathcal{G} \\right]\\leq \\liminf E\\left[ X_n |\\mathcal{G} \\right]. \\] <p>Remark</p> <p>Note that this is an extension of Fatou's Lemma for a uniformly integrable negative part of the sequence by taking \\(\\mathcal{G}\\) as the trivial \\(\\sigma\\)-algebra.</p> <p>Furthermore, note that the conditional expectation can be defined for every positive random variable by conditional monotone convergence. It can also be defined for any random variable bounded from below by some positive random variable.</p> <p>In this case, \\((X_n^-)\\) is in particular uniformly integrable. It follows that \\(\\liminf X_n^-\\) is integrable so that the inequality is well defined.</p> <p>Proof</p> <p>Let \\(X = \\liminf X_n\\) and \\(\\varepsilon &gt;0\\). By uniform conditional integrability of \\((X_n^-)\\), let \\(M&gt;0\\) such that</p> \\[ E\\left[ X_n^- 1_{\\{X_n^-&gt;M\\}} \\big |\\mathcal{G}\\right]\\leq \\varepsilon \\quad \\text{ for all }n. \\] <p>Using Fatou's Lemma for conditional expectation for positive random variables, and the fact that \\(X + M \\leq \\liminf (X_n+M)^+\\), it follows that</p> \\[ E\\left[ X+M\\big |\\mathcal{G} \\right]\\leq E\\left[ \\liminf (X_n+M)^+ \\big |\\mathcal{G}\\right]\\leq \\liminf E\\left[ (X_n+M)^+\\big |\\mathcal{G} \\right]. \\] <p>Since \\((X_n+M)^+=(X_n+M)+(X_n+M)^-\\leq X_n+M+X_n^-1_{\\{X_n^-&gt;M\\}}\\), it follows that</p> \\[ E\\left[ X \\big|\\mathcal{G} \\right]\\leq \\liminf E\\left[ X_n\\big |\\mathcal{G} \\right]+\\varepsilon. \\] <p>This completes the proof.</p>"},{"location":"lecture/04-Martingales/040-introduction/","title":"Discrete Time Processes, Martingales","text":"<p>We are now interested in \"time\" dependent random outcomes. In this chapter we consider discrete time, that is an ordered countable index. This could be anything from a finite set \\(\\{0, 1, \\ldots, T\\}\\) or at the other extreme \\(\\mathbb{Q}\\). We will however generically consider \\(\\mathbb{N}_0\\) as the standard index for time and denote different times by \\(s\\) or \\(t\\). The results in this chapter all holds for any countable ordered set. In the case where a specific different index shall be considered we will precise it.</p> <p>Throughout we fix a probability space \\((\\Omega, \\mathcal{F}, P)\\).</p> <ul> <li>Discrete Time Processes (filtration, adapted processes, stopping times.)</li> <li>Martingales (martingale, stochastic integral, doob's optional sampling theorem)</li> <li>Martingales: Almost Sure Convergence (Doob's upcrossing lemma, martingale convergence, Borel-Cantelli)</li> <li>Martingales: \\(L^p\\)-convergence (\\(L^p\\)-convergence, law of large numbers)</li> </ul>"},{"location":"lecture/04-Martingales/041-discrete-time-processes/","title":"Discrete Time Stochastic Processes","text":""},{"location":"lecture/04-Martingales/041-discrete-time-processes/#stochastic-process-filtration-adaptiveness","title":"Stochastic Process, Filtration, Adaptiveness","text":"<p>Definition</p> <p>A *stochastic process is a family \\(X=(X_t)\\) of random variables \\(X_t:\\Omega \\to \\mathbb{R}\\) indexed by \\(t\\) in \\(\\mathbb{N}_0\\).</p> <p>For a given state \\(\\omega\\), the mapping \\(t \\mapsto X_t(\\omega)\\) describing the evolution in state \\(\\omega\\) of the process is called a sample path or trajectory.</p> <p>A stochastic process \\(X=(X_t)_{t=0,\\ldots, T}\\) may also be viewed as:</p> <ul> <li> <p>A single random variable</p> \\[ \\begin{aligned}   X\\colon\\Omega \\times \\mathbb{N}_0 &amp;\\longrightarrow \\mathbb{R}\\\\   (\\omega,t)&amp;\\longmapsto X(t, \\omega)  X_t(\\omega) \\end{aligned} \\] <p>where the \\(\\sigma\\)-algebra on \\(\\Omega \\times \\{0,1, \\ldots \\}\\) is given by the product \\(\\sigma\\)-algebra \\(\\mathcal{F}\\otimes 2^{N_0}\\).</p> </li> <li> <p>A measurable function with values in the sample space</p> \\[ \\begin{aligned}     X\\colon\\Omega &amp;\\longrightarrow \\mathbb{R}^{\\mathbb{N}_0}\\\\     \\omega &amp;\\longmapsto X(\\omega) = (X_0(\\omega), X_1(\\omega), \\ldots) \\end{aligned} \\] </li> </ul> <p>where the \\(\\sigma\\)-algebra on the sample space is the product Borel \\(\\sigma\\)-algebra on \\(\\mathbb{R}^{\\mathbb{N}_0}\\).</p> <p>Exercice</p> <p>Show that the three definitions of a stochastic process in finite discrete time are equivalent.</p> <p>Example: Random Walk</p> <p>Consider now our example of coin tossing but infinitely many times. To do so consider a sequence \\((Y_t)_{t=1, 2, \\ldots}\\) of iid random variables with</p> \\[   P[Y_t = 1] = P[Y_1 = 1] = p\\quad \\text{and}\\quad P[Y_t = -1] = P[Y_1 = -1] = 1-p \\] <p>for \\(0\\leq p\\leq 1\\), in other terms the sequence are iid binary random variables.</p> <p>We define the random walk \\(S=(S_t)\\)</p> \\[ S_0=s_0 \\quad \\text{ and }\\quad S_t= S_{t-1} + Y_t = s_0+\\sum_{s=1}^t Y_s, \\quad t =1, \\ldots \\] <p>where \\(s_0\\) in \\(\\mathbb{R}\\) is the start value of the random walk.</p> <p> </p> <p>As such, a process is nothing else than an arbitrary family of random variables indexed by time. However, our intuitive understanding of a process rather corresponds to observing the outcome of which as times goes by. In other terms \\(X_s\\) \"is providing less information\" than \\(X_t\\) whenever \\(s\\leq t\\). To model this intuition, we use an increasing set of information.</p> <p>Definition: Filtration, Adapted Processes</p> <p>A filtration \\(\\mathbb{F}=(\\mathcal{F}_t)\\) is a family of \\(\\sigma\\)-algebras on \\(\\Omega\\)</p> \\[   \\mathcal{F}_0 \\subseteq \\mathcal{F}_1 \\subseteq \\ldots \\subseteq \\mathcal{F}_t \\subseteq \\ldots \\subseteq \\mathcal{F} \\] <p>A measurable space together with a filtration is called a filtered space and denoted by the tuple \\((\\Omega, \\mathcal{F}, \\mathbb{F},P)\\).</p> <p>We call a stochastic process \\(X\\) adapted if \\(X_t\\) is \\(\\mathcal{F}_t\\)-measurable for every \\(t\\).</p> <p>The \\(\\sigma\\)-algebras in a filtration become finer and finer due to the inclusion. It means that the considered events at time \\(t\\) provide more information than those at previous times.</p> <p>Filtrations can be given, but also generated by stochastic processes. Indeed, given a stochastic process \\(X\\), we can define the filtration generated by the information revealed by \\(X\\) over time, that is</p> \\[ \\mathcal{F}_t^X=\\sigma(X_0, X_1, \\ldots, X_t) \\] <p>for every time \\(t\\). It is clearly a filtration called filtration generated by \\(X\\) and denoted by \\(\\mathbb{F}^X\\). Note that \\(X\\) by definition \\(X\\) is adapted to \\(\\mathbb{F}^X\\). It is in fact the smallest filtration to which \\(X\\) is adapted to. That is, if \\(X\\) is adapted to any other filtration \\(\\mathbb{F}\\) then it holds that \\(\\mathbb{F}^X \\subseteq \\mathbb{F}\\).</p> <p>Example</p> <p>In our random walk example, we did not specify a filtration, but we can consider the following sequences of \\(\\sigma\\)-algebras</p> <ul> <li>\\(\\mathcal{F}^X_t\\);</li> <li>\\(\\mathcal{F}^S_t\\);</li> <li>\\(\\mathcal{G}_t:=\\sigma(S_t)\\);</li> <li>\\(\\mathcal{H}_t:=\\sigma(X_t)\\).</li> </ul> <p>As an exercise, try to figure out which sequence of \\(\\sigma\\)-algebras is a filtration.</p>"},{"location":"lecture/04-Martingales/041-discrete-time-processes/#stopping-time","title":"Stopping Time","text":"<p>A further important notion in the theory of stochastic processes is the so-called stopping time. Before diving into the definition and properties, let us consider the following game.</p> <p>Example: Strategic Betting?</p> <p>You have \\(100\\) renminbi and you are offered a choice between the following games:</p> <ul> <li> <p>Game 1: toss a coin 100 times and every time you get \\(1\\) you increase by 1 renminbi while if you get \\(-1\\) you loose \\(1\\) renminbi. (all in strategy)</p> </li> <li> <p>Game 2: toss a coin 100 times and every time you get \\(1\\) you increase by 1 renminbi while if you get \\(-1\\) you loose \\(1\\) renminbi.     However, as soon as your total amount of money drops to \\(70\\) renminbi you are allowed to quit playing. (stop loss strategy)</p> </li> <li> <p>Game 3: toss a coin 100 times and every time you get \\(1\\) you increase by 1 renminbi while if you get \\(-1\\) you loose \\(1\\) renminbi.     However, as soon as your total amount of money reaches a value of \\(120\\) renminbi you are allowed to stop playing. (stop gain strategy)</p> </li> <li> <p>Game 4: toss a coin 100 times and every time you get \\(1\\) you increase by 1 renminbi while if you get \\(-1\\) you loose \\(1\\) renminbi.     However, as soon as your total amount of money reaches a value of \\(120\\) or drop to \\(70\\) renminbi you are allowed to stop playing. (stop loss-gain strategy)</p> </li> <li> <p>Game 5: don't play and keep your 100 renminbi. (coward strategy)</p> </li> </ul> <p>The coin is fair, now which game would you choose? After your decision, suppose that I scale the game by \\(10000\\) (start value and $100% rmb per coin toss), would you choose the same game? Why?</p> <p>The random walk \\(S= (S_t)\\) starting at \\(100\\) is clearly well suited to model it. In the case of the first game your outcome is given by \\(S_{100}(\\omega)\\) where \\(\\omega = (\\omega_t)_{t=1,\\ldots, 100}\\) represents the outcomes of the coin toss \\(\\pm 1\\). In the case of the last game the outcome is \\(S_0 = 100\\). We are stilll facing the following questions</p> <ul> <li>What about the other games? The time at which you quit the game is random depending on the evolution of the stochastic process.</li> <li>Which game delivers in expectation the largest outcome?</li> </ul> <p>We therefore introduce the notion of a random time which intuitively provides information about when a random event occurs.</p> <p>Definition: Stopping Time</p> <p>A random time is a measurable mapping \\(\\tau :\\Omega \\to \\mathbb{N}_0 \\cup \\{\\infty\\}\\).</p> <p>A random time is a stopping time if \\(\\{\\tau \\leq t\\}\\) is an event in \\(\\mathcal{F}_t\\) for every \\(t=0, 1, \\ldots\\).</p> <p>Remark</p> <p>Since we are working in discrete time, for a random time \\(\\tau\\) to be a stopping time, it is equivalent to require \\(\\{\\tau = t\\}\\) being an event in \\(\\mathcal{F}_t\\) for all \\(t\\). Indeed, it follows from \\(\\mathbb{F}\\) being a filtration and</p> \\[   \\{\\tau \\leq t\\} = \\cup_{s=0}^t \\{\\tau = s\\} \\quad \\text{and}\\quad \\{\\tau = t\\} = \\{\\tau \\leq t\\} \\cap \\{\\tau \\leq t-1\\}^c \\] <p>The notion of stopping time just precises that the event to stop before a given time only depends on the information up to time \\(t\\). Stopping times are truly complex object conceptually as they are inherently depending on the whole past history. However, it is relatively easy to construct stopping times: Let \\(X=(X_t)\\) be a stochastic process and \\(B \\subseteq \\mathbb{R}\\). We define the function</p> \\[ \\tau_B(\\omega) = \\inf\\{t \\colon X_t(\\omega) \\in B\\} \\] <p>This function is called a hitting time or entry time and is well defined. However, it requires further assumption so as to be a random time let alone a stopping time.</p> <p>Proposition</p> <p>If \\(B\\) is a Borel set, then \\(\\tau_B\\) is a random time. If additionally \\(X\\) is adapted then \\(\\tau_B\\) is a stopping time.</p> <p>Proof</p> <p>For any \\(t\\) it holds that \\(\\{\\tau \\leq t\\} = \\cup_{s=0}^t\\{X_s \\in B\\}\\) showing that if \\(B\\) is borel, the right hand side is a finite union of events. If additionally \\(X\\) is adapted, each event in the finite union belongs to some \\(\\mathcal{F}_s \\subseteq \\mathcal{F}_t\\) for \\(s\\leq t\\).</p> <p>Let us collect some standard properties of stopping times.</p> <p>Proposition</p> <p>The following assertions hold:</p> <ol> <li>Every deterministic time \\(\\tau \\equiv t\\) is a stopping time;</li> <li> <p>\\(\\tau+\\sigma\\), \\(\\tau \\vee \\sigma\\) and \\(\\tau\\wedge \\sigma\\) are stopping times as soon as \\(\\tau,\\sigma\\) are stopping times;</p> </li> <li> <p>\\(\\lim \\tau^n\\) is a stopping time as soon as \\((\\tau^n)\\) is an increasing sequence of stopping times.</p> </li> <li> <p>If \\(\\tau\\) is a stopping time, then the collection \\(\\mathcal{F}_\\tau=\\{A \\in \\mathcal{F}: A\\cap \\{\\tau\\leq t\\}\\in \\mathcal{F}_t\\}\\) is a \\(\\sigma\\)-algebra and \\(\\tau\\) is \\(\\mathcal{F}_{\\tau}\\)-measurable.</p> </li> <li> <p>For any two stopping times, it holds \\(\\mathcal{F}_{\\sigma}\\cap \\{\\sigma\\leq \\tau\\}\\subseteq \\mathcal{F}_{\\sigma \\wedge \\tau}=\\mathcal{F}_{\\sigma}\\cap \\mathcal{F}_{\\tau}\\).     For every integrable random variable \\(X\\) with respect to some probability on \\(\\mathcal{F}\\), it holds</p> \\[ E[E[X\\,|\\,\\mathcal{F}_{\\sigma}]\\,|\\, \\mathcal{F}_{\\tau}]=E[X\\,|\\, \\mathcal{F}_{\\sigma \\wedge \\tau}] \\] </li> </ol> <p>Proof</p> <ol> <li> <p>Define \\(\\tau \\equiv t_0\\) for some given time.     From \\(\\{\\tau \\leq t\\} = \\emptyset\\) if \\(t&lt;t_0\\) or \\(\\Omega\\) otherwize, it follows that \\(\\tau\\) is a stopping time.</p> </li> <li> <p>Follows from</p> \\[   \\begin{align}     \\left\\{\\tau+\\sigma \\leq t\\right\\} &amp;= \\cup_{q=0}^t\\left\\{\\sigma\\leq t-q\\right\\}\\cap \\left\\{\\tau \\leq q\\right\\}\\\\     \\left\\{\\tau\\vee \\sigma \\leq t\\right\\} &amp;= \\left\\{\\tau \\leq t\\right\\}\\cap \\left\\{\\sigma \\leq t\\right\\}\\\\     \\left\\{\\tau\\wedge \\sigma \\leq t\\right\\} &amp;= \\left\\{\\tau \\leq t\\right\\}\\cup \\left\\{\\sigma \\leq t\\right\\}.   \\end{align} \\] <p>all right-hand sides being finite union of events contained in \\(\\mathcal{F}_t\\).</p> </li> <li> <p>Follows from</p> \\[ \\left\\{\\lim \\tau^n\\leq t\\right\\}=\\left\\{\\tau^n\\leq t:\\text{ for all }n\\right\\}=\\cap\\left\\{\\tau^n \\leq t\\right\\} \\] </li> <li> <p>Clearly, \\(\\emptyset\\) and \\(\\Omega\\) belong to \\(\\mathcal{F}_\\tau\\).     For \\(A \\in \\mathcal{F}_\\tau\\) it holds</p> \\[   A^c \\cap \\left\\{\\tau\\leq t\\right\\}=(A \\cup \\left\\{\\tau &gt;t\\right\\})^c=[(A\\cap \\left\\{\\tau \\leq t\\right\\})\\cup \\left\\{\\tau\\leq t\\right\\}^c]^c \\in \\mathcal{F}_t. \\] <p>Finally, for \\((A_n)\\subseteq \\mathcal{F}_{\\tau}\\) it holds</p> \\[   (\\cup A_n)\\cap \\left\\{\\tau \\leq t\\right\\}=\\cup (A_n \\cap \\{\\tau \\leq t\\}) \\in \\mathcal{F}_t. \\] </li> <li> <p>Let \\(A \\in \\mathcal{F}_{\\sigma}\\).     For every \\(t\\), it holds</p> \\[   A\\cap \\left\\{ \\sigma \\leq \\tau \\right\\}\\cap \\left\\{ \\tau \\leq t \\right\\} =\\left( A\\cap \\left\\{\\sigma \\leq t \\right\\} \\right)\\cap \\left\\{ \\tau \\leq t \\right\\}\\cap \\left\\{ \\sigma \\wedge t\\leq \\tau \\wedge t \\right\\}, \\] \\[   A\\cap \\left\\{ \\sigma \\leq \\tau \\right\\}\\cap \\left\\{ \\sigma \\leq t \\right\\} =\\left( A\\cap \\left\\{\\sigma \\leq t \\right\\} \\right)\\cap \\left\\{ \\sigma \\wedge t\\leq \\tau \\wedge t \\right\\}. \\] <p>Both of these are in \\(\\mathcal{F}_t\\) since \\(\\sigma \\wedge t\\) and \\(\\tau\\wedge t\\) are \\(\\mathcal{F}_t\\)-measurable. Hence, \\(\\mathcal{F}_{\\sigma}\\cap \\{\\sigma\\leq \\tau\\}\\subseteq \\mathcal{F}_{\\sigma}\\cap \\mathcal{F}_\\tau\\).</p> <p>We now show that \\(\\mathcal{F}_{\\sigma}\\cap \\mathcal{F}_{\\tau}=\\mathcal{F}_{\\sigma \\wedge \\tau}\\). Let \\(A \\in \\mathcal{F}_\\sigma\\cap \\mathcal{F}_{\\tau}\\). It follows that \\(A\\cap \\{\\sigma\\leq t\\} \\in \\mathcal{F}_t\\) and \\(A\\cap \\{\\tau \\leq t\\} \\in \\mathcal{F}_t\\) for every \\(t\\). Hence,</p> \\[   (A\\cap \\{\\sigma \\leq t\\})\\cup(A\\cap \\{\\tau \\leq t\\})=A\\cap(\\{\\sigma \\leq t\\}\\cup\\{\\tau\\leq t\\})=A\\cap \\{\\sigma\\wedge \\tau \\leq t\\} \\] <p>showing that \\(A \\in \\mathcal{F}_{\\sigma\\wedge \\tau}\\) and therefore \\(\\mathcal{F}_{\\sigma}\\cap \\mathcal{F}_{\\tau}\\subseteq \\mathcal{F}_{\\sigma\\wedge \\tau}\\).</p> <p>Conversely, let \\(A \\in \\mathcal{F}_{\\sigma\\wedge \\tau}\\). It follows that</p> \\[   A\\cap (\\{\\sigma\\leq t\\}\\cup \\{\\tau \\leq t\\})= (A\\cap \\{\\sigma \\leq t\\})\\cup(A\\cap \\{\\tau \\leq t\\})\\in \\mathcal{F}_t \\] <p>for every \\(t\\). Since \\(\\{\\sigma \\leq t\\}\\) is in \\(\\mathcal{F}_t\\), it follows that</p> \\[   (A\\cap \\{\\sigma \\leq t\\})\\cup(A\\cap \\{\\tau \\leq t\\})\\cap \\{\\sigma \\leq t\\}=A\\cap \\{\\sigma \\leq t\\} \\] <p>is also in \\(\\mathcal{F}_t\\) for every \\(t\\). Hence, \\(A\\) is in \\(\\mathcal{F}_\\sigma\\). Similarly, \\(A\\) is in \\(\\mathcal{F}_\\tau\\), and therefore \\(A\\) is in \\(\\mathcal{F}_{\\sigma}\\cap \\mathcal{F}_{\\tau}\\), showing that \\(\\mathcal{F}_{\\sigma\\wedge \\tau}=\\mathcal{F}_{\\sigma}\\cap \\mathcal{F}_{\\tau}\\). Note that \\(\\{\\sigma \\leq \\tau\\}\\) and \\(\\{\\tau \\leq \\sigma\\}\\) are both in \\(\\mathcal{F}_{\\sigma\\wedge \\tau}\\). Hence, for \\(X\\) integrable, it follows that</p> \\[   E[E[X|\\mathcal{F}_{\\sigma}]|\\mathcal{F}_\\tau]=E[E[X|\\mathcal{F}_\\sigma]1_{\\{\\sigma \\leq \\tau\\}}|\\mathcal{F}_{\\tau}]+E[E[X|\\mathcal{F}_\\sigma]|\\mathcal{F}_{\\tau}]1_{\\{\\tau&lt;\\sigma\\}}. \\] <p>From \\(\\mathcal{F}_{\\sigma}\\cap \\{\\sigma \\leq \\tau\\}\\subseteq \\mathcal{F}_{\\sigma \\wedge \\tau}\\), it follows that \\(E[X|\\mathcal{F}_\\sigma]1_{\\{\\sigma \\leq \\tau\\}}\\) is \\(\\mathcal{F}_{\\sigma \\wedge \\tau}\\)-measurable, and so is \\(E[E[X|\\mathcal{F}_\\sigma]1_{\\{\\sigma \\leq \\tau\\}}|\\mathcal{F}_{\\tau}]\\). A similar argument applies to \\(E[E[X|\\mathcal{F}_\\sigma]|\\mathcal{F}_{\\tau}]1_{\\{\\tau &lt;\\sigma\\}}\\), proving the assertion.</p> </li> </ol> <p>Proposition/Definition: Stopped Process</p> <p>Let \\(X\\) be an adapted process and \\(\\tau\\) a stopping time.</p> <ul> <li> <p>If \\(\\tau\\) is finite, that is \\(\\tau&lt;\\infty\\), then \\(X_\\tau(\\omega):=X_{\\tau(\\omega)}(\\omega)\\) is an \\(\\mathcal{F}_{\\tau}\\)-measurable random variable.</p> </li> <li> <p>The process \\(X^\\tau:=(X_{t\\wedge \\tau})\\) is an adapted process called the stopped process.</p> </li> </ul> <p>Proof</p> <p>Let \\(B\\) be a Borel subset of \\(\\mathbb{R}\\) and \\(\\tau\\) be a finite stopping time. It holds</p> \\[   \\left\\{ X_{\\tau}\\in B \\right\\}=\\cup_t (\\left\\{ X_t\\in B \\right\\}\\cap \\{\\tau=t\\}) \\] <p>the right hand side being a countable union of events in \\(\\mathcal{F}\\) showing that \\(X_{\\tau}\\) is a random variable. Let us show that this random variable is \\(\\mathcal{F}_\\tau\\)-measurable. Let \\(A=\\{ X_{\\tau}\\in B\\}\\) and fix \\(t\\). It holds</p> \\[   A\\cap \\{\\tau \\leq t\\}=\\cup_{s\\leq t}\\left(\\left\\{ X_s\\in B \\right\\}\\cap \\{\\tau=s\\}\\right). \\] <p>However, \\(\\{X_s\\in B\\}\\cap \\{\\tau=s\\}=\\{X_s\\in B\\}\\cap \\{\\tau\\leq s\\}\\cap \\{\\tau\\leq s-1\\}^c\\) is an event in \\(\\mathcal{F}_s\\subseteq \\mathcal{F}_t\\) for every \\(s\\leq t\\). Hence, \\(A\\cap \\{\\tau \\leq t\\}\\) is in \\(\\mathcal{F}_t\\) for every \\(t\\), showing that \\(A\\) is an event in \\(\\mathcal{F}_{\\tau}\\) by definition. Thus, \\(X_{\\tau}\\) is \\(\\mathcal{F}_{\\tau}\\)-measurable.</p> <p>Let now \\(\\tau\\) be any stopping time. It follows that \\(t\\wedge \\tau\\) is a finite stopping time smaller than \\(t\\), and therefore \\(\\mathcal{F}_{t\\wedge \\tau}\\subseteq \\mathcal{F}_t\\). Since \\(X^\\tau_t=X_{t\\wedge \\tau}\\) is \\(\\mathcal{F}_{t\\wedge \\tau}\\)-measurable, it is in particular \\(\\mathcal{F}_t\\)-measurable so that \\(X^\\tau\\) is an adapted process too.</p>"},{"location":"lecture/04-Martingales/041-discrete-time-processes/#stochastic-integral","title":"Stochastic Integral","text":"<p>Let us now define one of the most important objects in stochastic analysis, namely, the stochastic integral.</p> <p>Definition: Stochastic Integral</p> <p>Let \\(X = (X_t)\\) and \\(H=(H_t)\\) be two stochastic process whereby </p> <ul> <li>\\(X\\) is adapted;</li> <li>\\(H\\) is predictable, that is \\(H_0\\) is in \\(\\mathbb{R}\\) and \\(H_t\\) is \\(\\mathcal{F}_{t-1}\\)-measurable for every \\(t=1, \\ldots\\);</li> </ul> <p>The stochastic integral \\(H\\bullet X\\) of \\(H\\) with respect to \\(X\\) is defined as the process</p> \\[   H\\bullet X_t=H_0X_0+\\sum_{s=1}^t H_s \\left( X_s-X_{s-1} \\right)=H_0X_0+\\sum_{s=1}^t H_s \\Delta X_s. \\] <p>In other terms the stochastic integral is the integration of \\(H\\) against the increments \\(\\Delta X\\) of \\(X\\). In continuous terms it would formally look like this</p> \\[ H\\bullet X_t = H_0 X_0 + \\int_0^t H_s dX_s \\] <p>Lemma</p> <p>Clearly the collection of adapted and predictable processes are vector spaces.</p> <ul> <li>The operator \\(\\bullet\\) is bilinear;</li> <li>\\(H\\bullet X\\) is an adapted process itself;</li> <li> <p>For every stopping time \\(\\tau\\), the stochastic process \\(1_{\\{\\cdot \\leq \\tau\\}} = (1_{\\{t\\leq \\tau\\}})\\), is predictable</p> </li> <li> <p>Stopping the stochastic integral: For any stopping time \\(\\tau\\) it holds that</p> \\[   \\left( H1_{\\{\\cdot \\leq \\tau\\}}\\right) \\bullet X = \\left(H\\bullet X\\right)^\\tau = H\\bullet X^\\tau \\] <p>In particular \\(1_{\\{\\cdot \\leq \\tau\\}}\\bullet X = X^\\tau\\) since \\(1\\bullet X = X\\).</p> </li> </ul> <p>The last equality might be better understood in classical integral terms (ignoring the first constant term):</p> \\[ \\int_0^t H_s 1_{\\{s\\leq \\tau\\}} dX_s = \\int_0^{t\\wedge \\tau} H_s dX_s = \\int_0^t H_s dX^\\tau_s \\] <p>since \\(H1_{\\{\\cdot \\leq \\tau\\}}\\) is equal to \\(0\\) after \\(\\tau\\) and \\(X^\\tau\\) is constant after \\(\\tau\\) (hence null increments after \\(\\tau\\)).</p> <p>Proof</p> <p>The proof is mechanical and left as an exercise. Only for \\(1_{\\{\\cdot \\leq \\tau\\}}\\) being predictable it comes from the fact that \\(\\{t\\leq \\tau\\} = \\{\\tau &lt; t\\}^c = \\{\\tau \\leq t-1\\}^c\\) which is an event in \\(\\mathcal{F}_{t-1}\\).</p>"},{"location":"lecture/04-Martingales/042-martingale-doob/","title":"Martingales and Doob's Optional Sampling","text":""},{"location":"lecture/04-Martingales/042-martingale-doob/#martingales","title":"Martingales","text":"<p>Definition: Martingale (Sub/Super)</p> <p>A stochastic process \\(X\\) is called a martingale if:</p> <ol> <li>\\(X\\) is adapted;</li> <li>\\(X\\) is integrable, that is, \\(X_t\\) is integrable for every \\(t\\);</li> <li>\\(X_s=E[X_t\\,|\\, \\mathcal{F}_s]\\) whenever \\(s\\leq t\\).</li> </ol> <p>A process \\(X\\) is called a super-martingale if instead of (3) we require:</p> <p>3'. \\(X_s\\geq E[X_t\\,|\\, \\mathcal{F}_s]\\) whenever \\(s\\leq t\\).</p> <p>A process \\(X\\) is called a sub-martingale if instead of (3) we require:</p> <p>3''. \\(X_s\\leq E[X_t\\,|\\, \\mathcal{F}_s]\\) whenever \\(s\\leq t\\).</p> <p>Remark</p> <p>Note that a martingale is, in particular, both a super-martingale and a sub-martingale at the same time.</p> <p>Note that since we are working in discrete time, the martingal (sub/super) property can be checked only on each increment, that is \\(E[\\Delta X_t |\\mathcal{F}_{t-1}]=E[X_t - X_{t-1}|\\mathcal{F}_{t-1}] =0\\).</p> <p>Example</p> <ul> <li> <p>Given and integrable random variable \\(\\xi\\), the process \\(X = (E[\\xi|\\mathcal{F}_t])\\) defines a martingale.</p> </li> <li> <p>Consider the random walk \\(S\\) from the example in the previous section.     show that </p> <ul> <li>\\(p=1/2\\), then \\(S\\) is a martingale;</li> <li>\\(p\\geq 1/2\\), then \\(S\\) is a sub-martingale;</li> <li>\\(p\\leq 1/2\\), then \\(S\\) is a super-martingale.</li> </ul> </li> </ul> <p>Proposition</p> <p>Let \\(X\\) be an adapted process and \\(\\varphi:\\mathbb{R}\\to \\mathbb{R}\\) be a measurable function such that \\(\\varphi(X_t)\\) is integrable for every \\(t\\).</p> <ul> <li>If \\(X\\) is a martingale and \\(\\varphi\\) is convex, then \\(Y=(\\varphi(X_t))\\) is a sub-martingale.</li> <li>If \\(X\\) is a martingale and \\(\\varphi\\) is concave, then \\(Y=(\\varphi(X_t))\\) is a super-martingale.</li> <li>If \\(X\\) is a sub-martingale and \\(\\varphi\\) is convex and increasing, then \\(Y=(\\varphi(X_t))\\) is a sub-martingale.</li> </ul> <p>Proof</p> <p>Since a process \\(Y\\) is a sub-martingale if and only if \\(-Y\\) is a super-martingale, and \\(\\varphi\\) is convex if and only if \\(-\\varphi\\) is concave, we only need to prove the first point to get the second. Clearly, \\(Y\\) is adapted. By assumption, \\(Y_t\\) is integrable for every \\(t\\). Finally, using Jensen's inequality for conditional expectation and the martingale property \\(X_s=E[X_t|\\mathcal{F}_s]\\), it follows that:</p> \\[   E[Y_t|\\mathcal{F}_s]=E[\\varphi(X_t)|\\mathcal{F}_s]\\geq \\varphi(E[X_t|\\mathcal{F}_s])=\\varphi(X_s)=Y_s. \\] <p>If \\(X\\) is a sub-martingale and \\(\\varphi\\) is convex and increasing, then:</p> \\[ E[Y_t|\\mathcal{F}_s]=E[\\varphi(X_t)|\\mathcal{F}_s]\\geq \\varphi(E[X_t|\\mathcal{F}_s])\\geq \\varphi(X_s)=Y_s, \\] <p>showing the sub-martingale property and therefore proving the third point.</p> <p>Clearly the notion of martingale is very minimal. A martingale can be seen as a noise process (in a vague sense) in so far that it moves in any possible direction but in average like now. Sup-martingale are trending downwards while sub-martingales are trending upwards. This intuitive notion and the centrality of this fact can be inspected in the following Doob Meyer Theorem.</p> <p>Theorem: Doob Meyer Decomposition</p> <p>Let \\(X\\) be an adapted and integrable process. Then there exists a unique decomposition:</p> \\[   X=M+A, \\] <p>where \\(M\\) is a martingale and \\(A\\) is a predictable process with \\(A_0=0\\). This decomposition is called the Doob decomposition.</p> <p>Furthermore, \\(X\\) is a sub-martingale or super-martingale if and only if \\(A\\) is increasing or decreasing respectively.</p> <p>Proof</p> <p>Assume that we had such a decomposition, then it follows that \\(M = X - A\\) is a martingale. By the martingale property and predictability of \\(A\\) we get</p> \\[   0= E[\\Delta M_{t+1}|\\mathcal{F}_t] = E[\\Delta X_{t+1}|\\mathcal{F}_t] - E[\\Delta A_{t+1}|\\mathcal{F}_t] =  E[\\Delta X_{t+1}|\\mathcal{F}_t] -(A_{t+1} - A_t) \\] <p>This provides us a recursive way to define \\(A\\) as follows</p> \\[   \\begin{equation*}     \\begin{cases}         A_0 &amp;= 0\\\\         A_t &amp;=A_{t-1}+ E[X_t-X_{t-1}|\\mathcal{F}_{t-1}] \\quad \\text{for}\\quad t\\geq 1     \\end{cases}   \\end{equation*} \\] <p>It is immediate to check by induction that \\(A\\) is predictable and by definition \\(M:=X - A\\) is a martingale providing the decomposition.</p> <p>As for the uniqueness, let \\(X = M+A = \\tilde{M}+\\tilde{A}\\) where \\(M\\) and \\(\\tilde{M}\\) are martingales and \\(A\\) and \\(\\tilde{A}\\) are predictable processes starting at \\(0\\). It follows that \\(M - \\tilde{M} = \\tilde{A}-A\\) is a predictable martingale. Hence by martingale property and then predictability it holds that \\(M_{t_1}-\\tilde{M}_{t-1} = E[M_t - \\tilde{M}_{t} |\\mathcal{F}_{t-1}] = M_t - \\tilde{M}_{t}\\) showing that </p> \\[   M_t - \\tilde{M}_{t} = M_{t-1} - \\tilde{M}_{t-1} = \\cdots = M_0 - \\tilde{M}_0 = \\tilde{A}_0 - A_0 = 0 \\] <p>We deduce that \\(M=\\tilde{M}\\) and \\(A = \\tilde{A}\\).</p> <p>The assertions about super and sub martingales are immediate to get.</p>"},{"location":"lecture/04-Martingales/042-martingale-doob/#stochastic-integration-with-respect-to-a-martingale","title":"Stochastic integration with respect to a martingale","text":"<p>Doob's Optional Sampling Theorem, Modern Version</p> <p>Let \\(H\\) be a predictable process. The following holds true:</p> <ol> <li>If \\(X\\) is a martingale and \\(H\\bullet X_t\\) is integrable for every \\(t\\), then \\(H\\bullet X\\) is a martingale.</li> <li>If \\(X\\) is a super-martingale or sub-martingale, \\(H\\geq 0\\), and \\(H\\bullet X_t\\) is integrable for every \\(t\\), then \\(H\\bullet X\\) is a super-martingale or sub-martingale.</li> </ol> <p>Proof</p> <p>Suppose that \\(X\\) is a martingale and \\(H\\) is such that \\(H\\bullet X\\) is integrable. Adaptiveness is immediate. From \\(H\\) being predictable, that is, \\(H_{t+1}\\) is \\(\\mathcal{F}_t\\)-measurable, and \\(X\\) being a martingale, that is, \\(E[X_{t+1}-X_t|\\mathcal{F}_t]=E[X_{t+1}|\\mathcal{F}_t]-X_t=0\\), it follows that:</p> \\[   E\\left[ \\Delta H\\bullet X_{t+1}|\\mathcal{F}_t \\right]=E\\left[ H_{t+1} \\Delta X_{t+1}|\\mathcal{F}_t \\right]= H_{t+1}E\\left[ \\Delta X_{t+1}|\\mathcal{F}_t \\right]= 0. \\] <p>The argument in the sub-martingale case is similar, using the fact that \\(H_{t+1}\\geq 0\\) and \\(E[X_{t+1}-X_t|\\mathcal{F}_t]=E[X_{t+1}|\\mathcal{F}_t]-X_t\\geq 0\\) and similarly for the super-martingale case.</p> <p>Remark</p> <p>Note that in this theorem, if there exists a constant \\(C&gt;0\\) such that \\(|H_t|&lt;C\\) for every \\(t\\), then \\(H\\bullet X_t\\) is integrable for every \\(t\\) as soon as \\(X\\) is integrable. Indeed,</p> \\[   E\\left[ |H\\bullet X_t| \\right]\\leq E\\left[ |H_0 X_0| \\right]+\\sum_{s=1}^tE\\left[ |H_t||X_{t}-X_{t-1}| \\right]\\leq 2C\\sum_{s=0}^t E[|X_t|]&lt;\\infty. \\] <p>So the assumption that \\(|H\\bullet X_t|\\) is integrable for every \\(t\\) can be replaced by \\(H\\) being uniformly bounded.</p> <p>This remark allows us to formulate the original Doob's sampling theorem.</p> <p>Doob's Optional Sampling Theorem</p> <p>Let \\(X\\) be a (super/sub-)martingale and \\(\\tau\\) a stopping time. Then \\(X^\\tau\\) is a (super/sub-)martingale.</p> <p>Proof</p> <p>Let \\(\\tau\\) be a stopping time. It holds that \\(X^\\tau=H\\bullet X\\) for the process \\(H=1_{\\{\\cdot \\leq \\tau\\}}\\). However, \\(H\\) is predictable, uniformly bounded since \\(|H_t|\\leq 1\\), and positive. By the property of the stochastic integral \\(1_{\\{\\cdot \\leq \\tau\\}}\\bullet X = X^\\tau\\). Hence, according to Doob's optional sampling theorem (modern version), it follows that \\(X^\\tau\\) is a (super/sub-)martingale.</p> <p>Proposition</p> <p>If \\(X\\) is a martingale or sub-martingale, then:</p> \\[   E[X_\\tau \\,|\\, \\mathcal{F}_\\sigma]=X_{\\sigma} \\quad \\text{or} \\quad E[X_\\tau \\,|\\, \\mathcal{F}_\\sigma]\\geq X_{\\sigma}, \\] <p>respectively, for every pair of bounded stopping times \\(\\sigma\\leq \\tau \\leq T\\) for some \\(T\\).</p> <p>Proof</p> <p>Since \\(\\tau \\leq T\\) for some \\(T\\), it follows that:</p> \\[ \\left\\vert X_\\tau \\right\\vert\\leq \\left\\vert X_0\\right\\vert+\\cdots +\\left\\vert X_t\\right\\vert. \\] <p>Thus, \\(X_\\tau\\) is integrable. Furthermore, \\(X^\\tau\\) is a martingale from Doob's optional sampling theorem and \\(X^\\tau_T = X_\\tau\\). For \\(A \\in \\mathcal{F}_{\\sigma}\\), it holds that \\(A\\cap \\{\\sigma=s\\}\\) is an event in \\(\\mathcal{F}_s\\). Hence,</p> \\[   E\\left[ (X_{t}-X_{\\sigma})1_{A} \\right] =\\sum_{s\\leq k}E\\left[ (X_{t}-X_{s})1_{A\\cap \\{\\sigma =s\\}} \\right]=\\sum_{s\\leq k}E\\left[ E\\left[X_{t}-X_{s}\\,|\\, \\mathcal{F}_{s}\\right]1_{A\\cap \\{\\sigma =s\\}} \\right]=0, \\] <p>showing that \\(E[X_t\\,|\\, \\mathcal{F}_{\\sigma}]=X_{\\sigma}\\). Applying this to the stopped process \\(X^\\tau\\) yields the result. The proof in the sub-martingale case follows the same argumentation.</p>"},{"location":"lecture/04-Martingales/043-martingale-as-convergence/","title":"Martingale: Almost Sure Convergence","text":"<p>Given a martingale \\(X\\), this section treats the questions whether there exists \\(X_\\infty\\) such that \\(X_t\\to X_\\infty\\) in the almost sure sense. In other terms we want to study the asymptotic behavior of a stochastic process \\(X\\).</p> <p>Given a stochastic process \\(X\\), we can classify the different behaviors of the paths \\(t \\mapsto X_t(\\omega)\\) as follows</p> <ul> <li> <p>Convergence to \\(\\pm \\infty\\): the path converges to either \\(\\infty\\) or \\(-\\infty\\).     The corresponding event of those states where it happens is given by</p> \\[C = \\{\\limsup X_t = -\\infty \\text{ or }\\liminf X_t = \\infty\\}\\] </li> <li> <p>Convergence in \\(\\mathbb{R}\\): the path converges to a real limit, that is, \\(\\lim X_t(\\omega)\\) exists in \\(\\mathbb{R}\\).     The corresponding event of those states where it happens is given by</p> \\[B = \\{-\\infty &lt; \\liminf X_t = \\limsup X_t&lt;\\infty\\}\\] </li> <li> <p>Oscillatory behavior: In the case of non convergence, the path will oscillate infinitely.     The corresponding event of those states where it happens is given by</p> \\[A = \\{\\liminf X_t &lt; \\limsup X_t\\}\\] </li> </ul> <p>Clearly each of these sets are events and build a partition of \\(\\Omega\\). The seminal idea of Doob was to recognize that sub-martingales have properties helping to estimate the probability of the latter set. The building bloc for which is the Doob's upcrossing lemma.</p>"},{"location":"lecture/04-Martingales/043-martingale-as-convergence/#doobs-upcrossing-lemma-and-martingale-convergence","title":"Doob's Upcrossing Lemma and Martingale Convergence","text":"<p>Let \\(X\\) be a process, \\(x&lt;y\\) be real numbers and \\(F\\) be a finite subset of \\(\\mathbb{N}_0\\). Usually we take \\(F = [\\![0, T ]\\!] :=\\{0, 1, \\cdots, T\\}\\). We wish to count the number of upcrossings of the path \\(t \\mapsto X_t(\\omega)\\) on the set \\(F\\). We proceed as follows by defining the stopping times</p> \\[ \\begin{equation*}     \\tau_0=0 \\end{equation*} \\] <p>and recursively</p> \\[ \\begin{align*}     \\tau_1 &amp; = \\inf\\left\\{t \\in F: t\\geq \\tau_0 \\text{ and } X_t \\leq x\\right\\}\\\\     \\tau_2 &amp; = \\inf\\left\\{t \\in F: t\\geq \\tau_1 \\text{ and } X_t \\geq y\\right\\}\\\\            &amp; \\vdots\\\\     \\tau_{2k-1} &amp; = \\inf\\left\\{t \\in F: t\\geq \\tau_{2k-2}, X_t \\leq x\\right\\}\\\\     \\tau_{2k} &amp; = \\inf\\left\\{t \\in F: t\\geq \\tau_{2k-1}, X_t \\geq y\\right\\} \\end{align*} \\] <p>with the convention that the infimum over the empty set is infinite.</p> <p> </p> <p>We define the random quantity</p> \\[ \\begin{equation*}     U_F\\left( x,y,X(\\omega) \\right)=\\sup\\left\\{k:\\tau_{2k}(\\omega)&lt;\\infty\\right\\}. \\end{equation*} \\] <p>This corresponds to the strict positive number of up-crossing of \\([x,y]\\) by \\(t\\mapsto X_t(\\omega)\\) on \\(F\\).</p> <p>Finally, we adopt the notation \\([\\![s, t ]\\!]:=\\{s,s+1,\\ldots,t\\}\\) for every integers \\(s\\leq t\\).</p> <p>Doob's Upcrossing Lemma</p> <p>Let \\(X\\) be a sub-martingale. Then for every two reals \\(x&lt;y\\), the number \\(U_{[\\![0, T ]\\!]}(x,y,X)\\) of up-crossing of \\([x,y]\\) by \\(t\\mapsto X_t\\) up to time \\(T\\), is a positive random variable and it holds</p> \\[ \\begin{equation}     (y-x)E\\left[ U_{[\\![0, T ]\\!] }\\left( x,y,X \\right) \\right]\\leq E\\left[ \\left(X_T-x\\right)^+ \\right]-E\\left[ \\left( X_0-x \\right)^+ \\right]. \\end{equation} \\] <p>Proof</p> <p>First of all, the random times \\(\\tau_k\\), \\(k=0,1,\\ldots\\) defining the up-crossing function are all stopping times. Since \\([\\![0, T ]\\!]\\) is a discrete interval here, it follows that \\(U:=U_{[\\![0, T ]\\!]}(x,y,X)\\) is a positive random variable. Define now the predictable gamble strategy, that is, the predictable process</p> \\[ \\begin{equation*}     H_t=\\sum_{k\\geq 1} 1_{]\\tau_{2k-1},\\tau_{2k}]}(t) =      \\begin{cases}       1 &amp; \\text{if } \\tau_{2k-1}&lt; t\\leq \\tau_{2k} \\text{ for some }k\\\\       0 &amp; \\text{otherwize}     \\end{cases} \\end{equation*} \\] <p>for which holds \\(H_0=0\\). It is predictable since it takes only values \\(0\\) and \\(1\\) and it holds</p> \\[ \\begin{equation*}     \\{H_t=1\\}=\\cup \\left\\{ \\tau_{2k-1}&lt;t \\right\\}\\cap\\left\\{ \\tau_{2k}&lt;t \\right\\}^c \\in \\mathcal{F}_{t-1} \\end{equation*} \\] <p>This gamble strategy \\(H\\) is a bet on upcrossings. Note that by the definition of \\(\\tau_{2k}\\) it follows that for every \\(\\omega \\in \\Omega\\), either \\(\\tau_{2k}(\\omega)\\leq t\\) or \\(\\tau_{2k}(\\omega)=\\infty\\). Further, by the definition of \\(U\\) it holds that \\(U(\\omega)\\leq t\\), and therefore \\(\\tau_{2U(\\omega)}\\leq t\\) as well as \\(\\tau_{2U(\\omega)+2}=\\infty\\) for every \\(\\omega\\). Finally, since \\(U\\) is a random variable, it follows that \\(\\tau_{2U}\\) is a random time.</p> <p>We translate our problem at \\(0\\) by defining the process \\(Y=(X-x)^+\\). Since \\(\\varphi(z)= (z-x)^+\\) is increasing and convex function, it follows that \\(Y\\) is a sub-martingale too. It clearly holds that \\(U\\) also counts the number of up-crossings of \\([0,y-x]\\) up to time \\(T\\) by \\(t\\mapsto Y_t\\) and therefore since \\(\\tau_{2U}\\leq T\\) and \\(\\tau_{2U+2} = \\infty\\), it holds</p> \\[ \\begin{align*}     H\\bullet Y_T &amp;=\\sum_{t=1}^T H_t\\left( Y_{t}-Y_{t-1} \\right)\\\\     &amp;=\\sum_{t=1}^T\\sum_{k\\geq 1} 1_{]\\tau_{2k-1},\\tau_{2k}]}(t)\\left( Y_{t}-Y_{t-1} \\right)\\\\     &amp;=\\sum_{k\\geq 1}\\sum_{t=(\\tau_{2k-1}+1)\\wedge T}^{\\tau_{2k}\\wedge T}\\left( Y_t-Y_{t-1} \\right)\\\\     &amp; = \\sum_{k=1}^{U}\\left( Y_{\\tau_2k} - Y_{\\tau_{2k-1}}\\right) + (Y_T - Y_{\\tau_{2U+1}\\wedge T})\\\\     &amp; = \\sum_{k=1}^{U}\\left( Y_{\\tau_2k} - Y_{\\tau_{2k-1}}\\right) + (Y_T - Y_{\\tau_{2U+1}})1_{\\{\\tau_{2U+1}&lt;T\\}}\\\\ \\end{align*} \\] <p>On the one hand, we know that if \\(k\\leq U\\), then \\(Y_{\\tau_{2k}}-Y_{\\tau_{2k-1}}\\geq y-x\\). On the other hand, since \\(Y_{\\tau_{2U+1}}\\) is equal to \\(0\\) on \\(\\{\\tau_{2U+1}&lt;T\\}\\), it follows that  \\((Y_T - Y_{\\tau_{2U+1}})1_{\\{\\tau_{2U+1}&lt;T\\}}\\geq 0\\). Hence it holds</p> \\[ \\begin{align*}     E[H\\bullet Y_T] &amp; =E\\left[\\sum_{k=1}^U (Y_{\\tau_{2k}}-Y_{\\tau_{2k-1}})\\right]+E\\left[(Y_{T}-Y_{\\tau_{2U+1}})1_{\\{T&gt; \\tau_{2U+1}\\}}\\right]\\\\      &amp; \\geq E\\left[\\sum_{k=1}^U (y-x)\\right]=(y-x)E[U] \\end{align*} \\] <p>Defining \\(K_t=1-H_t\\) for every \\(t\\geq 1\\) and \\(K_0=0\\) which is a positive predictable process, hence by means of Doob's optional sampling theorem, it follows that \\(K\\bullet Y\\) is a sub-martingale and therefore \\(E[K\\bullet Y_T]\\geq E[K\\bullet Y_0]= 0\\). Since \\(K+H=1_{\\{1\\leq \\cdot\\} }\\), it follows that</p> \\[ \\begin{align*}     (y-x)E\\left[ U \\right]  &amp; \\leq E\\left[ H\\bullet Y_T \\right]\\\\                             &amp; \\leq E\\left[ H\\bullet Y_T \\right]+E[K\\bullet Y_T] \\\\                             &amp; = E\\left[ \\sum_{t=1}^T Y_{t}-Y_{t-1} \\right]\\\\                             &amp; = E\\left[ Y_T-Y_0 \\right]\\\\                             &amp; =E\\left[ \\left( X_T-x \\right)^+ \\right]-E\\left[ \\left( X_0-x \\right)^+ \\right] \\end{align*} \\] <p>which ends the proof.</p> <p>Theorem: Martingale Convergence \\(P\\)-Almost Sure</p> <p>Let \\(X\\) be a sub-martingale such that \\(\\sup E[X_t^+]&lt;\\infty\\). Then \\(X_t \\to X_\\infty\\) almost surely for some integrable random variable \\(X_\\infty\\).</p> <p>Proof</p> <p>Note that if \\(X\\) is a sub-martingale, then \\(\\sup E[\\left\\vert X\\right\\vert_t]&lt;\\infty\\) is equivalent to \\(\\sup E[X^+_t]&lt;\\infty\\). Indeed, it follows from \\(\\left\\vert X\\right\\vert_t= 2X_t^+-X_t\\) and the sub-martingale property, that \\(E[X_t]\\geq E[X_0]&gt;-\\infty\\).</p> <p>Let</p> <ul> <li> <p>\\(A\\) be the event of those states \\(\\omega\\) such that \\(t\\mapsto X_t(\\omega)\\) is oscillatory discontinuous.     In other terms, the path will cross infinitely many times some interval \\([q, r]\\) for \\(q&lt;r\\) rationals.     According to the definition in Doob's Upcrossing lemma, it follows that \\(U_{\\mathbb{N}_0}(q, r, X)(\\omega) = \\lim_{T\\to \\infty} U_{[\\![0, T ]\\!]} (q, r, X)(\\omega) = \\infty\\).     Hence we can write</p> \\[ \\begin{equation*}     A= \\bigcup_{q&lt;r \\text{ and }q,r \\in \\mathbb{Q}}\\left\\{ U_{\\mathbb{N}_0}\\left( q,r,X\\right)=\\infty\\right\\}=\\bigcup_{q&lt;r\\text{ and } q,r \\in \\mathbb{Q}}\\left\\{ \\sup_{T \\in \\mathbb{N}_0}U_{[\\![0, T ]\\!]}(q,r,X)=\\infty\\right\\} \\end{equation*} \\] </li> <li> <p>\\(B\\) be the event of those states \\(\\omega\\) such that \\(t \\mapsto X_t(\\omega)\\) has a real valued limit, that is</p> \\[ \\begin{equation*}   B=\\left\\{ \\infty &lt;\\liminf X_t=\\limsup X_t &lt;\\infty \\right\\} \\end{equation*} \\] </li> <li> <p>\\(C\\) be the event of those states \\(\\omega\\) such that \\(t \\mapsto X_t(\\omega)\\) diverges to either \\(\\infty\\) or \\(-\\infty\\), that is</p> \\[   C = \\left\\{ \\limsup X_t = -\\infty \\text{ or }\\liminf X_t = \\infty \\right\\} \\] </li> </ul> <p>In other terms \\(t\\mapsto X_t\\) converges to some extended random variable \\(X_\\infty\\) on \\(B\\cup C\\). As for \\(A\\), it is a measurable set as a countable union of measurable sets. Furthermore, by means of Doob's up-crossing's Lemma, as well as monotone convergence, the assumptions of the theorem yield</p> \\[ \\begin{equation*}    E\\left[ \\sup_{t \\in \\mathbb{N}_0}U_{[\\![0, T ]\\!]}(q,r,X) \\right]=\\sup_{t \\in \\mathbb{N}_0}E\\left[ U_{[\\![0, T ]\\!] }(q,r,X) \\right]\\leq \\frac{1}{q-r}\\sup_{t}\\left\\{ E\\left[ \\left( X_t-q \\right)^+ \\right] -E\\left[ \\left( X_0-q \\right)^+ \\right]\\right\\}&lt;\\infty \\end{equation*} \\] <p>It follows that \\(P[\\sup_{t \\in \\mathbb{N}_0}U_{[\\![0, T ]\\!] }(q,r,X)=\\infty]=0\\) from which follows</p> \\[ \\begin{equation*}     P[A]\\leq \\sum_{q&lt;r\\text{ and }q,r \\in \\mathbb{Q}}P\\left[ \\sup_{t \\in \\mathbb{N}_0}U_{[\\![0, T ]\\!]}(q,r,X)=\\infty \\right]=0. \\end{equation*} \\] <p>Hence, \\(P[B\\cup C]=1\\), showing that \\(t\\mapsto X_t\\) converges almost surely to the extended real valued random variable \\(X_\\infty\\). Finally, by Fatou's Lemma,</p> \\[ \\begin{equation*}     E[|X_\\infty|]\\leq \\liminf E[|X_t|]\\leq \\sup E[|X|_t]&lt;\\infty \\end{equation*} \\] <p>showing integrability of \\(X_\\infty\\) and also that \\(P[X_\\infty=\\infty\\text{ or }X_\\infty=-\\infty]=P[C]=0\\).</p> <p>Corollary</p> <p>Let \\(X\\) be a super-martingale such that \\(\\sup_t E[X_t^-]&lt;\\infty\\). Then \\(X_t\\to X_\\infty\\) almost surely for some integrable random variable \\(X_\\infty\\).</p>"},{"location":"lecture/04-Martingales/043-martingale-as-convergence/#applications-of-p-almost-sure-convergence","title":"Applications of \\(P\\)-almost Sure Convergence","text":"<p>This almost sure convergence results has numerous applications. We present in the following several of them that as a consequence recovers the Borel-Cantelli lemma.</p> <p>Theorem</p> <p>Let \\(X\\) be a martingale with \\(X_0=0\\). Suppose that \\(|X_{t+1}-X_t|\\leq c\\) for every \\(t\\) and some constant \\(c&gt;0\\). Then it holds</p> \\[ \\begin{equation*}     P\\left[ B\\cup D \\right]=1, \\end{equation*} \\] <p>where</p> \\[ \\begin{equation*}     B =\\left\\{ -\\infty&lt;\\liminf X_t=\\limsup X_t&lt;\\infty\\right\\} \\text{ and } D =\\left\\{ \\liminf X_t=-\\infty \\text{ and } \\limsup X_t=\\infty \\right\\}. \\end{equation*} \\] <p>This theorem states that martingales with uniformly bounded increments either converge to a real value or oscilate infinitely between \\(-\\infty\\) and \\(\\infty\\).</p> <p>Proof</p> <p>Define the stopping time \\(\\tau_k=\\inf\\{t\\colon X_t &gt; k\\}\\). According to Doob's sampling theorem, it follows that \\(X^{\\tau_k}\\) is a martingale such that \\(\\sup_tE[(X^{\\tau_k}_t)^+]\\leq k+c&lt;\\infty\\). Indeed, on \\(\\{t &lt; \\tau_k\\}\\), it holds \\(X_t^{\\tau_k}\\leq k\\) and on \\(\\{\\tau_k\\leq t\\}\\), it holds \\(X_t^{\\tau_{k}}=X_{\\tau_k}\\leq X_{\\tau_k-1}+(X_{\\tau_k}-X_{\\tau_k-1})\\leq k+c\\). By the martingale convergence theorem, \\(\\lim_{t\\to \\infty} X_t^{\\tau_k}\\) exists almost surely. On \\(\\{\\tau_k=\\infty\\}\\) the processes \\(X\\) and \\(X^{\\tau_k}\\) coincide, so that \\(\\lim  X_t\\) exists almost surely on \\(\\{\\tau_k=\\infty\\}\\). In particular \\(\\lim X_t\\) exists almost surely on</p> \\[ \\begin{equation*}     \\bigcup\\{\\tau_k=\\infty\\}=\\left\\{\\limsup X_t&lt;\\infty\\right\\}. \\end{equation*} \\] <p>A similar argumentation for \\(-X\\) shows that \\(\\lim X_t\\) exists almost surely on \\(\\{\\liminf X_t&gt;-\\infty\\}\\). That is \\(\\lim X_t\\) exists almost surely on \\(\\{\\liminf X_t&gt;-\\infty \\}\\cup\\{\\limsup X_t&lt;\\infty\\}=D^c\\). It means that \\(P[D^c\\setminus B]=P[D^c\\cap B^c]=0\\). Hence, taking complementation, it follows that \\(P[B\\cup D]=P[(D^c\\cap B^c)]=1\\) which ends the proof.</p> <p>Corollary: Pre Borel-Cantelli</p> <p>We suppose that \\(\\mathcal{F}_0=\\{\\emptyset,\\Omega\\}\\). Let \\((A_t)\\) be a sequence of events in \\(\\mathcal{F}\\) such that \\(A_t\\) is in \\(\\mathcal{F}_t\\) for every \\(t\\). Then</p> \\[ \\begin{align*}     \\limsup A_t&amp;=\\bigcap_{t}\\bigcup_{s\\geq t}A_s=\\left\\{\\omega:\\omega\\in A_t\\text{ for infinitely many }t\\right\\}=\\left\\{\\sum P(A_t|\\mathcal{F}_{t-1})         =\\infty\\right\\} \\end{align*} \\] <p>holds almost surely, whereby \\(P[A_t|\\mathcal{F}_{t-1}]=E[1_{A_t}|\\mathcal{F}_{t-1}]\\).</p> <p>Proof</p> <p>We define the process \\(X\\) as follows</p> \\[ \\begin{equation*}     X_0=0\\quad \\text{ and }\\quad X_t=\\sum_{s=1}^t1_{A_s}-P\\left[ A_s|\\mathcal{F}_{s-1} \\right], \\quad \\text{ for }t\\geq 1 \\end{equation*} \\] <p>Since \\(\\mathcal{F}_0=\\{\\emptyset,\\Omega\\}\\), it follows that \\(X\\) is a martingale. Indeed, \\(X\\) is clearly adapted by definition, and \\(|X_t|\\leq 2t\\) so that \\(X\\) is integrable. Furthermore, \\(E[X_1-X_{0}|\\mathcal{F}_{0}]=E[X_1-X_0]=P[A_1]-P[A_1]=0\\) and</p> \\[ \\begin{equation*}     E[X_t-X_{t-1}|\\mathcal{F}_{t-1}]=E[1_{A_t}-E[1_{A_t}|\\mathcal{F}_{t-1}]|\\mathcal{F}_{t-1}]=E[E[1_{A_t}-1_{A_t}|\\mathcal{F}_{t-1}|\\mathcal{F}_{t-1}]=0 \\end{equation*} \\] <p>for every \\(t\\geq 2\\). Since \\(|X_{t+1}-X_t|\\leq 2\\) holds for every \\(t\\), we may apply the previous theorem of convergence for martingales with bounded bounded increments. On \\(B=\\{\\liminf X_t=\\limsup X_t \\in\\mathbb{R}\\}\\), it holds</p> \\[ \\begin{equation*}    \\sum 1_{A_t}=\\infty\\quad \\text{if, and only if,}\\quad\\sum P\\left[A_n | \\mathcal{F}_{t-1}\\right]=\\infty.  \\end{equation*} \\] <p>On \\(D=\\{\\liminf X_t=-\\infty\\text{ and }\\limsup X_t=\\infty\\}\\) it holds</p> \\[ \\begin{equation*}    \\sum 1_{A_t}=\\infty\\quad \\text{and}\\quad \\sum P\\left[A_t| \\mathcal{F}_{t-1}\\right]=\\infty.  \\end{equation*} \\] <p>Since \\(P[B\\cup D]=1\\) we deduce</p> \\[ \\begin{equation*}     \\sum 1_{A_t}=\\infty\\quad \\text{if, and only if,}\\quad \\sum P[A_t|\\mathcal{F}_{t-1}]=\\infty \\end{equation*} \\] <p>almost surely. Moreover, \\(\\limsup A_t=\\{\\sum 1_{A_t}=\\infty\\}\\), hence the claim follows.</p> <p>We close these application with Borel-Cantelli's lemma.</p> <p>Borel-Cantelli's Lemma</p> <ul> <li>If \\((A_t)\\) is a sequence of events in \\(\\mathcal{F}\\) and \\(\\sum P[A_t]&lt;\\infty\\), then it holds \\(P[\\limsup A_t]=0\\).</li> <li>If \\((A_t)\\) is an independent sequence of events in \\(\\mathcal{F}\\) and \\(\\sum P(A_t)=\\infty\\), then it holds \\(P[\\limsup A_t]=1\\).</li> </ul> <p>Proof</p> <p>We consider the filtration \\(\\mathbb{F}=(\\mathcal{F}_t)_{t \\in \\mathbb{N}_0}\\) given by \\(\\mathcal{F}_0=\\{\\emptyset, \\Omega\\}\\) and \\(\\mathcal{F}_t:=\\sigma(A_s\\colon s\\leq t)\\) for \\(t\\geq 1\\). Define \\(\\xi:=\\sum P[A_t|\\mathcal{F}_{t-1}]\\). The monotone convergence theorem as well as the tower property shows that</p> \\[ \\begin{equation*}     E[\\xi]=E\\left[\\sum E[1_{A_t}|\\mathcal{F}_{t-1}]\\right]=\\sum E[E[1_{A_t}|\\mathcal{F}_{t-1}]]=\\sum P[A_t]. \\end{equation*} \\] <ol> <li> <p>If \\(\\sum P[A_t]&lt;\\infty\\), then it holds \\(P[\\xi=\\infty]=0\\).     The previous corollary yields \\(P[\\limsup A_t]=0\\).</p> </li> <li> <p>Suppose that \\((A_t)\\) is an independent sequence of events, therefore \\(A_t\\) is independent of \\(\\mathcal{F}_{t-1}\\) which implies \\(P[A_t| \\mathcal{F}_{t-1}]=P[A_t]\\) for all \\(t\\).    Hence \\(\\sum P[A_t|\\mathcal{F}_{t-1}]=\\sum P[A_t]=\\infty\\) almost surely and by the previous corollary it follows that \\(P[\\limsup A_t]=1\\).</p> </li> </ol>"},{"location":"lecture/04-Martingales/044-martingale-lp-convergence/","title":"Martingales \\(L^p\\)-Convergence","text":"<p>As astonishing and useful the martingales convergence in the \\(P\\)-almost sure case can be due to Doob's optional sampling and upcrossing lemma it is often limiting from a topological viewpoint. The \\(P\\)-almost sure convergence does not provides a satisfactory topology on spaces of random variable (in particular it is not a locally convex topology). We would prefer in most case to get stronger convergence in terms of norms.</p> <p>It turns out that convergence in \\(L^p\\) for \\(p&gt;1\\) can be derived as shown in the subsequent section due to Doob's maximal inequalities. The case of \\(L^1\\) due to its topological particularity (is not a reflexive space) is not available without additional uniform integrability conditions.</p>"},{"location":"lecture/04-Martingales/044-martingale-lp-convergence/#doobs-maximal-inequalities-and-lp-convergence","title":"Doob's Maximal Inequalities and \\(L^p\\) Convergence","text":"<p>The building block for \\(L^p\\) convergence are the so-called Doob's maximal inequalities. In the following, given a process \\(X\\) we define the</p> <ul> <li>Running supremum process \\(\\overline{X}\\) by \\(\\overline{X}_t=\\sup_{s\\leq t}X_s\\).</li> <li>Running infimum process \\(\\underline{X}\\) by \\(\\underline{X}_t=\\inf_{s\\leq t}X_s\\).</li> <li>Running absolute supremum process \\(X^\\ast\\) by \\(X^\\ast_t=\\sup_{s\\leq t} |X_s|\\).</li> </ul> <p>Proposition: Doob's Maximal Inequalities</p> <p>The following assertions hold true.</p> <ol> <li> <p>Let \\(X\\) be a sub-martingale and \\(\\lambda&gt;0\\).     Then it holds</p> \\[ \\begin{align*}    \\lambda P\\left[ \\overline{X}_t\\geq \\lambda \\right]&amp;\\leq E\\left[ 1_{\\{\\overline{X}_t \\geq \\lambda\\}}X_t \\right]\\leq E\\left[ X_t^+ \\right];\\\\    \\lambda P\\left[ \\underline{X}_t\\leq -\\lambda \\right]&amp;\\leq E\\left[ 1_{\\{\\underline{X}_t &gt;- \\lambda\\}} X_t \\right]-E[X_0]\\leq E\\left[ X_t^+ \\right]-E\\left[ X_0 \\right]. \\end{align*} \\] </li> <li> <p>For \\(X\\) a positive sub-martingale and \\(p&gt;1\\), it holds</p> \\[ \\begin{equation*}    \\left\\Vert\\sup_{s \\leq t}X_s\\right\\Vert_p\\leq \\frac{p}{p-1}\\left\\Vert X_t\\right\\Vert_p. \\end{equation*} \\] </li> </ol> <p>Remark</p> <p>Note that Doob's maximal inequalities are similar to Markov inequality. The fundamental difference though and powerfull fact is that the probability of the behavior of the whole path between \\(0\\) and \\(t\\) is controled by the expectation of the sub martingale at the begining and end.</p> <p>Note also that the second statement shows that the norm of the running maximum of the path between \\(0\\) and \\(t\\), which can be arbitrarily large, is controled by the norm at time \\(t\\). This however only holds for \\(p&gt;1\\).</p> <p>Those statements about the path being controled by the values at its boundary is certainly not trivial.</p> <p>Proof</p> <ol> <li> <p>For the stopping time \\(\\tau=\\inf\\{s:X_s\\geq \\lambda \\}\\), observe that \\(\\{\\tau\\leq t\\}=\\{\\overline{X}_t\\geq \\lambda\\}\\).     Also, on \\(\\{\\tau\\leq t\\}\\), it holds \\(X^\\tau_{t}=X_{\\tau\\wedge t}\\geq \\lambda\\).     Hence,</p> \\[ \\begin{equation*}    X_{\\tau\\wedge t}=X_{\\tau}1_{\\{\\overline{X}_t\\geq \\lambda\\}}+X_t1_{\\{\\tau&gt;t\\}}\\geq \\lambda 1_{\\{\\overline{X}_t\\geq \\lambda\\}}+X_t1_{\\{\\tau&gt;t\\}}. \\end{equation*} \\] <p>It also holds, \\(X_t1_{\\{\\tau&gt;t\\}}\\geq -X_t^-\\). Altogether, with Doob's optional sampling theorem and \\(X\\) being a sub-martingale, we get</p> \\[ \\begin{align*}    E\\left[ X_t \\right]  &amp; = E\\left[ E\\left[X_t |\\mathcal{F}_{t\\wedge \\tau} \\right] \\right]\\\\                         &amp; \\geq E\\left[ X_{\\tau\\wedge t} \\right]\\\\                         &amp; \\geq \\lambda P\\left[ \\overline{X}_t\\geq \\lambda \\right]+E\\left[ 1_{\\{\\tau&gt;t\\}} X_t\\right]\\\\                         &amp; \\geq \\lambda P\\left[ \\overline{X}_t\\geq \\lambda \\right]-E\\left[ X_t^- \\right], \\end{align*} \\] <p>and conclude the first inequality by observing that, on the one hand, \\(E[X_t^+]=E[X_t]+E[X_t^-]\\), and on the other hand,</p> \\[ \\begin{equation*}    E[X_t]-E\\left[ 1_{\\{\\tau&gt;t\\}} X_t\\right]=E[(1- 1_{\\{\\overline{X}_t&lt;\\lambda\\}}) X_t]=E[1_{\\{\\overline{X}_t\\geq \\lambda \\}}X_t]. \\end{equation*} \\] <p>As for the second inequality, for the stopping time \\(\\sigma=\\inf\\{s: X_s\\leq -\\lambda\\}\\), observe that \\(\\{\\sigma\\leq t\\}=\\{\\underline{X}_t\\leq -\\lambda\\}\\). Also, on \\(\\{\\sigma\\leq t\\}\\), it holds \\(X^\\sigma_{t}=X_{\\sigma\\wedge t}\\leq -\\lambda\\). Hence,</p> \\[ \\begin{equation*}    X_{\\sigma\\wedge t}=X_{\\sigma}1_{\\{\\underline{X}_t\\leq -\\lambda\\}}+X_t1_{\\{\\sigma&gt;t\\}}\\leq -\\lambda 1_{\\{\\underline{X}_t\\leq -\\lambda\\}}+X_t1_{\\{\\sigma&gt;t\\}}. \\end{equation*} \\] <p>Altogether, with Doob's optional sampling theorem and \\(X\\) being a sub-martingale, we get</p> \\[ \\begin{equation*}    E\\left[ X_0 \\right]\\leq E\\left[ X_{\\sigma\\wedge t} \\right]\\leq -\\lambda P\\left[ \\underline{X}_t \\leq -\\lambda\\right]+E\\left[ 1_{\\{\\sigma&gt;t\\}} X_t \\right]\\leq -\\lambda P\\left[ \\underline{X}_t \\leq -\\lambda\\right]+E\\left[ X_t^+ \\right] \\end{equation*} \\] <p>showing the second inequality by observing that \\(E[1_{\\{\\sigma&gt;t\\}}X_t]=E[1_{\\{\\underline{X}_t&gt;-\\lambda\\}}X_t]\\).</p> </li> <li> <p>Define the random variables \\(Y=\\sup_{s\\leq t}X_s\\) and \\(Z=X_t=X_t^+\\) since \\(X\\) is positive.     For \\(\\varphi\\) an increasing, right-continuous function with \\(\\varphi(0)=0\\), by Fubini's theorem and the previous inequalities, it holds</p> \\[ \\begin{align*}    E\\left[ \\varphi(Y) \\right] &amp; = E\\left[ \\int_{0}^\\infty 1_{\\{\\lambda \\leq Y\\}}d\\varphi(\\lambda) \\right]\\\\                               &amp; = \\int_{0}^\\infty P\\left[ Y\\geq \\lambda \\right]d\\varphi(\\lambda)\\\\                               &amp; \\leq \\int_0^\\infty E\\left[1_{\\{Y\\geq \\lambda\\}}Z\\right]\\frac{d\\varphi(\\lambda)}{\\lambda}\\\\                               &amp; = E\\left[ Z \\int_0^\\infty 1_{\\{Y\\geq \\lambda\\}}\\frac{d\\varphi(\\lambda)}{\\lambda} \\right]. \\end{align*} \\] <p>If we consider \\(\\varphi(\\lambda)=\\lambda^p\\), \\(p&gt;1\\), and define \\(q=p/(p-1)\\) for which holds \\(1/p+1/q=1\\), it follows from H\u00f6lder's inequality that</p> \\[ \\begin{equation*}    \\left\\Vert Y\\right\\Vert_p^p\\leq pE\\left[ Z \\int_0^\\infty 1_{\\{Y\\geq \\lambda\\}}\\lambda^{p-2}d\\lambda \\right]=\\frac{p}{p-1}E\\left[ Z Y^{p-1} \\right]\\leq q \\left\\Vert Z\\right\\Vert_p \\left\\Vert Y^{p-1}\\right\\Vert_q=q \\left\\Vert Z\\right\\Vert_p \\left\\Vert Y\\right\\Vert_p^{p/q}. \\end{equation*} \\] <p>If \\(0&lt;\\left\\Vert Y\\right\\Vert^{p/q}_p&lt;\\infty\\), dividing the inequality by \\(\\left\\Vert Y\\right\\Vert^{p/q}_p\\), noting that \\(p-p/q =1\\), yields</p> \\[ \\begin{equation*}    \\left\\Vert\\sup_{s\\leq t}X_s\\right\\Vert_p=\\left\\Vert Y\\right\\Vert_p\\leq q \\left\\Vert Z\\right\\Vert_p=q \\left\\Vert X_t\\right\\Vert_p, \\end{equation*} \\] <p>as desired.</p> </li> </ol> <p>Corollary</p> <p>If \\(X\\) is a martingale, and \\(p&gt;1\\), it holds</p> \\[ \\begin{equation}         \\left\\Vert X^\\ast_t\\right\\Vert_p=E\\left[ \\left( \\sup_{s\\leq t}\\left\\vert X_s\\right\\vert \\right)^p \\right]^{1/p}\\leq \\left(\\frac{p}{p-1}\\right) \\left\\Vert X_t\\right\\Vert \\end{equation} \\] <p>Proof</p> <p>Follows immediately that \\(|X|\\) is a sub-martingale and the doob's maximal inequalities.</p> <p>Martingale Convergence Theorem</p> <p>Let \\(X\\) be a martingale such that \\(\\sup_{t}E[|X_t|^p]&lt;\\infty\\) for some \\(p&gt;1\\). Then, there exists a random variable \\(X_{\\infty}\\) in \\(L^p\\) such that \\(X_t \\to X_{\\infty}\\) almost surely and in \\(L^p\\).</p> <p>Proof</p> <p>Since Jensen's inequality yields \\(E[X_t^+]\\leq E[|X_t|]\\leq E[|X_t|^p]^{\\frac{1}{p}}\\), it follows that \\(\\sup E[X_t^+]&lt;\\infty\\). By the \\(P\\)-almost sure martingale convergence Theorem, there exists an integrable random variable \\(X_{\\infty}\\) for which \\(X_t\\to X_{\\infty}\\) almost surely.</p> <p>We are left to show that the sequence \\(|X_t-X_\\infty|^p\\) satisfies the assumptions of Lebesgue's dominated convergence. It holds</p> \\[ \\begin{equation*}     |X_t -X_\\infty|^p\\leq c\\left(| X_t|^p + |X_\\infty|^p \\right)\\leq c\\left(\\sup |X_t|^p+|X_\\infty|^p\\right). \\end{equation*} \\] <p>On the one hand, by Fatou's lemma we have \\(E[|X_\\infty|^p]\\leq\\liminf E [|X_t|^p]&lt;\\infty\\). On the other hand, by means of the previous corollary, it holds</p> \\[ \\begin{equation*}     E[\\sup_{s\\leq t}|X_s|^p]\\leq (p/(p-1))^p E[|X_t|^p] \\end{equation*} \\] <p>showing that</p> \\[ \\begin{equation*}     E[\\sup|X_t|^p]=\\sup_tE[\\sup_{s\\leq t}|X_s|^p]\\leq (p/(p-1))^p\\sup E[|X_t|^p]&lt;\\infty.  \\end{equation*} \\] <p>Thus, the dominated convergence theorem yields \\(X_t\\to X_\\infty\\) in \\(L^p\\).</p>"},{"location":"lecture/04-Martingales/044-martingale-lp-convergence/#applications-of-lp-convergence-law-of-large-numbers","title":"Applications of \\(L^p\\) Convergence, Law of Large Numbers","text":"<p>We apply the \\(L^p\\)-convergence of martingales to show the law of large numbers that states that the sample average of independently distributed random variables with finite mean converges almost surely to its mean.</p> <p>Theorem</p> <p>Let \\(X\\) be a square integrable martingale for which holds</p> \\[ \\begin{equation*}     \\sum E\\left[ \\left( X_t-X_{t-1} \\right)^2 \\right]&lt;\\infty. \\end{equation*} \\] <p>Then, the sequence \\((X_t)\\) converges almost surely and in \\(L^2\\).</p> <p>Proof</p> <p>Beforehand, let us show the following lemma.</p> <p>Lemma</p> <p>Let \\(X\\) be a martingale such that \\(X_t\\) is square integrable for every \\(t\\). It follows that</p> \\[ \\begin{align*}     E\\left[ \\left( X_u-X_t \\right)X_s \\right] &amp; =0,\\\\     E\\left[ \\left( X_t-X_s \\right)^2|\\mathcal{F}_s \\right]&amp;=E\\left[ X_t^2|\\mathcal{F}_s \\right]-X_s^2, \\end{align*} \\] <p>for every \\(s\\leq t\\leq u\\).</p> <p>Proof</p> <p>Since \\(s\\leq t\\leq u\\) and \\(X\\) is a square integrable martingale, it follows from the properties of the conditional expectation</p> \\[ \\begin{equation*}     E\\left[ \\left( X_u-X_t \\right)X_s \\right]=E\\left[ E\\left[\\left( X_u-X_t \\right)X_s |\\mathcal{F}_t\\right]\\right]=E\\left[ E\\left[\\left( X_u-X_t \\right) |\\mathcal{F}_t\\right]X_s\\right]=0 \\end{equation*} \\] <p>showing the first equality. The same reasoning yields</p> \\[ \\begin{align*}     E\\left[ \\left( X_t-X_s \\right)^2 |\\mathcal{F}_s\\right] &amp; = E\\left[ X_t^2 |\\mathcal{F}_s\\right]-E\\left[ X_tX_s|\\mathcal{F}_s \\right]-E\\left[ (X_t-X_s)X_s|\\mathcal{F}_s \\right]\\\\     &amp; = E\\left[ X_t^2 |\\mathcal{F}_s\\right]- X_sE\\left[ X_t|\\mathcal{F}_s \\right]-X_sE\\left[ X_t-X_s|\\mathcal{F}_s \\right]\\\\     &amp; = E\\left[ X_t^2 |\\mathcal{F}_s\\right]- X_s^2, \\end{align*} \\] <p>showing the second equality.</p> <p>For every \\(t\\), by means of this lemma, it follows that</p> \\[ \\begin{align*}     E\\left[ X_t^2 \\right] &amp; = E\\left[X_0^2\\right]+\\sum_{s=1}^tE\\left[X_{s}^2-X_{s-1}^2\\right]\\\\                           &amp; = E\\left[X_0^2\\right]+\\sum_{s=1}^tE\\left[E\\left[X_{s}^2-X_{s-1}^2|\\mathcal{F}_{s-1}\\right]\\right]\\\\                           &amp; = E\\left[X_0^2\\right]+\\sum_{s=1}^tE\\left[E\\left[\\left(X_{s}-X_{s-1}\\right)^2|\\mathcal{F}_{s-1}\\right]\\right]\\\\                           &amp; = E\\left[X_0^2\\right]+\\sum_{s=1}^tE\\left[\\left(X_{s}-X_{s-1}\\right)^2\\right]\\\\                           &amp; \\leq E\\left[ X_0^2 \\right]+\\sum E\\left[ \\left( X_s-X_{s-1} \\right)^2 \\right]. \\end{align*} \\] <p>It follows that \\(\\sup_{t}E\\left[ X_t^2 \\right]&lt;\\infty\\) and therefore, by means of martingale convergence theorem, it follows that \\(X_t\\to X_\\infty\\) almost surely and in \\(L^2\\).</p> <p>Theorem</p> <p>Let \\(X\\) be a martingale and \\(a=(a_t)\\) be an increasing sequence such that \\(a_t\\to \\infty\\). If</p> \\[ \\begin{equation*}     \\sum E[(X_t-X_{t-1})^2/a_t^2]&lt;\\infty, \\end{equation*} \\] <p>then it follows that</p> \\[ \\begin{equation*}     \\frac{X_t}{a_t}\\longrightarrow 0 \\quad \\text{almost surely} \\end{equation*} \\] <p>In particular, if \\(\\sup E[(X_t-X_{t-1})^2]&lt;\\infty\\), then it holds</p> \\[ \\begin{equation*}     \\frac{X_t}{t}\\longrightarrow 0 \\quad \\text{almost surely} \\end{equation*} \\] <p>Proof</p> <p>Define the process \\(Y\\) by \\(Y_0=0\\) and \\(Y_t=\\sum_{s=1}^t (X_s-X_{s-1})/a_s\\) for \\(t\\geq 1\\). It follows that \\(Y\\) is a martingale. Indeed, adaptiveness and integrability are immediate since \\(X\\) is a martingale. As for the martingale property, it holds</p> \\[ \\begin{equation*}     E\\left[ Y_{t}-Y_{t-1} |\\mathcal{F}_t\\right]=\\frac{1}{a_t}E\\left[ X_t-X_{t-1}|\\mathcal{F}_t \\right]=0. \\end{equation*} \\] <p>Furthermore, it holds</p> \\[ \\begin{equation*}     \\sum E\\left[ \\left( Y_t-Y_{t-1} \\right)^2 \\right]=\\sum \\frac{1}{a_t^2}E\\left[ \\left( X_t-X_{t-1} \\right)^2 \\right]&lt;\\infty \\end{equation*} \\] <p>which by means of the previous Theorem implies that </p> \\[ \\begin{equation*}     Y_t=\\sum_{s=1}^t\\frac{X_s-X_{s-1}}{a_s}\\longrightarrow Y_T=\\sum \\frac{X_t-X_{t-1}}{a_t} \\end{equation*} \\] <p>almost surely and in \\(L^2\\). The Kronecker's lemma states that if \\(\\sum b_t/a_t&lt;\\infty\\) for two sequences \\((a_t)\\) and \\((b_t)\\) whereby \\((a_t)\\) is an increasing sequence of strictly positive numbers, it follows that \\((\\sum b_t)/a_t=0\\). Hence, applying Kronecker's lemma, it follows that</p> \\[ \\begin{equation*}     \\frac{X_t}{a_t}=\\frac{1}{a_t}\\sum_{s=1}^t X_s-X_{s-1}\\to 0 \\end{equation*} \\] <p>almost surely. In particular, if \\(\\sup E[(X_t-X_{t-1})^2]&lt;\\infty\\) it follows that </p> \\[   \\sum E[(X_t-X_{t-1})^2/t^2]\\leq \\sup E[(X_t-X_{t-1})^2]\\sum \\frac{1}{t^2}&lt;\\infty \\] <p>and the second assertion of the theorem follows.</p> <p>Corollary</p> <p>Let \\((X_t)\\) be a sequence of integrable independent random variables such that \\(E[X_t]=0\\) for every \\(t\\) and such that \\(\\sum E[X_t^2]/a_t^2&lt;\\infty\\) for some increasing sequence \\((a_t)\\) of strictly positive real numbers such that \\(a_t\\to \\infty\\). Then it holds</p> \\[ \\begin{equation*}     \\frac{1}{a_t}\\sum_{s=1}^t X_s\\longrightarrow 0 \\quad \\text{almost surely} \\end{equation*} \\] <p>Proof</p> <p>Define \\(\\mathcal{F}_0=\\{\\emptyset,\\Omega\\}\\) and \\(\\mathcal{F}_t=\\sigma(X_s\\colon s\\leq t)\\) and the process \\(S\\) by \\(S_0=0\\) and \\(S_t=\\sum_{s=1}^t X_s\\). It follows that \\(S\\) is a martingale. Indeed, it is integrable by assumption. It is furthermore adapted since \\(\\mathbb{F}\\) is the filtration generated by \\(X\\). Finally due to the independence, it follows that</p> \\[ \\begin{equation*}     E\\left[ S_t-S_{t-1} |\\mathcal{F}_{t-1}\\right]=E\\left[ X_t|\\mathcal{F}_{t-1} \\right]=E\\left[ X_t \\right]=0. \\end{equation*} \\] <p>Furthermore, since</p> \\[ \\begin{equation*}     \\sum \\frac{1}{a_t^2}E\\left[ \\left( S_t-S_{t-1} \\right)^2 \\right]=\\sum \\frac{1}{a_t^2}E\\left[ X_t^2 \\right]&lt;\\infty, \\end{equation*} \\] <p>Applying the previous theorem yields</p> \\[ \\begin{equation*}     \\frac{S_t}{a_t}=\\frac{1}{a_t}\\sum_{s=1}^t X_s\\longrightarrow 0 \\quad \\text{almost surely} \\end{equation*} \\] <p>Theorem: Strong Law of Large Numbers</p> <p>Let \\((X_t)\\) be a sequence of integrable, independent, and identically distributed random variables. Then it holds</p> \\[ \\begin{equation*}     \\frac{1}{t}\\sum_{s=1}^t X_s\\xrightarrow[t \\to \\infty]{\\text{almost surely}}E\\left[ X_1 \\right]. \\end{equation*} \\] <p>Note</p> <p>Note that we do not make here the traditional assumption that the sequence shall be square integrable.</p> <p>Proof</p> <p>Step 1: Define first the countable family \\((A_t)\\) as \\(A_t=\\{ |X_t|&gt;t\\}\\) of events in \\(\\mathcal{F}\\). Using the fact that \\(X_t\\sim X_1\\) for every \\(t\\) and Fubini's Theorem, it holds</p> \\[ \\begin{equation*}     \\sum P\\left[ A_t \\right]=\\sum P\\left[ |X_t|&gt;t \\right]=\\sum P\\left[ |X_1|&gt;t \\right]\\leq \\int_{0}^\\infty P\\left[ |X_1|&gt;\\lambda \\right]d\\lambda=E\\left[ |X_1| \\right]&lt;\\infty. \\end{equation*} \\] <p>By Borel-Cantelli, it follows that \\(P[\\limsup A_t]=0\\) and therefore, for almost all \\(\\omega\\) in \\(\\Omega\\), there exists \\(t_0(\\omega)\\) such that \\(\\omega\\) does not belong to \\(A_t\\) for every \\(t\\geq t_0\\). Hence, defining \\(Y_t=X_t1_{A_t^c}\\), it follows that </p> \\[ \\begin{equation*}     \\liminf \\frac{1}{t}\\sum_{s\\leq t} X_s=\\liminf \\frac{1}{t}\\sum_{s\\leq t}  Y_s\\quad \\text{as well as}\\quad \\limsup \\frac{1}{t} \\sum_{s\\leq t}X_s=\\limsup \\frac{1}{t}\\sum_{s\\leq t} Y_s, \\end{equation*} \\] <p>and so we just have to show that</p> \\[ \\begin{equation*}     \\frac{1}{t}\\sum_{s\\leq t} Y_s\\xrightarrow[t \\to \\infty]{\\text{almost surely}} E\\left[ X_1 \\right]. \\end{equation*} \\] <p>Step 2: Let \\(Z_t=Y_t-E[Y_t]\\) for every \\(t\\) which is an independent sequence of random variables. Furthermore, note that</p> \\[ \\begin{equation*}     \\sum \\frac{E[Z_t^2]}{t^2}=\\sum \\frac{E\\left[ (Y_t-E[Y_t])^2 \\right]}{t^2}=\\sum \\frac{E\\left[ Y_t^2 \\right]-E\\left[ Y_t \\right]^2}{t^2}\\leq \\sum \\frac{E[Y_t^2]}{t^2}. \\end{equation*} \\] <p>By Fubini's theorem, and the fact that \\(P[|Y_t|&gt;s]=P[|Y_t|&gt;t]=0\\) for every \\(s\\geq t\\) as well as \\(P[Y_t&gt;\\lambda ]\\leq P[X_t&gt;\\lambda]=P[X_1&gt;\\lambda]\\) for every \\(t\\), it holds</p> \\[ \\begin{equation*}     E\\left[ Y_t^2 \\right]=E\\left[ \\int_{0}^\\infty 1_{\\{|Y_t|&gt;\\lambda\\}}2\\lambda d\\lambda \\right]=\\int_{0}^\\infty P\\left[ |Y_t|&gt;\\lambda \\right]2\\lambda d\\lambda \\leq\\int_{0}^t P\\left[ |X_1|&gt;\\lambda \\right]2\\lambda d\\lambda. \\end{equation*} \\] <p>The monotone convergence of Lebesgue yields</p> \\[ \\begin{equation*}     \\sum \\frac{1}{t^2}\\int_{0}^t P\\left[ |X_1|&gt;\\lambda \\right]2\\lambda d\\lambda=\\int_{0}^\\infty \\sum \\frac{1_{\\{t\\geq \\lambda\\}}}{t^2}P\\left[ |X_1|&gt;\\lambda \\right]2\\lambda d\\lambda. \\end{equation*} \\] <ul> <li> <p>For \\(\\lambda &lt;1\\), it holds</p> \\[ \\begin{equation*}   2 \\lambda \\sum \\frac{1}{t^2}=2\\lambda\\frac{\\pi^2}{6}\\leq 4\\lambda\\leq 4. \\end{equation*} \\] </li> <li> <p>For \\(\\lambda\\geq 1\\), it holds</p> \\[ \\begin{equation*}   2\\lambda \\sum_{t\\geq \\lambda}\\frac{1}{t^2}\\leq \\frac{2}{\\lambda}+2\\lambda\\int_{\\lambda}^\\infty \\frac{1}{x^2}dx\\leq 2+2\\lambda \\frac{1}{\\lambda}=4. \\end{equation*} \\] </li> </ul> <p>Hence,</p> \\[ \\begin{equation*}     \\sum \\frac{E\\left[ Z_t^2 \\right]}{t^2}\\leq 4 \\int_0^\\infty P\\left[ |X_1|&gt;\\lambda \\right]d\\lambda=4E\\left[ |X_1| \\right]&lt;\\infty. \\end{equation*} \\] <p>According to the previous corollary, it follows that \\((\\sum_{s\\leq t} Z_s)/t\\to 0\\) almost surely.</p> <p>Step 3: We finally show that \\((\\sum_{s\\leq t} Y_s)/t\\to E[X_1]\\) almost surely. It holds</p> \\[ \\begin{align*}     \\left| \\frac{1}{t}\\sum_{s\\leq t}Y_s-E[X_1]\\right|         &amp; \\leq \\left| \\frac{1}{t} \\sum_{s\\leq t}Z_s\\right|+\\frac{1}{t}\\sum_{s\\leq t}\\left|E\\left[Y_s  \\right]-E\\left[ X_s \\right]\\right|\\\\         &amp; = \\left| \\frac{1}{t} \\sum_{s\\leq t}Z_s\\right|+\\frac{1}{t}\\sum_{s\\leq t}\\left|E\\left[X_s1_{\\{|X_s|\\leq s\\}}  \\right]-E\\left[ X_s \\right]\\right|\\\\         &amp; \\leq \\left| \\frac{1}{t} \\sum_{s\\leq t}Z_s\\right|+\\frac{1}{t}\\sum_{s\\leq t}E\\left[|X_s|1_{\\{|X_s|&gt; s\\}}  \\right]\\\\         &amp; = \\left| \\frac{1}{t} \\sum_{s\\leq t}Z_s\\right|+\\frac{1}{t}\\sum_{s\\leq t}E\\left[ |X_1|1_{|X_1|&gt;s} \\right]\\\\         &amp; =\\left| \\frac{1}{t} \\sum_{s\\leq t}Z_s\\right|+E\\left[|X_1|\\frac{1}{t}\\sum_{s\\leq t} 1_{\\{|X_1|&gt;s\\}}\\right]\\\\         &amp; =I_t+E[J_t]. \\end{align*} \\] <p>We already shown in the previous step that \\(I_t\\to 0\\) almost surely. On the other hand, it holds \\(J_t\\leq |X_1|\\) with \\(E[|X_1|]&lt;\\infty\\) and since \\((\\sum_{s\\leq t} 1_{\\{|X_1|&gt;s\\}})/t\\to 0\\) almost surely, it follows that \\(J_t\\to 0\\) almost surely. Hence, by Lebesgue's dominated convergence theorem, it follows that \\(E[J_t]\\to 0\\) which ends the proof.</p>"},{"location":"lecture/04-Martingales/044-martingale-lp-convergence/#l1-convergence","title":"\\(L^1\\) Convergence","text":"<p>The \\(L^1\\)-case is more delicate. There we have to make use of uniform integrability in order to show similar results.</p> <p>Theorem: \\(L^1\\)-Martingale Convergence</p> <p>Let \\(X\\) be a sub-martingale bounded in \\(L^1\\), that is, \\(\\sup E[|X_t|]&lt;\\infty\\) or equivalently \\(\\lim E[X_t]&gt;-\\infty\\). In particular, by almost sure martingale convergence Theorem, there exists \\(X_\\infty \\in L^1\\) such that \\(X_t\\to X_\\infty\\) almost surely. Then the following two assertions are equivalent:</p> <ol> <li>\\(X\\) is uniformly integrable;</li> <li>\\(X_t\\to X_{\\infty}\\) in \\(L^1\\) and \\(E[ X_{\\infty}|\\mathcal{F}_t ]\\geq X_t\\) for every \\(t\\).</li> </ol> <p>Proof</p> <p>If \\(X_t\\to X_\\infty\\) almost surely, uniform integrability of \\(X\\) and \\(L^1\\) convergence are equivalent by means of Uniform Integrability Theorem. In particular, by the sub-martingale property, for every event \\(A\\) in \\(\\mathcal{F}_t\\), it holds that</p> \\[ \\begin{equation*}     E[X_{\\infty}1_A]=\\lim_{s\\geq t}E\\left[ X_{s}1_A \\right]\\geq E[X_t1_A], \\end{equation*} \\] <p>showing that (i) is equivalent to (ii).</p> <p>The following corollaries are consequences of this theorem as well as uniform integrability properties.</p> <p>Corollary</p> <p>Let \\(X\\) be a martingale. Then the following assertions are equivalent:</p> <ul> <li>\\(X_t=E[\\xi|\\mathcal{F}_t]\\) for some \\(\\xi\\in L^1\\);</li> <li>\\(X\\) is uniformly integrable;</li> <li>\\(X_{t}\\to X_{\\infty}\\) in \\(L^1\\).</li> </ul> <p>Let \\(\\xi\\) be an integrable random variable and \\(\\mathcal{F}_\\infty=\\sigma(\\cup \\mathcal{F}_t)\\). Then it follows that </p> \\[ \\begin{equation*}     X_t:=E[\\xi|\\mathcal{F}_t]\\to X_{\\infty}:=E[\\xi|\\mathcal{F}_{\\infty}] \\end{equation*} \\] <p>almost surely and in \\(L^1\\).</p> <p>Proof</p> <p>The proof is left as an exercise for the first part of the corollary.</p> <p>We show the second part. Clearly, \\(X\\) is a martingale which is uniformly integrable. Hence, from the previous theorem, it converges to \\(X_\\infty\\) almost surely and in \\(L^1\\). We are left to show that \\(X_{\\infty}=E\\left[ \\xi|\\mathcal{F}_{\\infty} \\right]\\). Let </p> \\[ \\mathcal{C}=\\{A \\in \\mathcal{F}_{\\infty}\\colon E[X_{\\infty}1_A]=E[\\xi1_A]\\}. \\] <p>For every \\(A \\in\\mathcal{F}_t\\), it holds</p> \\[ \\begin{equation*}     E[X_{\\infty}1_A]=\\lim E[X_t1_A]=\\lim E\\left[ E[\\xi|\\mathcal{F}_t]1_A \\right]=E[\\xi1_A], \\end{equation*} \\] <p>showing that \\(\\cup \\mathcal{F}_t\\) is in \\(\\mathcal{C}\\). Since \\(\\mathcal{C}\\) is clearly a \\(\\lambda\\)-system, by Dynkin's Lemma, it follows that \\(\\mathcal{C}\\) contains \\(\\mathcal{F}_{\\infty}\\), ending the proof.</p>"},{"location":"lecture/05-Markov/050-introduction/","title":"Markov Processes Browninan Motion","text":"<p>This Chapter serves as a bridge in our study of stochastic processes towards the continuous time and the stochastic integral. The goal is to introduce the concepts of Markov processes\u2014that is, memoryless processes. We will start with the definition and key properties of general markov processes and how to construct them. We turn to specific applications in discrete time and then turn to the construction of a central Markov process, and martingale, namely the Brownian motion.</p> <ul> <li>Markov Processes</li> <li>Markov Chains</li> <li>Brownian Motion</li> </ul>"},{"location":"lecture/05-Markov/051-markov-extension/","title":"Markov Chain and Extension","text":"<p>In this section we will study a special class of stochastic processes namely those which immediate next evolution only depends on the current state. In other terms they are memory less. We will first introduce them in general framework and dive down into the more intuitive markov chain framework.</p> <p>In this section we consider a time index \\(\\mathbf{T}\\) which can be either \\(\\mathbb{N}_0\\) or \\([0,\\infty)\\). The Markov process will take value in a measurable state space \\((S, \\mathcal{S})\\) where \\(S\\) is a closed or open subset of \\(\\mathbb{R}^d\\) and \\(\\mathcal{S}\\) is the Borel \\(\\sigma\\)-Algebra\u2014in particular \\(S\\) is a Polish space. Though we can always consider processes from any abstract probability space, it is always possible to map it on the canonical space of paths with the product \\(\\sigma\\)-algebra.</p> <p>Definition: Path Space (or Canonical Space)</p> <p>The canonical space with values in \\((S, \\mathcal{S})\\) is given by the triplet</p> \\[   (\\Omega, \\mathcal{F}, \\mathbb{F}) \\] <p>together with the canonical process \\(X = (X_t)_{t \\in \\mathbf{T}}\\) where</p> <ul> <li> <p>State Space: </p> \\[   \\Omega = \\left\\{ \\omega = (\\omega_t)_{t \\in \\mathbf{T}}\\colon \\omega_t \\in S \\right\\} = S^{\\mathbf{T}} \\] </li> <li> <p>\\(\\sigma\\)-Algebra: is the resulting product \\(\\sigma\\)-algebra</p> \\[   \\mathcal{F} = \\otimes_{t \\in \\mathbf{T}} \\mathcal{S} \\] </li> <li> <p>Canonical Process: \\(X = (X_t)_{t \\in \\mathbf{T}}\\) where</p> \\[   \\begin{equation*}     \\begin{split}       X_t \\colon \\Omega &amp;\\longrightarrow S \\\\                 \\omega = (\\omega_s)_{s \\in \\mathbf{T}} &amp; \\longmapsto X_t(\\omega) = \\omega_t     \\end{split}   \\end{equation*} \\] <p>which is basically the projection process. By definition of the product \\(\\sigma\\)-algebra \\(X\\) is a stochastic process.</p> </li> <li> <p>Filtration: \\(\\mathbb{F}=(\\mathcal{F}_t)_{t \\in \\mathbf{T}}\\) where</p> \\[   \\mathcal{F}_t = \\sigma\\left( X_s \\colon s\\leq t \\right) \\] <p>turning \\(X\\) into an adapted process, its own filtration.</p> </li> </ul> <p>Definition: Markov Process on Canonical Space</p> <p>The canonical process \\(X\\) under a given probability measure \\(P\\) is called a Markov process if it satisfies the Markov property:</p> \\[ \\begin{equation}     P\\left[ X_{t} \\in B |\\mathcal{F}_s \\right]=P\\left[ X_{t} \\in B |X_s \\right] \\end{equation} \\] <p>for every \\(s\\leq t\\) and Borel set \\(B \\in \\mathcal{S}\\).</p> <p>The probability distribution \\(B\\mapsto \\mu[B]:=P[X_0 \\in B]\\) is called the start distribution of the Markov process.</p> <p>Note that the Markov property is equivalent to \\(E[ f(X_t)|\\mathcal{F}_s ]=E[ f(X_t)| X_s ]\\) for every bounded measurable function \\(f:S \\to \\mathbb{R}\\).</p> <p>Remark</p> <p>In this definition, a Markov process is in fact the specification of a probability measure on the path space such that the canonical process satisfies the Markov property.</p> <p>Note that we can alternatively give ourselves any abstract filtrated probability space \\((\\tilde{\\Omega}, \\tilde{\\mathcal{F}}, \\tilde{\\mathbb{F}},\\tilde{P})\\) and define Markov processes as adapted processes \\(\\tilde{X}\\) satisfying the markov property.</p> <p>However the mapping \\(\\omega \\mapsto (t\\mapsto \\tilde{X}_t(\\omega))\\) from \\(\\tilde{\\Omega}\\) to \\(\\Omega\\) with the pullback measure \\(P:=\\tilde{P}\\circ\\tilde{X}^{-1}\\) specifies the probability measure \\(P\\) under which the canonical process \\(X\\) is a Markov process with the same start distribution and same law.</p> <p>From this remark, we see that defining a Markov process corresponds precisely to specifying the probability on the canonical space \\(\\Omega = S^{\\mathbf{T}}\\). In other terms, providing the structure of the mappings</p> \\[ \\begin{equation*}     P_{s,t}(B):=P\\left[ X_t \\in B |X_s \\right] \\quad \\text{and}\\quad \\mu(B)=P[X_0 \\in B] \\end{equation*} \\] <p>for every \\(B \\in \\mathcal{S}\\) and \\(s\\leq t\\).  </p> <p>Example</p> <p>A typical way to see a discrete time finite state Markov process is as follows. We set \\(S=\\{1,2,3\\}\\) and \\(\\mu:=\\delta_1\\). We further assume that the Markov process \\(X\\) is time homogeneous, that is \\(P[X_{t+1}\\in B|X_s]=P[X_1 \\in B|X_0]\\) providing the following evolution starting from \\(1\\).</p> \\[ \\begin{equation*}     p=\\begin{pmatrix}         0 &amp; 1/2 &amp; 1/2 \\\\ 1/2 &amp;         1/4 &amp; 1/4\\\\ 2/3 &amp; 1/3 &amp; 0     \\end{pmatrix}.      \\end{equation*} \\] <pre><code>flowchart LR\n  s2((State 2)) --&gt;|1/4| s2\n  s2 --&gt;|1/2| s1((State 1))\n  s1 --&gt;|1/2| s2\n  s1 --&gt;|1/2| s3\n  s3 --&gt;|2/3| s1\n  s3 --&gt;|1/3| s2\n  s2 --&gt;|1/4| s3((State 3))</code></pre> <p>Random walk</p> <p>Let \\(Y\\) be a discrete stochastic process of independent random variables with values in \\(S=\\mathbb{Z}^d\\). Define \\(X_t:=\\sum_{s=0}^{t} Y_s\\) and \\(\\mathcal{F}_t:=\\sigma(X_s\\colon s\\leq t)\\). For \\(y\\in\\mathbb{Z}^d\\), it holds</p> \\[ \\begin{align*}     P[X_{t+1}=y | \\mathcal{F}_t]     &amp; =P[Y_{t+1} =y-X_t | \\mathcal{F}_t]     \\\\     &amp;=\\sum_{x\\in\\mathbb{Z}^d} P[Y_{t+1}=y-x | \\mathcal{F}_t] 1_{\\{X_t=x\\}}      \\\\     &amp; = \\sum_{x\\in\\mathbb{Z}^d} P[Y_{t+1}=y-x ] 1_{\\{X_t=x\\}}      \\\\     &amp; = \\sum_{x\\in\\mathbb{Z}^d} P[Y_{t+1}=y-x | X_t] 1_{\\{X_t=x\\}}     \\\\     &amp; = P[Y_{t+1}=y-X_t | X_t]     \\\\     &amp; = P[X_{t+1}=y | X_t]. \\end{align*} \\] <p>In both examples, we are either providing the transition probabilities or assuming a probability measure on the full space without checking whether such a probability measure can be constructed.</p> <p>Hence</p> <ul> <li>A Markov process defines a start measure \\(\\mu_0\\) and family of transition probabilities \\(P_{st}(\\cdot, \\cdot)\\). What are these transition probabilities? What are their properties?</li> <li>Is it possible to construct a markov process (or in other terms a probability measure \\(P\\) on the path space) from families of transition probabilities? If so, under which conditions?</li> </ul> <p>This is the content of the following section</p>"},{"location":"lecture/05-Markov/051-markov-extension/#from-markov-process-to-transition-probability","title":"From Markov Process to Transition probability","text":"<p>Every \\(\\sigma(X_s)\\)-measurable random variable \\(Y\\) can be written as \\(Y = f(X_s)\\) for some measurable function \\(f:S\\to \\mathbb{R}\\). Hence, there exist measurable functions \\(x\\mapsto P_{st}(x,B)\\) such that</p> \\[ \\begin{equation*}     P_{st}(X_s,B)=P\\left[ X_t\\in B|X_s \\right] \\end{equation*} \\] <p>for every \\(s&lt;t\\) and \\(B\\in \\mathcal{S}\\). Furthermore, it seems very natural that \\(B\\mapsto P_{st}(x,B)\\) should be a probability measure. This leads us to the notion of stochastic kernel and transition probability which is an alternative way to see Markov processes.</p> <p>Definition: Stochastic Kernel (see Fubini-Tonelli in Chapter 3)</p> <p>A stochastic kernel on \\((S, \\mathcal{S})\\) is a function \\(K:S\\times \\mathcal{S}\\to [0,\\infty)\\) such that</p> <ul> <li>\\(x \\mapsto K(x,B)\\) is measurable for every event \\(B\\) in \\(\\mathcal{S}\\).</li> <li>\\(B \\mapsto K(x,B)\\) is a measure for every \\(x\\) in \\(S\\).</li> </ul> <p>If \\(K(x,\\cdot)\\) is a probability measure, then we call it sometimes refered to as a transition probability.</p> <p>For a measurable function \\(f:S\\to \\mathbb{R}\\) and two kernels \\(K\\) and \\(L\\), we define</p> \\[ \\begin{equation*}     Kf(x)=\\int_{}^{} f(y)K(x,dy) \\quad \\text{and}\\quad KL(x,A)=\\int_{}^{}L(y,A) K(x,dy) \\end{equation*} \\] <p>An easy exercise shows that \\(Kf\\) is a measurable function and that \\(KL\\) is once again a stochastic kernel. The question of whether the mapping \\(B\\mapsto P_{st}(x,B)\\) is a probability measure for every \\(x\\) is in general not true. However, it holds on Borel spaces as the following proposition shows.</p> <p>Regular conditional distribution</p> <p>Let \\(X\\) and \\(Y\\) be two \\(S\\) valued random variables. Then, there exists a stochastic kernel \\(K:S\\times \\mathcal{S}\\to [0,1]\\) such that</p> \\[ K\\left( X,B \\right)= P\\left[ Y \\in B | X \\right] \\] <p>almost surely for every Borel set \\(B\\) and this stochastic kernel is unique almost surely.</p> <p>Proof</p> <p>Without loss of generality, we may assume that \\(S=\\mathbb{R}\\). For every rational \\(r\\), there exists a measurable function \\(f(\\cdot,r): S\\to [0,1]\\) such that \\(f(X,r)=P[ Y\\leq r |X ]\\) almost surely. Since \\(f(X,r)\\leq f(X,q)\\) almost surely for every \\(r\\leq q\\) and \\(\\lim_{r \\to  \\infty} f(X,r) =1\\) and \\(\\lim_{r\\to -\\infty}f(X,r)=0\\) almost surely, and these conditions are countable and only depending on \\(X\\), there exists a set \\(A\\in \\sigma(X)\\) of measure one such that \\(f(x,r)\\) is increasing with limit at \\(\\pm \\infty\\) being \\(0\\) or \\(1\\).</p> <p>Hence, defining</p> \\[ F(x,t)=1_A(x)\\inf_{r&gt;t, r\\in \\mathbb{Q}}f(x,r)+1_{A^c}(x)1_{[0,\\infty)}(t) \\] <p>we get that \\(F(x,\\cdot)\\) is a cumulative distribution function for every \\(x\\in \\mathbb{R}\\). In particular, there exists probability measures \\(K(x, \\cdot)\\) such that</p> \\[ K(x,B)=\\int_{B}^{} F(x,dt) \\] <p>and by construction \\(x\\mapsto K(x,B)\\) is measurable for every \\(x\\). A monotone class argument together with monotone convergence for conditional expectation shows that</p> \\[ K(X,B)=P\\left[ Y \\in B|X \\right] \\] <p>and uniqueness almost surely follows from the uniqueness almost surely of the conditional expectation.</p> <p>Remark</p> <p>In particular for any bounded measurable function \\(f:\\mathbb{R}^2\\to \\mathbb{R}\\), it holds</p> \\[ E\\left[ f(X,Y)| X \\right]=\\int_{}^{} f(X,y)K(X,dy) = g(X) \\] <p>where</p> \\[ g(x) = \\int_{}^{} f(x,y)K(x,dy) \\] <p>This statement leads to the following proposition.</p> <p>Markov process transition probabilities</p> <p>If \\(X\\) is a Markov process under \\(P\\) with start distribution \\(\\mu\\), there exists a family \\((P_{st})_{s&lt;t}\\) of transition probabilities such that</p> \\[ P\\left[ X_t \\in B |\\mathcal{F}_s \\right]=P_{st}(X_s,B) \\] <p>for every \\(s&lt;t\\). Furthermore, \\((P_{st})_{s&lt;t}\\) satisfies the semi-group property</p> \\[ P_{st}P_{tu}=P_{su} \\] <p>for every \\(s&lt;t&lt;u\\).</p> <p>Proof</p> <p>Suppose that \\(X\\) is a Markov process. According to Proposition \"Regular conditional distribution\", it follows that there exists a family of stochastic kernels \\(P_{st}\\) such that \\(P[ X_t \\in B|X_s ]=P_{st}(X_s, B)\\), hence Property.</p> <p>As for the semi-group property, it follows from the tower property of conditional expectation. Indeed, it holds \\(P_{st}(x,B)=P[X_t\\in B|X_s=x]\\) almost surely for every \\(x\\), \\(B\\in \\mathcal{S}\\) and \\(s&lt;t\\) as well as \\(E[f(X_t)|\\mathcal{F}_s]=E[f(X_t)|X_s]=\\int_{}^{} f(y)P_{st}(X_s,dy)\\). Hence, using the Markov property,</p> \\[ P_{st}P_{tu}(X_s,B)=\\int_{S}^{} P_{tu}(y,B)P_{st}(X_s,dy)=E\\left[ P_{tu}(X_t,B)|\\mathcal{F}_s \\right] = E\\left[ E\\left[ 1_{\\{X_u \\in B\\}}|X_t \\right]|\\mathcal{F}_s \\right] \\] \\[ E\\left[ E\\left[ 1_{\\{X_u \\in B\\}} |\\mathcal{F}_t\\right] |\\mathcal{F}_s\\right]=E\\left[ 1_{\\{X_u \\in B\\}} |\\mathcal{F}_s\\right]=E\\left[ 1_{\\{X_u \\in B\\}} | X_s\\right]=P_{su}(X_s,B) \\] <p>By the uniqueness of the stochastic kernel, we deduce that \\(P_{st}P_{tu}=P_{su}\\).</p>"},{"location":"lecture/05-Markov/051-markov-extension/#from-transition-probabilities-to-markov","title":"From Transition Probabilities to Markov","text":"<p>The most interesting, and involving question though is the reciprocal. In other terms, given a start distribution \\(\\mu\\) and a family of transition probabilities, does there exists a probability measure \\(P\\) on the path space such that the canonical process is a Markov process with the given transition probabilities? Is this process unique? It is indeed true and based on the fondamental Kolmogorov extension Theorem that we will treat in a separate subsection during the construction of the Brownian motion.</p> <p>Existence of Markov process</p> <p>Given a probability distribution \\(\\mu\\) and a family of transition probabilities \\((P_{st})_{s&lt;t}\\) satisfying the semi-group property, then there exists a probability measure \\(P^\\mu\\) such that \\(X\\) is a Markov process with start distribution \\(\\mu\\) and such that the transition probability holds.</p> <p>Proof</p> <p>We define the family of probability measures \\((P^\\mu_I)\\) for every finite family \\(I=\\{0&lt;t_1&lt;\\ldots&lt;t_n\\}\\subseteq \\mathbf{T}\\) by setting for every \\(n+1\\)-dimensional Borel set \\(A_0\\times A_1\\times \\cdots A_n\\)</p> \\[ P^\\mu_I(A_0\\times \\ldots \\times A_n )=\\int_{A_0}^{}\\mu(dx_0)\\int_{A_1}^{}P_{0t_1}(x_0,dx_1)\\int_{A_2}^{}P_{t_1t_2}(x_1,dx_2)\\cdots \\int_{A_n}^{} P_{t_{n-1}t_n}(x_{n-1},dx_n) \\] <p>Due to the semi-group property, it defines a consistent family of probability measures. Hence, by means of the Kolmogorov extension theorem, we can extend this projective family to a unique probability measure \\(P^\\mu\\) such that \\(X\\) by definition fulfills the transition probability property. Hence, \\(X\\) is a Markov process under \\(P^\\mu\\) with start distribution \\(\\mu\\) and transition probability \\((P_{st})_{s&lt;t}\\).</p> <p>Remark</p> <p>Note that given a family of transition probabilities \\((P_{st})\\) we can define a probability measure \\(P^\\mu\\) on the path space such that \\(X\\) is a Markov process with start distribution \\(\\mu\\). We denote in that case \\(E^\\mu\\) the expectation under this probability measure. This is in particular the case for all the Dirac measures \\(\\delta_{x}\\) for \\(x\\) in \\(S\\), that is, for Markov processes starting at \\(x\\). We denote \\(P^x\\) instead of \\(P^{\\delta_x}\\) and \\(E^x\\) instead of \\(E^{\\delta_x}\\). Given \\(P^x\\) for every \\(x\\) and a distribution \\(\\mu\\), we can recover any \\(P^\\mu\\) as follows.</p> \\[ P^{\\mu}[A]=\\int_{}^{} P^{x}[A] \\mu(dx) \\] <p>In fact, \\((x,A)\\mapsto P^{x}[A]\\) is a stochastic kernel on \\(S\\times \\mathcal{F}\\) and therefore the relation</p> \\[ P^{\\mu}[A]=\\int_{}^{} P^{x}[A] \\mu(dx) \\]"},{"location":"lecture/05-Markov/051-markov-extension/#markov-property","title":"Markov Property","text":"<p>From now on, we will only consider time homogeneous Markov processes:</p> <p>Time homogeneous Markov process</p> <p>A Markov process is called time homogeneous if and only if \\(P_{st}=P_{0t-s}\\), or in other terms, \\(P[X_t\\in B|X_s]=P[X_{t-s}\\in B|X_0]\\).</p> <p>In that case, we only have to specify the start distribution \\(\\mu\\) and a family of transition probabilities \\(P_t\\).</p> <p>Let \\(Z\\) be a random variable on \\(\\Omega\\), that is, \\(\\mathcal{F}=\\mathcal{F}_{\\infty}=\\sigma(X_t\\colon t \\in \\mathbf{T})\\)-measurable. It is, in particular, a function \\(\\omega=(\\omega_t)\\mapsto Z(X(\\omega))=Z((\\omega)_{t \\in \\mathbf{T}})\\).</p> <p>However:</p> <ul> <li>If \\(Z\\) is \\(\\mathcal{F}_{t}=\\sigma(X_s\\colon s\\leq t)\\)-measurable, then \\(Z\\) can actually be seen as a measurable function that only depends on the paths up to time \\(t\\), that is, \\(\\omega \\mapsto Z((X_s(\\omega))_{s\\leq t})=Z((\\omega)_{s\\leq t})\\).</li> <li>If \\(Z\\) is \\(\\sigma(X_t)\\)-measurable, then it can be seen as a measurable function that only depends on the value of the path at time \\(t\\), that is, \\(\\omega \\mapsto Z(X_t(\\omega))=Z(\\omega_t)\\).</li> </ul> <p>In particular, for the shift operator of a path by \\(t\\):</p> \\[ \\begin{aligned}     \\theta_t :\\Omega &amp; \\longrightarrow \\Omega\\\\     \\omega =(\\omega_s)_{s \\in \\mathbf{T}} &amp; \\longmapsto \\theta_t(\\omega)=(\\omega_{s+t})_{s\\in \\mathbf{T}} \\end{aligned} \\] <p>we have that \\(Z\\circ \\theta_t(\\omega)=Z\\left( (\\omega_{s+t})_{s\\in \\mathbf{T}} \\right)\\) is the evaluation of the random variable \\(Z\\) on the shifted path space by \\(t\\). In particular, if \\(Z\\) is \\(\\sigma(X_s)\\)-measurable, then it holds that \\(Z\\circ \\theta_t(\\omega)=Z(\\omega_{t+s})\\) is \\(\\sigma(X_{t+s})\\)-measurable.</p> <p>Markov Property</p> <p>Let \\((P_{t})\\) be a family of transition probabilities and \\(\\mu\\) be a probability distribution. For every bounded or positive measurable random variable \\(Z\\), it holds that \\(x \\mapsto E^x[Z]\\) is measurable. In particular, for every \\(t\\), the function \\(E^{X_t}[Z]\\) defined as</p> \\[ \\omega \\mapsto E^{X_t(\\omega)}[Z] \\] <p>is \\(\\sigma(X_t)\\)-measurable, and the Markov property holds</p> \\[ E^\\mu\\left[ Z\\circ \\theta_t |\\mathcal{F}_t  \\right]=E^{X_t}\\left[ Z \\right] \\] <p>Proof</p> <p>By monotony and approximation arguments, it is enough to show the proposition for the function \\(Z = 1_{A_0\\times A_1\\times \\ldots \\times A_n}(X_0,X_{t_{1}},\\ldots, X_{t_n})\\) where \\(A_k\\) are Borel sets. By definition of \\(P^x\\) and the previous remarks, it follows that</p> \\[   \\begin{align*}      E^x[Z] &amp; = \\int_{A_0}^{}\\delta_x(dx_0)\\int_{A_1}^{}P_{t_1}(x_0,dx_1)\\ldots \\int_{A_n}^{}P_{t_n-t_{n-1}}(x_{n-1},dx_n)\\\\             &amp; = 1_{A_0}(x)\\int_{A_1}^{}P_{t_1}(x,dx_1)\\ldots \\int_{A_n}^{}P_{t_n - t_{n-1}}(x_{n-1},dx_n)   \\end{align*} \\] <p>Since \\(x \\mapsto 1_{A_0}(x)\\) is measurable, and the \\((P_{st})\\) are stochastic kernels, the first assertion follows. In particular, for this specific form of \\(Z\\), it holds  </p> \\[ E^{X_t}[Z]=1_{A_0}(X_t)\\int_{A_1}^{}P_{t_1}(X_t,dx_1)\\ldots \\int_{A_n}^{}P_{t_n-t_{n-1}}(x_{n-1},dx_n) \\] <p>As for the second assertion, once again with a monotone class argument, it is enough to show that</p> \\[   E^\\mu[ Z\\circ \\theta_t Y ]=E^\\mu[ E^{X_t}[ Z ]Y] \\] <p>for \\(Y = 1_{B_0\\times B_1\\times \\ldots \\times B_m}(X_0,X_{s_{1}},\\ldots, X_{s_m})\\) where \\(A_k\\) are Borel sets and \\(s_m= t\\). By definition of \\(P^\\mu\\), the shift operator, and the expression of \\(E^{X_t}[Z]\\) above we get</p> \\[   \\begin{align*}      E^\\mu[ Z\\circ \\theta_t Y ] &amp; = E^\\mu\\left[ 1_{A}(X_{t}, X_{t+t_1}, \\ldots, X_{t+t_n}) 1_B(X_{0}, \\ldots, X_{s_m}) \\right]\\\\                                 &amp; = \\int_{B_0}^{}\\mu(dx_0)\\int_{B_1}^{}P_{s_1}(x_0,dx_1)\\ldots\\\\                                 &amp; \\qquad \\qquad \\qquad\\ldots \\int_{B_m\\cap A_0}^{}P_{t-s_{m-1}}(x_{m-1},dx_m)\\int_{A_1}^{} P_{t+t_1-t}(x_{m},dy_1)\\cdots \\int_{A_n}^{} P_{t+t_{n} -t-t_{n-1}}(y_{n-1},dy_n)\\\\                                 &amp; = \\int_{B_0}^{}\\mu(dx_0)\\int_{B_1}^{}P_{s_1}(x_0,dx_1)\\ldots \\int_{B_m\\cap A_0}^{}P_{t-s_{m-1}}(x_{m-1},dx_m)\\int_{A_1}^{} P_{t_1}(x_{m},dy_1)\\cdots\\\\                                 &amp; \\qquad \\qquad \\qquad \\cdots \\int_{A_n}^{} P_{t_{n}-t_{n-1}}(y_{n-1},dy_n)\\\\                                 &amp; = E^\\mu[ E^{X_t}[ Z ]Y]   \\end{align*} \\]"},{"location":"lecture/05-Markov/052-markov-discrete/","title":"Markov Chains","text":"<p>In this subsection we treat discrete time Markov processes chains with a finite or eventually countable state space \\(S\\), also called Markov Chains. Recall that we only consider time homogenous Markov processes. Defining</p> \\[ \\begin{align*}     \\mu_x:=P[X_0=x] \\quad p_{xy}:=P\\left[X_{t+1}=y|X_t=x\\right]=P[X_1=y|X_0=x] \\end{align*} \\] <p>it follows that \\(0\\leq \\mu_x, p_{xy}\\leq 1\\) and \\(\\sum \\mu_x=1\\), as well as \\(\\sum_y p_{xy}=1\\) for every \\(x\\). In other terms, the initial distribution \\(\\mu:=(\\mu_x)\\) is a random vector and the one step transition probability can be represented by the stochastic matrix \\(P=(p_{xy})\\). For \\(B\\subseteq S\\), it holds</p> \\[ \\begin{equation*}     P_1(x,B)=\\sum_{y \\in B}p_{xy} \\end{equation*} \\] <p>By Chapman Kolmogorov relation, we have \\(P_t=P_{t-1}P_1\\), hence, defining recursively</p> \\[ \\begin{equation}\\label{eq:repetition_markov}     p^t_{xy}=\\sum_{z}p^{t-1}_{xz}p_{zy} \\end{equation} \\] <p>we have</p> \\[ \\begin{equation*}     P_t(x,B)=\\sum_{y \\in B}p^t(x,y) \\end{equation*} \\] <p>In the discrete case the following strong Markov property holds.</p> <p>Strong Markov Property</p> <p>Let \\(Z\\) be a bounded random variable and \\(\\tau\\) a stopping time. Then it holds</p> \\[ \\begin{equation*}     1_{\\{\\tau&lt;\\infty\\}}E^{\\mu}\\left[ Z\\circ\\theta_{\\tau}|\\mathcal{F}_\\tau \\right]=1_{\\{\\tau&lt;\\infty\\}} E^{X_{\\tau}}\\left[ Z \\right]. \\end{equation*} \\] <p>Proof</p> <p>For every \\(A \\in \\mathcal{F}_\\tau\\) we get by Theorem \\ref{thm:markov property} and the fact that \\(A\\cap \\{\\tau=t\\}\\in \\mathcal{F}_t\\) for every \\(t\\),</p> \\[ \\begin{align*}     E^{\\mu}\\left[1_A1_{\\{\\tau&lt;\\infty\\}} Z\\circ\\theta_\\tau\\right]     &amp;=\\sum_{t} E^\\mu \\left[1_A1_{\\{\\tau=t\\}} Z\\circ\\theta_t\\right]\\\\     &amp;= \\sum_{t} E^{\\mu}\\left[1_A1_{\\{\\tau=t\\}} E^{X_t}\\left[Z\\right]\\right]\\\\      &amp;=E^\\mu\\left[1_A1_{\\{\\tau&lt;\\infty\\}} E^{X_\\tau}\\left[Z\\right]\\right], \\end{align*} \\] <p>from which the claim follows.</p> <p>Markov chains are defined locally: The next step is entirely defined by the place where you are now and the transition probability. The natural question therefore is whether you will come back or not. Given a state \\(y\\) in \\(S\\), define recursively </p> \\[ \\begin{align*}     \\tau^0_y  &amp; :=0\\\\     \\tau^1_y &amp; := \\inf\\left\\{ t&gt; \\tau^0_y\\colon X_t=y \\right\\}\\\\     \\vdots\\\\     \\tau^k_y &amp; := \\inf\\left\\{ t&gt;\\tau^{k-1}_y\\colon X_t=y \\right\\}\\\\     \\vdots \\end{align*} \\] <p>Here \\(\\tau^k_y\\) denotes the \\(k\\)-th return time to the state \\(y\\). Let further </p> \\[ \\begin{equation*}     \\rho_{xy}:=P^{x}\\left[ \\tau_y&lt;\\infty \\right] \\end{equation*} \\] <p>be the probability that starting from \\(x\\), the Markov chain visits the state \\(y\\) at least once,  where we denote the first return time as \\(\\tau_y = \\tau^1_y\\).</p> <p>Definition</p> <p>We say that a time homogeneous Markov chain is </p> <ul> <li>recurrent: if \\(\\rho_{xx}=1\\);</li> <li>transient: if \\(\\rho_{xx}&lt;1\\).</li> </ul> <p>Theorem</p> <p>For any two states \\(x\\) and \\(y\\) in \\(S\\), it holds</p> \\[ \\begin{equation*}     P^x[\\tau_y^k &lt; \\infty] = \\rho_{xy}\\rho_{yy}^{k-1}. \\end{equation*} \\] <p>Proof</p> <p>We show it per induction. Per definition, it holds \\(P^x[\\tau_y^1 &lt; \\infty]= \\rho_{xy}\\). Suppose therefore that the claim holds for every \\(l=1,\\ldots,k-1\\) and we show it for \\(k\\).</p> <p>Define \\(\\tau=\\tau_y^{k-1}\\) and \\(Z:=1_{\\{\\tau_y &lt; \\infty \\}}\\). It follows that \\(\\{ \\tau_y^k &lt; \\infty \\} = \\{ \\tau_y \\circ \\theta_\\tau &lt; \\infty \\}\\). Furthermore, from \\(1_{\\{\\tau&lt;\\infty\\}}\\) bounded and \\(\\mathcal{F}_{\\tau}\\)-measurable and \\(X_{\\tau}=y\\) on \\(\\{\\tau&lt;\\infty\\}\\), using the strong Markov property it follows that</p> \\[ \\begin{align*}     P^{x}[\\tau_y^k &lt; \\infty ] &amp;= P^{x}[\\tau_y \\circ \\theta_\\tau &lt; \\infty ]\\\\                               &amp;= E^x\\left[1_{\\{\\tau &lt; \\infty\\}}  (Z \\circ \\theta_\\tau) \\right]\\\\                               &amp;= E^x\\left[1_{\\{\\tau &lt; \\infty\\}}  E^x\\left[Z \\circ \\theta_\\tau| \\mathcal{F}_\\tau\\right]\\right]\\\\                               &amp;= E^x\\left[1_{\\{\\tau &lt; \\infty\\}}  E^{X_\\tau}\\left[Z\\right] \\right]\\\\                               &amp;= E^x\\left[1_{\\{\\tau &lt; \\infty\\}}  P^y[\\tau_y&lt;\\infty]\\right]\\\\                               &amp;= \\rho_{yy} P^x[\\tau_y^{k-1} &lt; \\infty]\\\\                               &amp;= \\rho_{yy} \\rho_{xy} \\rho_{yy}^{k-2}\\\\                               &amp;= \\rho_{xy} \\rho_{yy}^{k-1}. \\end{align*} \\] <p>Theorem</p> <p>Let \\(x\\) and \\(y\\) be two states in \\(S\\), where \\(x\\) is recurrent and \\(\\rho_{xy}&gt;0\\). Then \\(y\\) is recurrent and \\(\\rho_{yx}=1\\).</p> <p>Proof</p> <p>Let us first show that \\(\\rho_{yx}=1\\). Since \\(x\\) is recurrent, it follows that \\(\\tau_x(\\omega)&lt;\\infty\\) for almost all \\(\\omega \\in \\Omega\\). Hence, for almost all \\(\\omega \\in \\Omega\\) such that \\(\\tau_y(\\omega)&lt;\\infty\\) we get \\(\\tau_x \\circ \\theta_{\\tau_y}(\\omega) &lt;\\infty\\).</p> <p>Thus with \\(H:=1_{\\{\\tau_x=\\infty\\}}\\), the strong markov property, and the fact that \\(X_{\\tau_y}=y\\), we get</p> \\[ \\begin{align*}     0 &amp; =P^x[\\tau_y &lt; \\infty, \\tau_x \\circ \\theta_{\\tau_y} = \\infty]\\\\       &amp; =E^x\\left[1_{\\{\\tau_y&lt;\\infty\\}} 1_{\\{\\tau_x \\circ \\theta_{\\tau_y} = \\infty\\}} \\right]\\\\       &amp; =E^x\\left[1_{\\{\\tau_y&lt;\\infty\\}} E^{X_{\\tau_y}}\\left[H\\right] \\right]\\\\       &amp; =E^x\\left[1_{\\{\\tau_y&lt;\\infty\\}} P^y[\\tau_x=\\infty]\\right] \\\\       &amp; =\\rho_{xy}(1-\\rho_{yx}). \\end{align*} \\] <p>Since \\(\\rho_{xy} &gt; 0\\), it must be that \\(\\rho_{yx}=1\\).</p> <p>Let us finally show that \\(y\\) is recurrent. Let \\(i\\) and \\(j\\) be two states in \\(S\\) and \\(k\\) in\\(\\mathbb{N}\\). Then by Chapman-Kolmogorov relation and an induction we get \\(P_i[X_k=j]=p^k_{ij}\\). Since \\(\\rho_{xy}&gt;0\\) and \\(\\rho_{yx}&gt;0\\), there exist \\(k_1\\) and \\(k_2\\) in \\(\\mathbb{N}\\) with \\(p_{xy}^{k_1}&gt;0\\) and \\(p_{yx}^{k_2}&gt;0\\). By the Chapman-Kolmogorov relation we get</p> \\[ p_{yy}^{k_1+t+k_2} \\geq p_{yx}^{k_2} p_{xx}^t p_{xy}^{k_1} \\] <p>so that</p> \\[ E^y\\left[N_y\\right] =p_{yx}^{k_2} E^x\\left[N_x\\right]p_{xy}^{k_1} =\\infty. \\] <p>Recurrence/Transience Theorem implies that \\(y\\) is recurrent.</p> <p>Definition</p> <p>A set \\(C \\subseteq S\\) is called</p> <ul> <li>closed, if for \\(x\\in C\\), \\(y\\in S\\) and \\(\\rho_{xy}&gt;0\\) it holds \\(y \\in C\\),</li> <li>irreducible, if \\(\\rho_{xy}&gt;0\\) for all \\(x,y\\in C\\).</li> </ul> <p>Theorem</p> <p>Suppose \\(C \\subset S\\) is finite and closed.</p> <ul> <li>There exists \\(x\\in C\\) such that \\(x\\) is recurrent.</li> <li>If \\(C\\) is irreducible, then all its states are recurrent.</li> </ul> <p>Proof</p> <ul> <li> <p>By contradiction, assume that all states are transient, that is \\(E^x[N_y] &lt; \\infty\\) for all \\(x\\) and \\(y\\) in \\(C\\) by the recurrent/transient Theorem.     But then, since \\(C\\) is finite we get with Tonelli's theorem</p> \\[ \\sum_{y\\in C} E^x[N_y] =\\sum_{y\\in C} \\sum_{t} P^x [X_t=y] =\\sum_{t}\\sum_{y\\in C} P^x[X_t=y] =\\sum_{t} P^x [ X_t\\in C] =\\sum_{t} 1=\\infty. \\] <p>Indeed, if \\(P^x [X_t\\in C] &lt; 1\\), then there is \\(y\\) in \\(C^c\\) with \\(P^x[X_t=y] &gt;0\\), that is \\(\\rho_{xy} &gt;0\\). Since \\(C\\) is closed, it follows \\(y \\in C\\), a contradiction.</p> </li> <li> <p>If \\(x \\in C\\) is recurrent and \\(\\rho_{xy}&gt;0\\), then \\(y\\) is recurrent by Theorem the previous theorem.</p> </li> </ul> <p>Theorem</p> <p>Let \\(R:=\\{ x \\in S \\colon \\rho_{xx} =1  \\}\\) be the set of recurrent states. Then \\(R\\) is a disjoint union of closed and irreducible sets.</p> <p>Proof</p> <p>Fix \\(x\\) in$ R$ and let $C_x :={ y\\in R \\colon \\rho_{xy} &gt;0 } $.</p> <ul> <li> <p>For \\(x\\), \\(y\\) and \\(z\\) in \\(R\\) it holds \\(\\rho_{xy} \\geq \\rho_{xz} \\rho_{zy}\\).     Indeed, with \\(H:=1_{\\{\\tau_y &lt; \\infty\\}}\\) and the strong Markov property using the same argument as in recurrence Theorem we obtain</p> \\[ \\begin{align*}    \\rho_{xy}  &amp; =P^x[\\tau_y&lt;\\infty]\\\\               &amp; \\geq P^x[\\tau_z&lt;\\infty, \\tau_y \\circ \\theta_{\\tau_z} &lt; \\infty]\\\\               &amp; =E^x[1_{\\{\\tau_z &lt; \\infty\\}} E^x[H \\circ \\theta_{\\tau_z} | \\mathcal{F}_{\\tau_z}]]\\\\               &amp; =E^x[1_{\\{\\tau_z &lt; \\infty\\}}] E^z[1_{\\{\\tau_y &lt; \\infty\\}}]]\\\\               &amp; =\\rho_{xz}\\rho_{zy}. \\end{align*} \\] </li> <li> <p>\\(C_x\\) is irreducible and closed.     Indeed, for \\(y\\) in \\(C_x\\) and \\(\\rho_{yz} &gt;0\\), since \\(y\\) is in \\(C_x\\) we have \\(\\rho_{xy}&gt;0\\) and therefore by the previous step \\(\\rho_{xz} &gt;0\\), that is \\(z\\) belongs to \\(C_x\\).     Furthermore, for \\(y\\) and \\(z\\) in \\(C_x\\), it implies that \\(\\rho_{xy} &gt;0\\), \\(\\rho_{xz}&gt;0\\).     Recurrence Theorem yields \\(\\rho_{yx}&gt;0\\) and therefore \\(\\rho_{yz}&gt;0\\).</p> </li> <li> <p>We now show that \\(C_x \\cap C_y \\neq \\emptyset\\) implies \\(C_x=C_y\\).     Let \\(z\\) be in \\(C_x \\cap C_y\\) and \\(y^\\prime\\) in \\(C_y\\).     Therefore \\(\\rho_{xz}&gt;0\\), \\(\\rho_{yz}&gt;0\\), \\(\\rho_{yy^\\prime}&gt;0\\) and by recurrence Theorem \\(\\rho_{zy}&gt;0\\).     This means \\(\\rho_{xy^\\prime} &gt;0\\) which yields \\(y^\\prime\\in C_x\\), hence \\(C_y \\subset C_x\\).     Analogously, \\(C_x \\subset C_y\\).</p> </li> </ul> <p>Definition</p> <p>A measure \\(\\mu\\) on is called stationary\u2014or invariant\u2014if</p> <ul> <li>\\(\\mu(y)&lt; \\infty\\) for all \\(y \\in S\\),</li> <li>\\(\\mu(y)=\\sum_{x\\in S} \\mu(x)p_{xy}\\).</li> </ul> <p>If \\(\\mu\\) is a probability measure, then \\(\\mu\\) is called a stationary distribution.</p> <p>Remark</p> <p>Suppose that \\(\\mu\\) is a stationary distribution, and let \\(P^\\mu\\) the measure such that \\(X\\) is Markov with start distribution \\(\\mu\\). Then</p> \\[   \\begin{align*}      P^\\mu[X_1=y] &amp; = \\sum_{x\\in S}P^\\mu[X_1=y | X_0=x] P^\\mu[X_0=x]\\\\                   &amp; =\\sum_{x\\in S} \\mu(x) p_{xy}\\\\                   &amp; =\\mu(y)   \\end{align*} \\] <p>By induction, we assume that \\(P^\\mu[X_t=y]=\\mu(y)\\). Then</p> \\[   \\begin{align*}      P^\\mu[X_{t+1}=y] &amp; =\\sum_{z\\in S} P^\\mu[X_{t+1}=y|X_t=z]P^\\mu[X_t=z]\\\\                       &amp; =\\sum_{z\\in S} \\mu(z) p_{zy}\\\\                       &amp; =\\mu(y)   \\end{align*} \\] <p>This shows \\(P^\\mu(X_t=y) = \\mu(y)\\) for every \\(t\\).</p> <p>Example: Randomw Walk</p> <p>Let \\(X_t:=X_0+\\sum_{s=1}^{t} Y_s\\) and \\(\\mathcal{F}_t:=\\sigma(X_0,\\dots,X_t)\\). Then</p> \\[   \\begin{align*}      p_{xy} &amp; = P[X_{t+1}=y | X_t=x]=P[\\xi_{t+1}=y-X_t | X_t=x]\\\\             &amp; =P[\\xi_1=y-x]\\\\             &amp; =f(y-x)   \\end{align*} \\] <p>where \\(f \\colon S \\to [0,1]\\) is such that \\(\\sum_{y\\in S}f(y)=1\\), that is \\(f(y):=P[\\xi_1=y]\\). In this case \\(\\mu \\equiv 1\\) is a stationary measure. Indeed,</p> \\[   \\begin{align*}      \\sum_{x\\in S} \\mu(x)p_{xy}   &amp; = \\sum_{x\\in S} p_{xy}\\\\                                   &amp; = \\sum_{x\\in S} f(y-x) \\\\                                   &amp; =\\sum_{x^\\prime\\in S} f(x^\\prime) \\\\                                   &amp; =1\\\\                                   &amp; =\\mu(y)   \\end{align*} \\] <p>Remark</p> <p>Let \\(x\\in S\\) be transient and \\(\\pi\\) a stationary distribution. Then \\(\\pi(x)=0\\).</p> <p>Proof</p> <p>Exercise.</p> <p>Theorem</p> <p>Suppose \\(x\\) is recurrent and \\(\\tau:=\\inf\\{ t\\colon X_t = x \\}\\). Define</p> \\[ \\mu(y)=E^x\\left[\\sum_{t=0}^{\\tau-1}1_{\\{X_t=y\\}}\\right]=\\sum_{t} P^x[X_t=y, t&lt;\\tau] \\] <p>for all \\(y\\in S\\). Then \\(\\mu\\) is a stationary measure.</p> <p>Proof</p> <p>For \\(z\\) in \\(S\\) and \\(t\\) let \\(\\bar{p}_t(x,z):=P^x[X_t=z,\\tau&gt;t]\\). We claim that \\(\\sum_{t} \\bar{p}_t(x,\\cdot)\\) is a stationary measure.</p> <ul> <li> <p>For \\(z\\neq x\\) we have</p> \\[   \\begin{align*}      \\sum_{y\\in S} \\bar{p}_t(x,y) p_{yz}        &amp; = \\sum_{y\\in S} P^x[X_t=y, \\tau&gt;t] p_{yz} \\\\       &amp; = \\sum_{y\\in S} P^x(X_t=y,\\tau&gt;t,X_{t+1}=z]\\\\       &amp; = P^x[\\tau&gt;t+1,X_{t+1}=z]\\\\       &amp; =\\bar{p}_{t+1}(x,z)   \\end{align*} \\] <p>Hence,</p> \\[   \\begin{align*}      \\sum_{y\\in S}\\sum_{t} \\bar{p}_t(x,y)p_{yz}          &amp; = \\sum_{t}\\sum_{y\\in S} \\bar{p}_t(x,y)p_{yz}\\\\         &amp; = \\sum_{t} \\bar{p}_{t+1}(x,z)\\\\         &amp; = \\mu(z)   \\end{align*} \\] <p>since \\(\\bar{p}_0(x,z)=0\\).</p> </li> <li> <p>For \\(z=x\\) we have</p> \\[ \\sum_{y\\in S} \\bar{p}_t(x,y) p_{yx} = \\sum_{y\\in S} P^x[X_t=y,\\tau&gt;t,X_{t+1}=x] = P^x[\\tau=t+1]. \\] <p>Hence,</p> \\[ \\sum_{t}\\sum_{y\\in S} \\bar{p}_t(x,y)p_{yz} = \\sum_{t} P^x[\\tau=t+1] = 1 = \\mu(x). \\] </li> <li> <p>Finally we show that \\(\\mu(y)&lt;\\infty\\) for any \\(y\\) in \\(S\\).     We have \\(\\mu(x)=1\\), and \\(\\mu p^t=\\mu\\), showing that \\(\\mu(y)&lt;\\infty\\) if \\(p^t_{xy}&gt;0\\) for some \\(t\\).</p> <ul> <li>If \\(P^x[\\tau_y&lt;\\infty]=0\\), then \\(\\mu(y)=0\\).</li> <li>If \\(P^x[\\tau_y&lt;\\infty]&gt;0\\), then by recurrence Theorem we have \\(P^y[\\tau_x&lt;\\infty]&gt;0\\) so that \\(p_{yx}^t&gt;0\\) for some \\(t\\), hence \\(\\mu(y)&lt;\\infty\\).</li> </ul> </li> </ul>"},{"location":"lecture/05-Markov/053-brownian/","title":"Brownian Motion","text":"Note: Browninan Motion and Wiener Measure <p>The Brownian motion, named after the botanist Robert Brown, was first observed in 1827 when Brown noticed the erratic motion of pollen particles suspended in water. While Brown initially attributed this movement to some \"life force,\" it was later understood as a physical phenomenon resulting from molecular collisions.</p> <p>At the end of the 19th and the beginning of the 20th century, physicists such as Albert Einstein and Marian Smoluchowski provided theoretical models that linked Brownian motion to the kinetic theory of gases, demonstrating the random behavior of particles in a fluid. This work provided critical evidence for the atomic theory of matter.</p> <p>In spite of its success and applications, it tooks a century until a rigorous mathematical formulation and proof of the existence of Brownian motion as a stochastic process was achieved by Norbert Wiener in the 1930 ies. This layed the foundation for modern probability theory and the study of continuous-time stochastic processes. The resulting process, is now often called the Wiener process.</p> <p>The Brownian motion is one of the central objects of continuous time stochastic analysis. Though being classical and intuitive, it is not an easy object with fascinating mathematical properties.</p> <p>Definition: Brownian Motion</p> <p>Let \\((\\Omega,\\mathcal{F},P)\\) be a probability space. A stochastic process \\(B\\) is called a Brownian Motion if</p> <ul> <li>Independence of Increments: The increments \\(B_{t_n}-B_{t_{n-1}}\\), \\(B_{t_{n-1}}-B_{t_{n-2}}\\), \\(\\ldots\\), \\(B_{t_1}-B_{t_0}\\) are independent for every finite family \\(0\\leq t_0&lt;t_1&lt;\\ldots&lt;t_n\\).</li> <li>Normal Behavior of Increments: Increments are normally distributed \\(B_{t}-B_s\\sim \\mathcal{N}(0,t-s)\\).</li> <li> <p>Continuity of paths: (almost surely)</p> \\[ P\\left[ \\left\\{ \\omega \\colon t\\mapsto B_t(\\omega) \\text{ is continuous} \\right\\} \\right]=1 \\] </li> </ul> <p>Note that we do not require any filtration in the definition of the Brownian motion. As we will see later, we will often consider the filtration \\(\\mathcal{F}_t=\\sigma(B_s\\colon s\\leq t)\\) up to completion, and this filtration has particular properties.</p> <p>It is intuitive to see the Brownian motion as a scaling limit of a symmetric random wall\u2014and it actually is. However, one of the first questions to ask is whether such an object exists. Precisely, one asks if, for an adequate measurable space \\((\\Omega,\\mathcal{F})\\) and stochastic process \\(B\\), there exists a probability measure \\(P\\colon\\mathcal{F}\\to [0,1]\\) such that under \\(P\\), the stochastic process \\(B\\) is a Brownian motion. This measure is called the Wiener measure.</p> <p>There are several ways to construct such a measure, including a wavelet construction, a Hilbert space approach, or an appropriate scaling of the symmetric random walk together with some weak convergence. Hereafter, we will present a pure stochastic construction based on the Kolmogorov-Carath\u00e9odory extension theorem. It may be a little technical, but it has the advantage of presenting several important aspects of measure theory without relying on any advanced functional analysis assumptions.</p> <p>The construction will follow two important steps:</p> <ul> <li>Construction of a measure \\(P\\) on the path space such hat the canonical process \\(X\\) satisfies the two first properties. \\(\\leadsto\\) Kolmogorov extension Theorem.</li> <li>Modification of the Canonical process \\(X\\) to \\(B\\) such that \\(B\\) additionally satisfies the continuity property. \\(\\leadsto\\) Kolmogorov-Centov Theorem.</li> </ul>"},{"location":"lecture/05-Markov/053-brownian/#pre-brownian-motion-kolmogorov-caratheodory-extension-theorem","title":"Pre-Brownian Motion: Kolmogorov-Carath\u00e9odory extension Theorem","text":"<p>For a given index set \\(\\mathbf{T}\\), we consider \\((S,\\mathcal{S})\\) where \\(S\\) is a Polish space<sup>1</sup> with Borel \\(\\sigma\\)-algebra \\(\\mathcal{S}\\). For ease, you may assume that \\(S \\subseteq \\mathbb{R}^d\\) is a closed or open subset. The product of countable Polish spaces is Polish, and recall the following theorem from Ulam.</p> <p>Ulam</p> <p>Let \\(P\\) be a probability measure on \\((S,\\mathcal{S})\\), then \\(P\\) is regular, that is</p> \\[   P\\left[ A \\right]=\\sup\\left\\{ P[K]\\colon K\\subseteq A, K\\text{ compact} \\right\\} \\] <p>for every Borel set \\(A\\subseteq S\\).</p> Proof <p>Let \\(A \\subseteq S\\) be a Borel set. By inner regularity of the probability measure \\(P\\), we have</p> \\[ P(A) = \\sup\\{P(K) \\colon K \\subseteq A, K \\text{ compact} \\}. \\] <p>To prove this, define</p> \\[ \\mu(A) = \\sup\\{P(K) \\colon K \\subseteq A, K \\text{ compact} \\}. \\] <p>Clearly, \\(\\mu\\) is an outer measure with \\(\\mu(S) = 1\\) and \\(\\mu \\leq P\\).</p> <p>Now, consider an open set \\(G \\supseteq A\\). Since \\(S\\) is Polish, there exists an increasing sequence of compact sets \\((K_n)\\) such that \\(G = \\bigcup_{n=1}^\\infty K_n\\). By the continuity of \\(P\\),</p> \\[ \\mu(G) = \\lim_{n \\to \\infty} P(K_n) \\leq P(G). \\] <p>Taking the infimum over all such open sets \\(G\\) containing \\(A\\) gives</p> \\[ \\mu(A) \\geq P(A). \\] <p>Combining both inequalities, we conclude \\(\\mu(A) = P(A)\\), proving the result.</p> <p>Recall standard notations for the path space:</p> <ul> <li>Sample space given by \\(\\Omega :=\\{\\omega=(\\omega_t) :\\omega_t \\in  S \\text{ for all } t\\}=S^{\\mathbf{T}}\\).</li> <li>Coordinate process \\(X\\) given by \\(X_t(\\omega)=\\omega_t\\) for every \\(t\\) and \\(\\omega\\).</li> <li> <p>Projection For \\(F\\) and \\(G\\) subsets of \\(\\mathbf{T}\\) with \\(F\\subseteq G\\), define the projections:</p> \\[ \\begin{equation*}   \\begin{split}     \\pi_F \\colon \\Omega &amp; \\longrightarrow S^{F}\\\\                   \\omega &amp; \\longmapsto  \\pi_F(\\omega) = (\\omega_t)_{t\\in F}   \\end{split} \\end{equation*} \\] <p>and</p> \\[ \\begin{equation*}   \\begin{split}     \\pi_{GF} \\colon S^{G} &amp; \\longrightarrow S^{F}\\\\                   (\\omega_t)_{t \\in G} &amp; \\longmapsto  \\pi_{GF}\\left((\\omega_t)_{t \\in G}\\right) = (\\omega_t)_{t\\in F}   \\end{split} \\end{equation*} \\] </li> <li> <p>Algebra \\(\\mathcal{C}\\) of those sets \\(C=\\pi^{-1}_F(A)=\\{ \\omega \\in \\Omega\\colon (\\omega_{t_1},\\ldots,\\omega_{t_n})\\in A \\}=\\{(X_{t_1},\\ldots X_{t_n})\\in A\\}\\) where \\(A\\) is in the Borel \\(\\sigma\\)-algebra \\(\\mathcal{F}^F:=\\mathcal{B}(S^F)\\) and \\(F\\subseteq\\mathbf{T}\\) is finite.</p> </li> <li>Product \\(\\sigma\\)-Algebra \\(\\mathcal{F}=\\sigma(\\mathcal{C}) =\\otimes_{t \\in \\mathbf{T}}\\mathcal{S}\\)</li> </ul> <p>Consistent family</p> <p>A family \\((P_{F})\\) of probability measures \\(P_F:\\mathcal{F}^F\\to [0,1]\\) for every finite subset \\(F\\) of \\(\\mathbf{T}\\) is called consistent if</p> \\[   P_{F}(A)=P_{G}\\left( B \\right) \\] <p>for every \\(F\\subseteq G\\) and \\(A\\in \\mathcal{F}^F\\) where \\(B=\\pi_{GF}^{-1}(A)\\).</p> <p>We can now formulate the Kolmogorov extension theorem, a consequence of Carath\u00e9odory's Theorem.</p> <p>Kolmogorov Extension Theorem</p> <p>Let \\((P_F)\\) be a consistent family of Borel probability measures. Then, there exists a unique probability measure \\(P\\) on \\(\\mathcal{F}\\) such that</p> \\[ \\begin{equation*}     P(C)=P_F(A)     \\end{equation*} \\] <p>for every \\(A \\in \\mathcal{F}^F\\) where \\(C=\\pi^{-1}_F(A)\\).</p> <p>In other words, it is possible to construct a unique probability measure whose finite-dimensional restriction coincides exactly with each given finite-dimensional distribution \\(P_F\\), provided they are consistent with each other.</p> <p>Proof</p> <p>Note that for every finite family \\((C_k)_{k\\leq n}=(\\pi^{-1}_{F_k}(A_k))_{1\\leq k\\leq n}\\), if we define \\(F=\\cup_{k\\leq n}F_k\\) and \\(B_k=\\pi^{-1}_{FF_k}(A_k) \\in \\mathcal{F}^F\\), it follows that \\(C_k=\\pi^{-1}_{F}(B_k)\\). In particular, up to enlargement, we can always assume that we take a common finite family to represent the \\((C_k)\\). Furthermore, if the \\((C_k)\\) are disjoint, then so are the \\((B_k)\\).</p> <p>Step 1: Definition of \\(P\\) as a set function Define \\(P:\\mathcal{C}\\to [0,1]\\) by setting \\(P[C]=P_F(A)\\) where \\(C=\\pi^{-1}_F(A)\\) for \\(A\\in \\mathcal{F}^F\\). This function is well-defined due to consistency. Suppose \\(C=\\pi^{-1}_F(A)=\\pi^{-1}_{G}(B)\\) for \\(A\\) in \\(\\mathcal{F}^F\\) and \\(B\\) in \\(\\mathcal{F}^G\\). Then \\(D := \\pi^{-1}_{F\\cup G F}(A) = \\pi^{-1}_{F\\cup G G}(B)\\) and \\(\\pi^{-1}_{F\\cup G}(D)=C\\). By the consistency of the family, it follows that \\(P_F(A)=P_{F\\cup G}(D)=P_{G}(B)\\), confirming that \\(P\\) is well-defined.</p> <p>Step 2: \\(P\\) as an additive measure Clearly, \\(P[\\emptyset]=0\\) and \\(P[\\Omega]=1\\). As mentioned above, let \\((C_k)_{k\\leq n}=(\\pi^{-1}_{F}(A_k))_{k\\leq n}\\) be a finite disjoint family of elements in the algebra \\(\\mathcal{C}\\) with common finite set \\(F\\). Since \\(P_F\\) is a probability measure, it follows that</p> \\[ P\\left[\\cup_{k\\leq n} C_k\\right] = P_F\\left[\\cup_{k\\leq n} A_k\\right] = \\sum_{k\\leq n} P_F[A_k] = \\sum_{k\\leq n} P[A_k]. \\] <p>Therefore, \\(P\\) is an additive probability measure on the algebra \\(\\mathcal{C}\\).</p> <p>Step 3: \\(P\\) is \\(\\sigma\\)-additive Using continuity at \\(\\emptyset\\), let \\((C_n) = (\\pi^{-1}_{F_n}(A_n))\\) be a decreasing sequence of elements in \\(\\mathcal{C}\\) such that \\(\\cap C_n = \\emptyset\\) and suppose that \\(P[C_n] \\geq \\varepsilon &gt; 0\\) for some \\(\\varepsilon &gt; 0\\). Up to redefining \\(\\tilde{F}_n = \\cup_{k\\leq n}F_k\\) and \\(\\tilde{A}_n = \\pi^{-1}_{\\cup_{k\\leq n}F_k F_n}(A_n)\\), where \\(C_n = \\pi_{\\tilde{F}_n}(\\tilde{A}_n)\\) with \\(\\tilde{A}_n\\) in \\(\\mathcal{F}^{\\tilde{F}_n}\\), we may assume that \\((F_n)\\) is an increasing sequence of finite subsets of \\(\\mathbf{T}\\). Since \\((F_n)\\) is increasing and \\((C_n)\\) decreasing, it follows that \\(A_{n+1}\\subseteq \\pi_{F_{n+1}F_n}^{-1}(A_n)\\). By inner regularity of Borel probability measures on \\(\\mathbb{R}\\), we can find a sequence of compacts \\((\\tilde{K}_n)\\) such that \\(\\tilde{K}_n\\subseteq A_n\\) and \\(P_{F_n}[A_n\\setminus \\tilde{K}_n]\\leq \\varepsilon/2^{n+1}\\). However, we do not know whether this sequence of compacts fulfills the same monotonicity condition as \\((A_n)\\). We therefore shrink the compacts to </p> \\[ K_n=\\cap_{k\\leq n}\\pi^{-1}_{F_nF_k}(\\tilde{K}_k)\\subseteq \\tilde{K}_n \\] <p>ensuring \\(K_{n+1} \\subseteq \\pi^{-1}_{F_{n+1}F_n}(K_n)\\) for every \\(n\\). By construction,</p> \\[   \\begin{align*}      P_{F_n}\\left[ K_n \\right]        &amp; = P_{F_n}[A_n]-P_{F_n}[A_n\\setminus K_n]\\\\       &amp; \\geq \\varepsilon -P_{F_n}[A_n\\setminus \\cap_{k\\leq n}\\pi^{-1}_{F_nF_k}(\\tilde{K}_k)]\\\\       &amp; =\\varepsilon -P_{F_n}[\\cup_{k\\leq n} A_n\\setminus \\pi^{-1}_{F_nF_k}(\\tilde{K}_k)]\\\\       &amp; \\geq \\varepsilon -P_{F_n}[\\cup_{k\\leq n} \\pi^{-1}_{F_nF_k}(A_k\\setminus \\tilde{K}_k)]\\\\       &amp; \\geq \\varepsilon -\\sum_{k\\leq n}P_{F_n}[\\pi^{-1}_{F_nF_k}(A_k\\setminus \\tilde{K}_k)]\\\\       &amp; =\\varepsilon -\\sum_{k\\leq n}P_{F_k}[A_k\\setminus \\tilde{K}_k]\\\\       &amp; \\geq \\varepsilon -\\sum_{k=1}^{n}\\frac{\\varepsilon}{2^{k+1}}\\\\       &amp; \\geq \\frac{\\varepsilon}{2}   \\end{align*} \\] <p>In particular, each compact is non empty, so let \\(\\omega_n\\) be in \\(\\pi^{-1}_{F_n}(K_n)\\), that is \\(\\pi_{F_n}(\\omega_n)\\) is in \\(K_n\\). It holds from \\(K_n \\subseteq \\pi_{F_1}^{-1}(K_1)\\) that \\(\\pi_{F_1}(\\omega_n)\\in K_1\\) for every \\(n\\). By compactness of \\(K_1\\), we have for a subsequence, again denoted by \\((\\omega_n)\\), that \\(\\pi_{F_1}(\\omega_{n})\\to \\pi_{F_1}(\\bar{\\omega}_1)\\) some \\(\\bar{\\omega}_1\\) in \\(\\Omega\\). Further, from \\(K_n\\subseteq \\pi_{F_2}^{-1}(K_2)\\) we have for the same reason as before for a sub-subsequence also denoted by \\((\\omega_n)\\), that \\(\\pi_{F_2}(\\omega_n)\\to \\pi_{F_2}(\\bar{\\omega}_2)\\). Since \\(\\pi_{F_1}(\\bar{\\omega}_1)=\\pi_{F_1}(\\bar{\\omega}_2)\\), we deduce that the coordinates of \\(\\bar{\\omega}_2\\) and \\(\\bar{\\omega}_1\\) coincide on \\(F_1\\). Doing so forth, we can construct a point \\(\\bar{\\omega}\\) by setting arbitrarily points on \\((\\cup F_n)^c\\) such that \\(\\pi_{F_n}(\\bar{\\omega})=\\pi_{F_n}(\\bar{\\omega}_n)\\) for every \\(n\\). It follows in particular that \\(\\bar{\\omega}\\in \\pi^{-1}_{F_n}(K_n)\\subseteq C_n\\) for every \\(n\\) showing that \\(\\cap C_n\\neq \\emptyset\\), contradicting our initial assumption that \\(\\cap C_n = \\emptyset\\).</p> <p>Step 4: Carath\u00e9odory's Extension Theorem Since \\(P\\) is a \\(\\sigma\\)-additive probability measure on the algebra \\(\\mathcal{C}\\), by Carath\u00e9odory's Theorem, it can be uniquely extended to \\(\\mathcal{F}=\\sigma(\\mathcal{C})\\), hence the theorem.</p> <p>Example: Existence of iid Sequences and Henceforth a Random Walk</p> <p>We can apply Kolmogorov's extension theorem to show the existence of iid desquences and henceforth a Random Walk. Let \\(S = \\{-1, 1\\}\\), which is a closed subset of \\(\\mathbb{R}\\) with the Borel \\(\\sigma\\)-algebra \\(\\mathcal{S}\\), which reduces to \\(2^S\\). Let \\(\\mathbb{T} = \\mathbb{N}_0\\) and \\(0&lt;p&lt;1\\). For every finite subset \\(F = \\{t_0 &lt; t_1 \\ldots &lt; t_n\\} \\subset \\mathbb{N}_0\\), define</p> \\[   P_F\\left[ (\\omega_{t_1},\\ldots, \\omega_{t_n})=(e_1,\\ldots, e_n) \\right] = p^l(1-p)^{n-l}, \\] <p>where \\(e_i \\in \\{-1, 1\\}\\) for every \\(i\\), and \\(l = \\#\\{ i \\colon e_i = 1, i = 1, \\ldots, n \\}\\), which defines a probability measure on \\(\\mathcal{F}^F = \\otimes_F \\mathcal{S}\\).</p> <p>It is straightforward to check that the family \\((P_F)\\) is a consistent family of probability measures. By Kolmogorov's extension theorem, there exists a unique probability measure on \\(S^{\\mathbf{T}} = \\{-1, 1\\}^{\\mathbb{N}_0}\\) with the product \\(\\sigma\\)-algebra such that</p> \\[   P[B] = P_F[A]  \\] <p>where \\(B = \\pi^{-1}_F(A)\\) for any \\(F \\subseteq \\mathbb{N}_0\\) finite and \\(A\\) in \\(\\mathcal{F}^F\\). In particular,</p> \\[   P[X_t = 1] = P_{\\{t\\}}[\\omega_t = 1] = p \\quad \\text{and} \\quad P\\left[ X_t = -1 \\right] = 1 - p. \\] <p>A straightforward computation shows that the canonical process \\(X\\) is a sequence of i.i.d. random variables. As seen before, it is also a Markov process.</p> <p>Doing so, the random walk \\(S_0 =0\\) and \\(S_t = \\sum_{s=1}^t X_s\\) is well defined.</p> <p>With this extension theorem at hand, we can therefore construct a pre-Brownian motion.</p> <p>Proposition: Existence of a Pre-Brownian Motion</p> <p>Taking \\(S = \\mathbb{R}\\) and \\(\\mathbf{T} = [0, \\infty)\\), with the notations of the sample space here above, there exists a probability measure on \\(\\mathcal{F}\\) such that \\(X\\) is, up to continuity of paths, a Brownian motion.</p> <p>Proof</p> <p>For the sample space \\(\\Omega =\\mathbb{R}^{[0,\\infty)}\\) with the \\(\\sigma\\)-algebra \\(\\mathcal{F}=\\sigma(\\mathcal{C})\\) where \\(\\mathcal{C}\\) is the collection of those \\(C=\\pi_{F}^{-1}(A)\\) for \\(A\\) in \\(\\mathcal{B}(\\mathbb{R}^F)\\), \\(F=\\{t_0, t_1,\\ldots,t_n\\}\\) for \\(0&lt; t_0&lt;t_1&lt; \\ldots&lt;t_n\\), we define the conditional probability distribution</p> \\[   p(t,x,y)=\\frac{1}{\\sqrt{t2\\pi}}\\exp\\left( -\\frac{(x-y)^2}{2t} \\right) \\] <p>for \\(t&gt;0\\) and \\(x\\), \\(y\\) in \\(\\mathbb{R}\\). For \\(x=(x_1,\\ldots,x_n)\\) we set the cumulative distribution </p> \\[ P_F\\left[ X_{t_0}\\leq x_0, \\ldots, X_{t_n}\\leq x_n \\right] =\\int_{-\\infty}^{x_0}\\int_{-\\infty}^{x_1}\\ldots \\int_{-\\infty}^{x_n}p(t_0,0,y_0)p(t_1-t_0,y_0,y_1)\\ldots p(t_n-t_{n-1},y_{n-1},y_n)dy_0\\ldots dy_{n}    \\] <p>For \\(F\\subseteq G\\subseteq \\mathbf{T}\\) both finite set and \\(A \\in \\mathcal{B}(\\mathbb{R}^F)\\), it holds \\(D=\\pi_{GF}^{-1}(A)\\approx \\mathbb{R}^{G\\setminus F}\\times A\\) where \\(\\approx\\) stands for reordering of the coordinates of \\(G\\) along those who belong to \\(F\\). It follows that</p> \\[ P_{F}\\left[ A \\right]=\\int_{A}^{}dP_{F}=\\int_{A}^{}\\int_{\\mathbb{R}^{G\\setminus F}}^{}dP_{F}dP_{G\\setminus F}=\\int_{D}^{}dP_{G}=P_{G}[D]  \\] <p>showing that \\((P_{F})\\) is a consistent family of probability measures. Hence, by Kolmogorov extension's theorem, it follows that it can uniquely be extended to \\(\\Omega\\) and it holds</p> \\[ P\\left[ X \\in \\pi_{F}^{-1}(A) \\right]=P_{F}\\left[ A \\right] \\] <p>for any \\(A\\) in \\(\\mathcal{F}^{F}\\) where $F={t_0,t_1,\\ldots, t_n} with \\(0&lt; t_0&lt; t_1&lt;\\ldots&lt;t_n\\).</p> <p>Let us show that \\(X\\) under \\(P\\) fulfills the first two properties of a Brownian motion. As for the first property, it holds that</p> \\[   \\begin{align*}      P\\left[ X_t-X_s \\leq x \\right]   &amp; =P_{\\{s,t\\}}\\left[ \\{(x_1,x_2)\\in \\mathbb{R}^2 \\colon x_2\\leq x+x_1\\} \\right]\\\\                                       &amp; =\\int_{\\mathbb{R}}^{}\\int_{-\\infty}^{x+y_1}p(s,0,y_1)p(t-s,y_1,y_2)dy_1dy_2\\\\                                        &amp; =\\int_{\\mathbb{R}}^{}\\int_{-\\infty}^{x}p(s,0,y_1)p(t-s,y_1,y_2-y_1)dy_1dy_2\\\\                                       &amp; =\\int_{-\\infty}^{x}p(t-s,0,y_2)dy_2   \\end{align*} \\] <p>showing that \\(X_t-X_s\\sim \\mathcal{N}(0,t-s)\\).</p> <p>As for the independence, \\((X_{t_n}-X_{t_{n-1}}, \\ldots, X_{t_1}-X_{t_0})\\) is a Gaussian vector for every finite \\(0\\leq t_0&lt;\\ldots&lt; t_n\\). Indeed, by the definition of \\(P\\), every linear combination of them yields a normal distribution -- check it as an exercise. Hence, being a Gaussian vector, it is enough to check that their covariance is equal to \\(0\\).</p> <p>We check it for \\(n=2\\), and the general case being left to you. By obvious variable change we have</p> \\[   \\begin{align*}      E\\left[ (X_{t_{2}}-X_{t_1})(X_{t_1}-X_{t_0}) \\right]         &amp; = \\int_{\\mathbb{R}^4} \\left( x_3-x_2 \\right)\\left( x_1-x_0 \\right)p(t_0,0,x_0)p(t_0-t_1,x_0,x_1)p(t_1-t_2,x_1,x_2)p(t_3-t_2,x_2,x_3)dx_0dx_1dx_2dx_3\\\\         &amp; =\\int_{\\mathbb{R}^2} y_1 y_2 p(t_1-t_0,0,y_1)p(t_3-t_2,0,y_2)dy_1dy_2\\\\         &amp; =0   \\end{align*} \\] <p>finishing the proof.</p>"},{"location":"lecture/05-Markov/053-brownian/#continuity-of-paths-kolmogorov-centov-theorem","title":"Continuity of Paths: Kolmogorov-\u010centov Theorem","text":"<p>As for the continuity, we should prove that</p> \\[   P[\\{\\omega \\colon t\\mapsto X_t(\\omega)\\text{ is continuous}\\}]=1 \\] <p>to ensure that we have a Brownian motion. Since \\(X\\) is the canonical process, it follows that \\(\\{\\omega \\colon t\\mapsto X_t(\\omega)\\text{ is continuous}\\}\\) is exactly the set \\(C[0,t)\\subseteq \\mathbb{R}^{[0,\\infty)}\\) of continuous functions. A naive way to show this would be to show that \\(P\\) assigns probability one to the subset \\(C[0,t)\\subseteq \\mathbb{R}^{[0,\\infty)}\\) of continuous functions. In that case, it would follow that almost all the paths of \\(X\\) are concentrated under \\(P\\) in the space of continuous functions. This strategy is hopeless as the following proposition shows.</p> <p>Proposition</p> <p>The set of continuous functions is not a measurable set for the product \\(\\sigma\\)-algebra of \\(\\mathbb{R}^{[0,\\infty)}\\).</p> <p>Proof</p> <p>Intuitively, it follows from the fact that measurable sets of \\(\\mathbb{R}^{[0,\\infty)}\\) for the product \\(\\sigma\\)-algebra are generated by finite-dimensional ones of the form</p> \\[   C=\\{\\omega \\colon \\omega_{t_0}\\in A_0, \\ldots, \\omega_{t_n}\\in A_n\\}, \\quad A_k \\in \\mathcal{B}(\\mathbb{R}) \\] <p>As an exercise, try to show why this proposition holds.</p> <p>To overcome this difficulty, we will modify the canonical process \\(X\\) to another process \\(B\\) which has continuous paths with probability \\(1\\), that is construct a version of the Brownian motion that has continuous paths.</p> <p>Definition: Modification vs Indistinguishable</p> <p>Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space and \\(X\\) and \\(Y\\) be two stochastic processes. We say that</p> <ul> <li> <p>\\(X\\) is a modification of \\(Y\\) if</p> \\[   P\\left[ X_t = Y_t \\right] = 1 \\text{ for any }t \\text{ in }\\mathbf{T} \\] </li> <li> <p>\\(X\\) is indistinguishable from \\(Y\\) if</p> \\[   P\\left[ \\{ \\omega\\colon X_t(\\omega) = Y_t(\\omega) \\text{ for all }t \\text{ in }\\mathbf{T}\\}\\right] = 1 \\] </li> </ul> <p>Remark</p> <p>Clearly, indistinguishable implies modification. If \\(\\mathbf{T}\\) is countable, then the reciprocal is true as</p> \\[     P[\\{ \\omega\\colon X_t(\\omega) \\neq Y_t(\\omega) \\text{ for some }t \\text{ in }\\mathbf{T}\\}] = P\\left[ \\cup_{t \\in \\mathbf{T}} \\{X_t \\neq Y_t\\} \\right] \\leq \\sum_{t \\in \\mathbf{T}} P[X_t \\neq Y_t] = 0 \\] <p>If \\(\\mathbf{T}\\) is not countable, then \\(\\cup_{t \\in \\mathbf{T}}\\{X_t\\}\\) is not a countable union of measurable sets and we can not apply subadditivity. The following counter example illustrate this fact.</p> <p>Let \\(\\Omega = [0,1]^{\\mathbf{T}}\\) and \\(\\mathcal{F} = \\otimes_{t \\in \\mathbf{T}} \\mathcal{B}([0,1])\\) for \\(\\mathbf{T} = [0,1]\\). As for the measure we consider the product measure such that each marginal is equal to lebesgue. Define the two processes</p> \\[     X_t \\equiv 0 \\quad \\text{and}\\quad Y_t (\\omega) =        \\begin{cases}       0 &amp; \\text{if }\\omega \\neq t\\\\       \\omega &amp; \\text{if }\\omega = t       \\end{cases} \\] <p>This defines two stochastic processes. Clearly, for every \\(t\\), the two processes differes only on one point which is of lebesgue measure \\(0\\). Hence they are modifications of each others. However \\(\\{\\omega\\colon X_t(\\omega) = Y_t(\\omega)\\text{ for all }0\\leq t\\leq 1\\} = \\emptyset\\). </p> <p>If \\(P\\) is a probability measure on the path space such that \\(X\\) is a pre-Brownian motion and \\(B\\) is a modification of \\(X\\), then \\(B\\) is a pre Brownian motion as well as it only involves finitely many marginals. The goal is therefore to find a modification \\(B\\) of \\(X\\) such that \\(P[\\{\\omega\\colon t \\mapsto B_t(\\omega) \\text{ is continuous }\\}] = 1\\) which is a consequence of the following theorem</p> <p>Kolmogorov-\u010centov</p> <p>Let \\((\\Omega,\\mathcal{F},P)\\) be a probability space and \\(\\tilde{X}=(\\tilde{X}_{t})_{0\\leq t\\leq T}\\) be a stochastic process. Suppose that</p> \\[   E\\left[ |\\tilde{X}_t-\\tilde{X}_s|^{\\alpha} \\right]\\leq C|t-s|^{1+\\beta} \\] <p>for every \\(s&lt;t\\leq T\\) and some strictly positive constants \\(\\alpha,\\beta\\), and \\(C\\). Then \\(\\tilde{X}\\) admits a continuous modification \\(X\\) which is locally H\\\"older continuous for every exponent \\(0&lt;\\gamma&lt;\\beta/\\alpha\\), that is</p> \\[   P\\left[ \\left\\{ \\omega \\colon \\sup_{0&lt;t-s&lt;h(\\omega), t,s\\leq T}\\frac{|X_t(\\omega)-X_s(\\omega)|}{|t-s|^\\gamma}\\leq \\delta \\right\\} \\right]=1 \\] <p>where \\(h\\) is an almost surely strictly positive random variable and \\(\\delta&gt;0\\).</p> <p>Proof</p> <p>We show it for \\(T=1\\) and define \\(\\Pi^n=\\{k/2^n\\colon k=0,\\ldots,2^n\\}\\) as well as \\(\\Pi=\\cup \\Pi^n\\), the sequence of dyadic numbers in \\([0,1]\\).</p> <ul> <li> <p>Step 1: H\\\"older continuity restricted to \\(\\Pi^n\\)     Denote by</p> \\[ A_n = \\max_{\\substack{s,t \\in \\Pi^n\\\\ |t-s|\\leq 2^{-n}}}|\\tilde{X}_t-\\tilde{X}_s|\\geq 2^{-n\\gamma} \\] <p>By Markov's inequality,</p> \\[   P[|\\tilde{X}_t-\\tilde{X}_s|\\geq \\varepsilon] \\leq \\frac{1}{\\varepsilon^\\alpha} E[|\\tilde{X}_t-\\tilde{X}_s|^\\alpha] \\leq C \\varepsilon^{-\\alpha}|t-s|^{1+\\beta}. \\] <p>Hence for \\(0&lt;\\gamma &lt; \\beta/\\alpha\\), \\(\\varepsilon=2^{-\\gamma n}\\), it holds</p> \\[ P\\left[ A_n \\right] \\leq C 2^{-n(\\beta - \\alpha\\gamma)} \\] <p>Since \\(\\beta - \\alpha \\gamma &gt; 0\\) by the very choice of \\(\\gamma\\), it follows from Borel-Cantelli that</p> \\[ P\\left[\\limsup A_n \\right] = P\\left[ \\bigcap_n \\bigcup_{k\\geq n} A_k \\right]=0 \\] <p>In other terms, for each \\(\\omega\\) out of a set \\(\\mathcal{N}\\) of zero measure, it holds that</p> \\[ 1_{\\cap_{k \\geq n} }A_k^c (\\omega) \\rightarrow 1 \\] <p>Denote by </p> \\[ n_0\\left( \\omega \\right) = \\inf\\left\\{ n \\in \\mathbb{N}\\colon 1_{ \\cap_{k\\geq n}A_k^c}(\\omega)\\geq \\frac{1}{2}\\right\\} \\] <p>it follows that \\(n_0\\) is a random variable finite on \\(\\Omega \\setminus \\mathcal{N}\\). Hence</p> \\[ \\max_{\\substack{s,t \\in \\Pi^n\\\\ |t-s|\\leq 2^{-n}}}|\\tilde{X}_t(\\omega)-\\tilde{X}_s(\\omega)|&lt; 2^{-n\\gamma} \\] <p>for all \\(n\\) greater than \\(n_0(\\omega)\\).</p> </li> <li> <p>Step 2: Definition of the continuous version \\(X\\).</p> </li> </ul> <p>For \\(\\omega\\) in \\(\\mathcal{N}\\) we set \\(X(\\omega)=0\\).   For \\(\\omega\\) outside of \\(\\mathcal{N}\\), and \\(t\\), we pick a sequence \\((s_n)\\) of elements in \\(\\Pi\\) converging to \\(t\\), which by the uniform continuity of \\(\\tilde{X}(\\omega)\\) on \\(\\Pi\\) yields a limit</p> <p>[     X_t(\\omega)=\\lim \\tilde{X}_{s_n}(\\omega)   ]</p> <p>independent of the choice of the sequence \\((s_n)\\) in \\(\\Pi\\) converging to \\(t\\).   It follows that \\(X\\) has continuous paths.   Defined as a limit of a sequence of random variables, it follows also that \\(X\\) is a process.   Finally, \\(\\{X_t=\\tilde{X}_t\\}\\) is contained in the set of those \\(\\omega\\) such that \\(\\tilde{X}_{s_n}(\\omega)\\) has a limit for some sequence \\((s_n)\\) in \\(\\Pi\\) converging to \\(t\\), it follows that \\(\\{X_t = \\tilde{X}_t\\} \\supseteq \\mathcal{N}^c\\).   Hence \\(P[X_t=\\tilde{X}_t]\\geq P[\\mathcal{N}^c]=1\\), showing that \\(X\\) is a version of \\(\\tilde{X}\\) which ends the proof of the theorem.</p> <ol> <li> <p>A complete metric space which is separable.\u00a0\u21a9</p> </li> </ol>"},{"location":"lecture/06-Stochastic-Exponential/031-multi-period-financial-markets/","title":"Multi Period Financial Markets","text":"<p>Up to now, we have only considered a static version of a financial market, with a single decision at time \\(0\\) and a delivery at time \\(1\\). This analysis applies to many buy-and-hold investors. However, as time progresses, more information about the outcomes of contingent claims or portfolios becomes available, allowing investors to adapt their strategies to take advantage of new information.</p>"},{"location":"lecture/06-Stochastic-Exponential/031-multi-period-financial-markets/#mathematical-model","title":"Mathematical Model","text":"<p>As always, we consider a probability space \\( (\\Omega, \\mathcal{F}, P) \\), which describes the states of the world, the events, and their likelihood. The market still consists of a bank account \\( B \\) and \\( d \\) financial assets \\( \\boldsymbol{S} = (S^1, \\ldots, S^d) \\). Now, we extend the model to \\( T+1 \\) time periods \\( t = 0, \\ldots, T \\).</p> <ul> <li> <p>Bank Account:     As in the static case, the bank account \\( B \\) describes the evolution of one unit of currency over time.     However the bank now can revise the interest rate it delivers between \\(t\\) and \\(t+1\\) at every time \\(t\\) and this might not be know at time \\(0\\).     Hence, we define the growth of the bank account as following classical compounding for interest rates:</p> \\[   B_0 = 1, \\quad B_t(\\omega) = B_{t-1}(1+r_t) = \\prod_{s=1}^t (1+r_s), \\] <p>where \\(r_t = (B_t - B_{t-1})/B_t\\) is a random variable strictly greater than \\(-1\\).</p> <p>In other terms \\(r_t(\\omega)\\) represents the interest rate set by the bank at time \\(t-1\\) to remunerate cash deposited in the account at time \\(t-1\\) for the period between \\(t-1\\) and \\(t\\).</p> </li> <li> <p>Financial Assets:</p> <p>We have \\(d\\) of them and as the bank account, their price evolution can be observed at any time \\(t=0, \\ldots, T\\). Therefore we define the \\(k\\)-th financial asset \\(S^k = (S^k_t){t=0, \\ldots, T}\\) as a family of random variables</p> \\[ \\begin{equation*}   \\begin{split}     S_t^k \\colon \\Omega &amp; \\longrightarrow [0, \\infty)\\\\                 \\omega &amp; \\longmapsto S_t^k(\\omega) = \\text{Price at time $t$ in state $\\omega$ of asset $k$}   \\end{split} \\end{equation*} \\] <p>We assume however that \\(S_0^k&gt;0\\) for the price at time \\(0\\) meaning that the financial asset is not defaulted at time \\(0\\).</p> </li> </ul> <p>In general, a family \\(Z = (Z_t)_{t=0, \\ldots, T}\\) where each \\(Z_t\\) is a random variable, is called a stochastic process.</p> <ul> <li> <p>Portfolio:</p> <p>An investor starts with an initial wealth \\( \\bar{V}_0 \\) and may adjust their holdings over time. This strategy is modeled by a \\( d \\)-dimensional vector \\( \\boldsymbol{\\eta} = (\\eta^1, \\ldots, \\eta^d) \\), which is also a (\\(d\\)-dimensional) stochastic process:</p> \\[ \\begin{equation*}     \\begin{split}         \\eta_t^k \\colon \\Omega &amp;\\longrightarrow \\mathbb{R}\\\\         \\omega &amp; \\longmapsto \\eta_t^k(\\omega),     \\end{split} \\end{equation*} \\] <p>for every \\( t = 1, \\ldots, T \\) and \\( k = 1, \\ldots, d \\). The decision about \\( \\boldsymbol{\\eta}_t \\) is made at time \\( t-1 \\) and represents the portfolio held from time \\( t-1 \\) to \\( t \\).</p> <p>Now given a portfolio value \\(\\bar{V}_{t-1}\\) at time \\(t-1\\) and a strategic decision \\(\\boldsymbol{\\eta}_{t}\\) of holdings until \\(t\\), yields an updated value of the portfolio at time \\(t\\) given by</p> \\[ \\begin{align*}     \\bar{V}_{t}  &amp; =  \\underbrace{\\left( \\bar{V}_{t-1} - \\boldsymbol{\\eta}_{t}\\cdot \\boldsymbol{S}_{t-1} \\right)}_{\\text{Cash minus cost of holdings}}(1+r_{t}) + \\underbrace{\\boldsymbol{\\eta}_{t}\\cdot \\boldsymbol{S}_{t}}_{\\text{Current holding value}} \\\\     &amp;= \\bar{V}_{t-1}(1+r_{t}) + \\boldsymbol{\\eta}_{t}\\cdot \\left( \\boldsymbol{S}_{t} - \\boldsymbol{S}_{t-1}(1+r_{t})\\right) \\end{align*} \\] </li> <li> <p>Discounted Values</p> <p>As in the one-period model, it is convenient to work with discounted values. The discounted stock prices \\( \\boldsymbol{X} = (X^1, \\ldots, X^d) \\) and discounted portfolio value \\( V_t \\) are defined as:</p> \\[     X_t^k = \\frac{S_t^k}{B_t} \\quad\\text{and}\\quad  V_t = \\frac{\\bar{V}_t}{B_t}, \\] <p>for \\( k = 1, \\ldots, d \\) and \\( t = 0, \\ldots, T \\). Clearly, \\( X_0 = S_0 \\) and \\( V_0 = \\bar{V}_0 \\).</p> <p>Just as in the one period model, it holds that</p> \\[     \\begin{align*}         V_t &amp; = \\frac{\\bar{V}_t}{B_t} \\\\         &amp; = \\frac{1}{B_{t-1}(1+r_{t})}\\left( \\bar{V}_{t-1}(1+r_{t}) + \\boldsymbol{\\eta}_{t}\\cdot \\left( \\boldsymbol{S}_{t} - \\boldsymbol{S}_{t-1}(1+r_{t}\\right) \\right)\\\\         &amp; = \\frac{\\bar{V}_{t-1}}{B_{t-1}} + \\boldsymbol{\\eta}_t\\cdot \\left( \\frac{\\boldsymbol{S}_t}{B_t} - \\frac{\\boldsymbol{S}_{t-1}}{B_{t-1}} \\right)\\\\         &amp; = V_{t-1} + \\boldsymbol{\\eta}_t \\cdot  \\underbrace{\\left(\\boldsymbol{X}_t - \\boldsymbol{X}_{t-1}\\right)}_{:= \\Delta \\boldsymbol{X}_t}         = V_0 + \\sum_{s=1}^t \\boldsymbol{\\eta}_s\\cdot\\Delta \\boldsymbol{X}_s     \\end{align*} \\] <p>leading to </p> <p>Lemma</p> <p>Let \\( \\boldsymbol{\\eta} \\) be a strategy and \\( V_0 \\) the initial value of a self-financing portfolio. Then:</p> \\[   V_t = V_0 + \\sum_{s=1}^t \\boldsymbol{\\eta}_s \\cdot \\Delta \\boldsymbol{X}_s \\] <p>where \\( \\Delta \\boldsymbol{X}_s = \\boldsymbol{X}_s - \\boldsymbol{X}_{s-1} \\).</p> </li> </ul>"},{"location":"lecture/06-Stochastic-Exponential/031-multi-period-financial-markets/#information","title":"Information","text":"<p>Up to now, we have not considered how decisions are influenced by additional information. Mathematically, information is represented by collections of events. A random variable is \"known\" at a given time if its value is determined by the events available at that time.</p> <p>Definition</p> <p>A filtration is a family \\( \\mathbb{F} = (\\mathcal{F}_t)_{0 \\leq t \\leq T} \\) of \\(\\sigma\\)-algebras such that:</p> \\[ \\mathcal{F}_0 \\subseteq \\mathcal{F}_1 \\subseteq \\cdots \\subseteq \\mathcal{F}_T \\subseteq \\mathcal{F}. \\] <p>Note</p> <p>Throughout, we assume that the initial information \\( \\mathcal{F}_0 \\) is the trivial \\(\\sigma\\)-algebra \\( \\mathcal{F}_0 = \\{\\emptyset, \\Omega\\} \\). In particular, any \\(\\mathcal{F}_0\\) random variable is a constant.(1)</p> <ol> <li>It is a basic exercise to check. Suppose that a random variable \\(X\\) is \\(\\mathcal{F}_0=\\{\\emptyset, \\Omega\\}\\)-measurable then it follows that \\(X\\) is constant. Indeed, if it where not, let \\(\\omega_1\\) and \\(\\omega_2\\) be two states on which \\(X(\\omega_1) &lt; X(\\omega_2)\\), let \\(x\\) be such that \\(X(\\omega_1)&lt;x&lt;X(\\omega_2)\\), it follows that \\(\\omega_1 \\in A=\\{X\\leq x\\}\\) while \\(\\omega_2 \\not \\in A\\). This is however not possible since the event \\(A\\) is either \\(\\Omega\\) or \\(\\emptyset\\).</li> </ol> <p>Definition</p> <p>A collection of random variables indexed by time is called a stochastic process. A stochastic process \\( X = (X_t)_{0 \\leq t \\leq T} \\) is:</p> <ol> <li>Adapted if \\( X_t \\) is \\(\\mathcal{F}_t\\)-measurable for all \\( t = 0, \\ldots, T \\).</li> <li>Predictable if \\( X_t \\) is \\(\\mathcal{F}_{t-1}\\)-measurable for all \\( t = 1, \\ldots, T \\).</li> </ol> <p>Adapted means that the process at time \\(t\\) only depends on the information up to time \\(t\\) while predictable means that it depends on the information of yesterday. With this definition in mind, it follows that in a multi-period financial market we have</p> <ul> <li>The stochastic process modeling financial assets \\( \\boldsymbol{S} \\) is adapted.</li> <li>The interest rate process \\( r \\) is predictable (announced beforehand the next period by the bank)</li> <li>Financial strategies \\( \\boldsymbol{\\eta} \\) are predictable (decided for the next period).</li> </ul> <p>Multi-period</p> <p>A multi-period financial market consists of:</p> <ol> <li>A probability space \\( (\\Omega, \\mathcal{F}, P) \\).</li> <li>A filtration \\( \\mathbb{F} = (\\mathcal{F}_t)_{0\\leq t\\leq T} \\).</li> <li>A \\(d\\)-dimensional adapted stochastic process \\( \\boldsymbol{S} = (\\boldsymbol{S}_t)_{0 \\leq t \\leq T} \\) of positive random variables.</li> <li>A bank account \\( B_0 = 1 \\), \\( B_t = \\prod_{s=1}^t (1 + r_s) \\), where \\( r \\) is a predictable process.</li> </ol> <p>Financial strategies \\( \\boldsymbol{\\eta} = (\\boldsymbol{\\eta}_t)_{0 \\leq t \\leq T} \\) are \\( d \\)-dimensional predictable processes.</p>"},{"location":"lecture/06-Stochastic-Exponential/031-multi-period-financial-markets/#arbitrage-pricing-measure","title":"Arbitrage, Pricing Measure","text":"<p>The primary notion of arbitrage in the static case is given by the fact that one can find a strategy such that it is possible to make strict positive gain without losing any money for sure. The same notion holds in the multi-period case.</p> <p>Definition: Arbitrage</p> <p>Given a financial market, a self-financing portfolio \\(\\bar{V}\\) with start value \\(\\bar{V}_0\\) and strategy \\(\\boldsymbol{\\eta}\\) is called an arbitrage opportunity if</p> \\[     P\\left[ \\bar{V}_T \\geq \\bar{V}_0 B_T \\right] = 1 \\quad \\text{and} \\quad P\\left[ \\bar{V}_T &gt; \\bar{V}_0 B_T \\right] &gt; 0. \\] <p>Note that as in the one-period model, this definition is independent of discounting and start since it is equivalent to</p> \\[   P\\left[ V_T \\geq V_0 \\right] = 1 \\quad \\text{and} \\quad P\\left[ V_T &gt; V_0 \\right] &gt; 0 \\] <p>and  </p> \\[   P\\left[ \\sum_{s=1}^T \\boldsymbol{\\eta}_s \\Delta \\boldsymbol{X}_s \\geq 0 \\right] = 1 \\quad \\text{and} \\quad P\\left[ \\sum_{s=1}^T \\boldsymbol{\\eta}_s \\cdot \\Delta \\boldsymbol{X}_s &gt; 0 \\right] &gt; 0 \\] <p>for a strategy \\(\\eta\\).</p> <p>This definition is global between \\(0\\) and \\(T\\) in the sense that along the way an arbitrage can be constructed. However, the following proposition shows that it is equivalent to a local definition where an arbitrage must exist between two consecutive times.</p> <p>Proposition</p> <p>Given a financial market, the following assertions are equivalent:</p> <ol> <li>Global arbitrage: There exists an arbitrage opportunity.</li> <li>Local arbitrage: There exists some time \\(t\\) between \\(1\\) and \\(T\\) and an \\(\\mathcal{F}_{t-1}\\)-measurable random variable \\(\\boldsymbol{\\mu} \\colon \\Omega \\to \\mathbb{R}^d\\) such that</li> </ol> \\[   P[\\boldsymbol{\\mu} \\cdot \\Delta \\boldsymbol{X}_t \\geq 0] = 1 \\quad \\text{and} \\quad P\\left[ \\boldsymbol{\\mu} \\cdot \\Delta \\boldsymbol{X}_t &gt; 0 \\right] &gt; 0. \\] Proof <ol> <li> <p>We show that the second assertion implies the first with some \\(\\mathcal{F}_{t-1}\\)-measurable strategy \\(\\boldsymbol{\\mu}\\) which is an arbitrage between \\(t-1\\) and \\(t\\).     Let \\(\\boldsymbol{\\eta}\\) be the \\(\\mathbb{R}^d\\)-valued process given by</p> \\[   \\eta_s =       \\begin{cases}         \\mu &amp; \\text{if } s = t \\\\         0 &amp; \\text{otherwise.}       \\end{cases} \\] <p>By definition, \\(\\boldsymbol{\\eta}\\) is predictable, for which holds \\(\\sum_{s=1}^T \\boldsymbol{\\eta}_s \\cdot \\Delta \\boldsymbol{X}_s = \\boldsymbol{\\mu}\\cdot \\Delta \\boldsymbol{X}_t\\) showing that \\(\\boldsymbol{\\eta}\\) is an arbitrage globally.</p> </li> <li> <p>Conversely, let \\(\\boldsymbol{\\eta}\\) be a global arbitrage strategy strategy with the corresponding discounted value process \\(V\\), and define</p> \\[   t := \\min\\left\\{s = 0, \\ldots, T : V_s \\geq V_0 \\text{ and } P\\left[ V_s &gt; V_0 \\right] &gt; 0 \\right\\}. \\] <p>being the first time where an arbitrage kicks in your portfolio. By assumption, \\(t \\leq T\\) and by definition, it follows that \\(V_{t-1} = V_0\\) or \\(P[V_{t-1} &lt; V_0] &gt; 0\\).</p> <p>In the first case, since</p> \\[   \\boldsymbol{\\eta}_t \\cdot \\Delta \\boldsymbol{X}_t = V_t - V_{t-1} = V_t - V_0 \\geq 0, \\] <p>it follows that \\(\\boldsymbol{\\mu} = \\boldsymbol{\\eta}_t\\), which is \\(\\mathcal{F}_{t-1}\\)-measurable and fulfills the assumptions of the local arbitrage.</p> <p>In the second case, let \\(\\boldsymbol{\\mu} = \\boldsymbol{\\eta}_t 1_{\\{V_{t-1} &lt; V_0\\}}\\), which is \\(\\mathcal{F}_{t-1}\\)-measurable. It follows that</p> \\[   \\boldsymbol{\\mu} \\cdot \\Delta \\boldsymbol{X}_t = \\left(V_t - V_{t-1}\\right)1_{\\{V_{t-1} &lt; V_0\\}} \\geq \\left(V_0 - V_{t-1}\\right)1_{\\{V_{t-1} &lt; V_0\\}} \\geq 0, \\] <p>and since \\(P[V_{t-1} &lt; V_0] &gt; 0\\), this inequality holds strictly with a strict positive probability showing that \\(\\boldsymbol{\\mu}\\) is a local arbitrage.</p> </li> </ol> <p>In the first section, we saw that a market is fair if and only if there exists a pricing measure \\(P^\\ast\\) such that</p> \\[   E^{P^\\ast}\\left[\\boldsymbol{X}_1\\right] = \\boldsymbol{X}_0 \\] <p>In a multiperiod setting, this equation translates into:</p> \\[ E^{P^\\ast}\\left[\\boldsymbol{X}_{t+1}|\\mathcal{F}_{t}\\right] = \\boldsymbol{X}_{t} \\] <p>saying that under the pricing measure, the expected discounted value of every asset at time \\(t+1\\) conditioned on the information available at time \\(t\\) is equal to the discounted value of those assets at time \\(t\\). This statement brings us to the following important notion in the theory of stochastic processes.</p> <p>Definition: Martingale</p> <p>Given a probability measure \\(Q\\), a stochastic process \\(M = (M_t)_{0 \\leq t \\leq T}\\) is called a \\(Q\\)-martingale if:</p> <ol> <li>\\(M\\) is an adapted process.</li> <li>\\(M\\) is \\(Q\\)-integrable.  </li> <li> <p>\\(M\\) satisfies the \\(Q\\)-martingale property, that is,  </p> \\[   E^Q\\left[M_{t+1}|\\mathcal{F}_t\\right] = M_t, \\] <p>for every \\(t = 0, \\ldots, T-1\\).</p> </li> </ol> <p>Note</p> <p>Given a probability \\(Q\\), the most simple example of a martingal is the expectation of a given (integrable random variable). Indeed, let \\(C\\) be an (integrable) random variable and define </p> \\[   M_t = E^Q\\left[ C | \\mathcal{F}_t \\right] \\] <p>By definition \\(M = (M_t)\\) is an adapted and integrable process. Furthermore, by the tower property of the conditional expectation we get</p> \\[   E^Q\\left[ M_{t+1} |\\mathcal{F}_t\\right] = E^{Q}\\left[ E^Q\\left[ C|\\mathcal{F}_{t+1} \\right] |\\mathcal{F}_t\\right] = E^Q\\left[ C|\\mathcal{F}_t \\right] = M_t \\] <p>which leads to the definition of a pricing measure</p> <p>Definition</p> <p>A probability measure \\(P^\\ast\\) is called a pricing measure if the discounted price process \\(\\boldsymbol{X}\\) is a \\(d\\)-dimensional \\(P^\\ast\\)-martingale.</p> <p>Proposition</p> <p>Let \\(P^\\ast\\) be a probability measure. The following conditions are equivalent</p> <ul> <li>\\(P^\\ast\\) is a pricing measure;</li> <li>Any (discounted) portfolio \\(V = V_0 + \\sum \\boldsymbol{\\eta}_s \\cdot \\Delta \\boldsymbol{X}_s\\) for some (bounded) strategy \\(\\boldsymbol{\\eta}\\) is a \\(P^\\ast\\)-martingale;</li> </ul> <p>Proof</p> <p>As for the first assertion implying the second, let \\(P^\\ast\\) be a risk pricing measure and \\(\\boldsymbol{\\eta}\\) a bounded strategy, then \\(V\\) is a \\(P^\\ast\\)-martingale.(1) Since \\(\\boldsymbol{\\eta}\\) is predictable, it follows that</p> <ol> <li> <p>In your context you just need to check the martingale property, however the other two assumption shall be checked too. Adaptiveness is usually given right away, in our case it follows from the fact that \\(V\\) is an adapted process as a scalar product between a predictable and an adapted process. Integrability is usually also straightforward, yet mathematically should be checked. In our case since \\(\\boldsymbol{\\eta}\\) is uniformly bounded, it follows that     [       \\begin{equation}         |V_t|\\leq |V_0|+\\sum_{s=1}^t\\sum_{k=0}^d|\\eta_s^k||\\Delta X_s^k|\\leq |V_0|+C\\sum_{s=1}^t\\sum_{k=0}^d|\\Delta X_s^k|       \\end{equation}     ]</p> <p>for every \\(t\\), where \\(C\\) is the constant such that \\(|\\eta_s^k|\\leq C\\) for every \\(k=0,\\ldots,d\\) and \\(s=0,\\ldots ,T\\). Since \\(\\boldsymbol{X}\\) is \\(P^\\ast\\) integrable, it follows that \\(V\\) is also too.</p> </li> </ol> \\[   \\begin{equation*}       E^{P^\\ast}\\left[ V_{t+1}|\\mathcal{F}_{t} \\right]=E^{P^\\ast}\\left[ V_{t} + \\boldsymbol{\\eta}_{t+1}\\cdot \\Delta \\boldsymbol{X}_{t+1}|\\mathcal{F}_{t} \\right]=V_t + \\boldsymbol{\\eta}_{t+1}\\cdot E^{P^\\ast}\\left[ \\Delta \\boldsymbol{X}_{t+1}|\\mathcal{F}_{t} \\right]=0   \\end{equation*} \\] <p>Hence, \\(V\\) is a \\(P^\\ast\\)-martingale.</p> <p>Reciprocally, if every discounted portfolio is a \\(P^\\ast\\) martingale, fix \\(k\\) in \\(\\{1,\\ldots,d\\}\\), and define</p> \\[ \\begin{equation*}     \\eta_t^i=     \\begin{cases}         1 &amp;\\text{if }i=k\\\\         0 &amp;\\text{otherwise}     \\end{cases} \\end{equation*} \\] <p>for any \\(t=1,\\ldots,T\\) and \\(i=1,\\ldots, d\\) which defines a uniformly bounded strategy \\(\\boldsymbol{\\eta}\\). Furthermore, by definition, it holds that \\(V = X^k\\). Hence \\(V\\) being a \\(P^\\ast\\) martingale implies that \\(X^k\\) is a \\(P^\\ast\\) martingale too. Hence \\(P^\\ast\\) is a pricing measure.</p>"},{"location":"lecture/06-Stochastic-Exponential/031-multi-period-financial-markets/#fundamental-theorem-of-asset-pricing","title":"Fundamental Theorem of Asset Pricing","text":"<p>As exposed before, the multi period setting can be cast as a sequence of pasted successive one period models. Hence, it is not a surprise that all the results derived in one period model do extend into the multiperiod one. The intuition is the same, it just requires the infinite dimensional extension of the separation Theorem of Hahn-Banach which is beyond the scope of this lecture.</p> <p>Fundamental Theorem of Asset Pricing</p> <p>The financial market model is arbitrage free if, and only if, there exists a pricing measure \\(P^\\ast\\) equivalent to \\(P\\).</p> <p>This risk pricing neutral pricing measure can be chosen such that \\(dP^\\ast/dP\\) is bounded.</p> <p>Super- and sub-hedging results carries over the same way. In particular, given a contingent claim \\(C\\) paying off at maturity \\(T\\), a fair price \\(\\pi(C)=(\\pi_t(C))\\) is a stochastic process given by</p> \\[ \\pi_t(C) = B_t E^{P^\\ast}\\left[ \\frac{C}{B_T} | \\mathcal{F}_t \\right] \\] <p>As in the one period model you have the price at time \\(0\\) given by \\(\\pi_0(C) = E^{P^\\ast}[C/B_T]\\) which is the discounted expectation of the contingent claim, while \\(\\pi_T(C) = B_T E^{P^\\ast}[C/B_T |\\mathcal{F}_T] = C\\) is the contingent claim itself at time \\(T\\).</p>"},{"location":"lecture/06-Stochastic-Exponential/031-multi-period-financial-markets/#exotic-derivatives","title":"Exotic Derivatives","text":"<p>European derivatives (Put/Call, butterfly, Straddle, Forward) definition follows directly from the one period. However, since we have now several periods, new class of options can be defined where intermediary conditions depending on time can be part of the contract. Those options are typically called exotic derivatives.</p> <ul> <li> <p>European Options:</p> <p>As mentioned it is a straight forward extension, where the horizon \\(T\\) is called maturity. For instance European call and put options</p> \\[ \\begin{align*}   C^{call} &amp; = (S_T - K)^+ \\\\   C^{put} &amp; = (K-S_T)^+  \\end{align*} \\] </li> <li> <p>Asian Option: Asian option typically depends on the time average of an underlying.</p> <p>We define the rolling time average between \\(1\\) and \\(t\\) of an asset \\(S\\) as follows</p> \\[     S^{av}_t = \\frac{1}{t} \\sum_{s=1}^t S_s \\] <p>Asian options, are contracts written on this time average, for instance an Asian call or put option</p> \\[ \\begin{align*}   C^{asian\\,call} &amp; = (S^{av}_T - K)^+ = \\left( \\frac{1}{T}\\sum_{t=1}^T S_t -K \\right)^+\\\\   C^{asian\\,put} &amp; = (S^{av}_T - K)^+ = \\left( K-\\frac{1}{T}\\sum_{t=1}^T S_t \\right)^+ \\end{align*} \\] <p>In the following illustration thick lines stands for paths while dashed stands for exanding average. For an asian call, only the red scenario returns a positive outcome. In contrast, for the standard call, only the red scenario is void.</p> <p> </p> </li> <li> <p>Barrier Options: Barrier options are options that depends on the path of the underlying hitting or not a given barrier at a given previous time.</p> <p>To do so, for a financial asset \\(S\\), we define the rolling maximum \\(\\overline{S}\\) and minimum \\(\\underline{S}\\) as</p> \\[   \\begin{align*}     \\overline{S}_t &amp; =\\max_{s\\leq t} S_s\\\\     \\underline{S}_t &amp; =\\min_{s\\leq t} S_S   \\end{align*} \\] <p>With these rolling maximum and minimum, we can define several conditions for a standard contract to hold</p> <ul> <li>knocked up and in: The contract hold only if \\(\\overline{S}_T \\geq B\\);</li> <li>knocked up and out: The contract is void if \\(\\overline{S}_T \\geq B\\);</li> <li>knocked down and in: The contract hold only if \\(\\underline{S}_T \\leq B\\);</li> <li>knocked down and out: The contract is void if \\(\\underline{S}_T \\leq B\\);</li> </ul> <p>With these conditions at hand, myriad of options can be defined starting from European or Asian options. For instance</p> \\[   \\begin{align*}     C^{call}_{up\\&amp;in} &amp;      = (S_T - K)^+ 1_{\\{ \\overline{S}_T\\geq B \\}}     =      \\begin{cases}       (S_T-K)^+ &amp; \\text{if }\\overline{S}_T \\geq B \\\\       0 &amp; \\text{otherwize}     \\end{cases}\\\\     C^{call}_{down\\&amp;out} &amp;      = (S_T - K)^+ 1_{\\{ \\underline{S}_T &gt; B \\}}     =      \\begin{cases}       (S_T-K)^+ &amp; \\text{if }\\underline{S}_T &gt; B \\\\       0 &amp; \\text{otherwize}     \\end{cases}   \\end{align*} \\] <p>In the following illustration thick lines stands for paths while dashed stands for the running maximum. For an up and out call, only the orange scenario returns a positive outcome. The blue scenario is above the strike however knocked the barrier before \\(T\\).</p> <p> </p> </li> </ul> <p>Computational Issues</p> <p>Given a contingent claim \\(C\\), any fair price of this contingent claim is given as \\(E^{P^\\ast}[C/B_T]\\). Setting the discounting factor to \\(1\\), if \\(C = f(S_T)\\) for some function, such as a plain European option, it follows that the price can be computed as:</p> \\[   E^{P^\\ast}\\left[ f(C) \\right] = E^{P^\\ast}\\left[ f(S_T) \\right] = \\int_{\\mathbb{R}} f(s) dF^\\ast_{S_T}(s) \\] <p>However, if the contingent claim is of Asian or barrier type, the payoff does not only depend on the last value of the financial asset but on the whole path. In other terms, \\(C = f(S_1, \\ldots, S_T)\\), such as \\(f(s_1, \\ldots, s_T) = \\left(\\frac{\\sum s_t}{T} - K\\right)^+\\), meaning that the contingent claim depends on the multidimensional distribution of the price over time. Suppose that this price has a density, that is:</p> \\[ dF^{\\ast}(S_1, \\ldots, S_T)(s_1, \\ldots, s_T) = \\phi(s_1, \\ldots, s_T) ds_1 \\ldots ds_T. \\] <p>It follows that:  </p> \\[   E^{P^\\ast}\\left[ f(C) \\right] = E^{P^\\ast}\\left[ f(S_1, \\ldots, S_T) \\right] = \\int_{\\mathbb{R}} \\int_{\\mathbb{R}} \\ldots \\int_{\\mathbb{R}} f(s_1, \\ldots, s_T) \\varphi(s_1, \\ldots, s_T) ds_1 \\ldots ds_T \\] <p>which is a high-dimensional integration.</p> <p>Classical integration becomes unusable beyond 2-3 dimensions as it suffers from the curse of dimensionality. Monte Carlo methods are then the way to go, but reasonable accuracy requires very large samples. This is one drawback of those exotic options.</p> <p>However, for barrier options, it turns out that in specific contexts, regardless of the horizon \\(T\\), the integration can be reduced to a two-dimensional one, which is tractable. Therefore, the popularity of barrier options (or similarly, snowball options).</p>"},{"location":"lecture/06-Stochastic-Exponential/032-crr-model/","title":"Binomial Model aka Cox Ross Rubinstein Model","text":"<p>The Cox Ross Rubinstein model can litteraly be considered as the discrete time full analog to the Black-Scholes-Merton model. The dynamic is entirely determined by the result of a coin toss every day, hence the following model:</p>"},{"location":"lecture/06-Stochastic-Exponential/032-crr-model/#probability-model","title":"Probability Model","text":"<p>The space of coint toss sequences is given by \\(\\Omega = \\{\\omega = (\\omega_1, \\ldots, \\omega_T) : \\omega_t = \\pm 1\\}\\) where \\(\\omega=(\\omega_1, \\ldots, \\omega_T)\\) represents each sequence of \\(\\pm1\\) as result of the coin toss. As for the \\(\\sigma\\)-algebra of events we consider \\(\\mathcal{F}=2^\\Omega\\). We also consider a probability measure \\(P\\) such that \\(P[\\{\\omega\\}] &gt; 0\\) for every \\(\\omega\\).</p> <p>As for the information we consider the stochastic process \\(Y=(Y_t)_{t=1, \\ldots T}\\) given by</p> \\[ \\begin{equation*}   \\begin{split}     Y_t\\colon \\Omega &amp; \\longmapsto \\{-1, 1\\}\\\\               \\omega &amp; \\longrightarrow Y_t(\\omega) = \\omega_t =  \\text{result of coin toss at time }t   \\end{split} \\end{equation*} \\] <p>In other terms the stochastic process \\(Y\\) provides information about the result of the coin toss at each time. We therefore define the filtration</p> \\[ \\mathcal{F}_0 = \\{\\emptyset, \\Omega\\} \\quad \\text{and}\\quad \\mathcal{F}_t = \\sigma\\left( Y_s\\colon s\\leq t \\right) \\] <p>In other terms, \\(\\mathcal{F}_t\\) is the set of events which have been revealed by all coin tosses up to time \\(t\\). It can be shown that if \\(\\xi\\) is a \\(\\mathcal{F}_t\\)-measurable random variable, then this random variable only depends on the coin tosses up to time \\(t\\), that is </p> \\[ \\xi(\\omega) = \\xi( \\underbrace{\\omega_1, \\ldots, \\omega_t}_{\\text{Information up to $t$}}, \\underbrace{\\omega_{t+1}, \\ldots, \\omega_T}_{\\text{Information after $t$}} ) = \\xi( \\omega_1, \\ldots, \\omega_t ) \\]"},{"location":"lecture/06-Stochastic-Exponential/032-crr-model/#market-model","title":"Market Model","text":"<ul> <li> <p>Bank account \\(B=(B_t)_{t=0, \\ldots, T}\\) whith</p> \\[   B_0 = 1, \\quad \\text{and}\\quad B_t = B_{t-1}(1+r) = (1+r)^t \\] <p>for a fixed interest rate \\(r&gt;-1\\). Clearly, the bank account is predictable since the interest rate is constant.</p> </li> <li> <p>Single risky asset \\(S = (S_t)_{t=0, \\ldots, T}\\) where</p> \\[   S_0&gt;0 \\quad \\text{and}\\quad S_t = S_{t-1}\\left( 1+R_t\\right) \\] <p>where the returns of the stock \\(R=(R_t)_{t=1, \\ldots, T}\\) is a stochastic process given by</p> \\[   \\begin{align*}     R_t(\\omega) &amp; =      \\begin{cases}       u &amp; \\text{if }\\omega_t = 1\\\\       d &amp; \\text{if }\\omega_t = -1     \\end{cases} \\end{align*} \\] <p>where \\(-1&lt;d&lt;u\\) are constants. Clearly, \\(R_t\\) only depends on the result of the coin toss at time \\(t\\), which can also be seen from</p> \\[     R_t = u 1_{\\{Y_t = 1\\}} + d 1_{\\{Y_t = -1\\}} = (u-d)1_{\\{Y_t = 1\\}} +d \\] <p>showing that \\(S\\) is an adapted process.</p> </li> </ul>"},{"location":"lecture/06-Stochastic-Exponential/032-crr-model/#no-arbitrage-and-pricing-measure","title":"No arbitrage and Pricing Measure","text":"<p>This financial model is called the binomial model or Cox, Ross, and Rubinstein model (CRR for short).</p> <p>Proposition</p> <p>The CRR model is arbitrage-free if and only if \\(d &lt; r &lt; u\\). In this case, the CRR model is complete with unique pricing measure \\(P^\\ast\\) equivalent to \\(P\\). This risk pricing measure is characterized by the fact that the random variables \\(R_1, \\ldots, R_T\\) are iid with common distribution:</p> \\[   P^\\ast[R_t = u] = p = \\frac{r - d}{u - d}, \\quad t = 1, \\ldots, T \\] <p>Proof</p> <p>By the FTAP, arbitrage-free is equivalent to the existence of a pricing measure \\(P^\\ast\\) equivalent to \\(P\\). Under such \\(P^\\ast\\), the discounted price process is a martingale. In particular, it holds:</p> \\[   0 = E^\\ast\\left[ \\Delta X_t \\mid \\mathcal{F}_{t-1} \\right] = E^\\ast\\left[ X_{t-1} \\left( \\frac{1 + R_t}{1 + r} - 1 \\right) \\mid \\mathcal{F}_{t-1} \\right] = X_{t-1} E^\\ast\\left[ \\frac{R_t - r}{1 + r} \\mid \\mathcal{F}_{t-1} \\right] \\] <p>Since \\(X_{t-1} &gt; 0\\) and \\(R_t = (u-d)1_{\\{Y_t = 1\\}} +d\\), it follows that</p> \\[ r = E^\\ast[R_t \\mid \\mathcal{F}_{t-1}] = (u - d) P^\\ast\\left[ Y_t = 1 \\mid \\mathcal{F}_{t-1} \\right] + d \\] <p>Showing that</p> \\[ P^\\ast\\left[ Y_t = 1 \\mid \\mathcal{F}_t \\right] = \\frac{r - d}{u - d} \\] <p>for every \\(t = 0, \\ldots, T\\).</p> <p>From this relation, we conclude three facts:</p> <ul> <li>The conditional distribution of \\(P^\\ast\\) under \\(\\mathcal{F}_{t-1}\\), that is, \\(P^\\ast[\\cdot \\mid \\mathcal{F}_{t-1}]\\), is constant for every \\(t = 0, \\ldots, T-1\\).     Hence,     [       P^\\ast\\left[ Y_t = 1 \\mid \\mathcal{F}_t \\right] = P^\\ast\\left[ Y_t = 1 \\right] = P^\\ast\\left[ Y_1 = 1 \\right]     ]</li> <li> <p>Since \\(P^\\ast\\) is a probability measure equivalent to \\(P\\), it follows that</p> \\[   0 &lt; \\frac{r - d}{u - d} &lt; 1 \\] <p>That is, \\(d &lt; r &lt; u\\).</p> </li> <li> <p>The formula for the probability \\(P^\\ast\\) is uniquely determined, and therefore, if the market is arbitrage-free, it has to be complete.</p> </li> </ul> <p>As for the independence of \\(R\\), it follows readily from the definition of \\(P^\\ast\\).</p> <p>Remark</p> <p>Note that the distribution of the pricing measure \\(P^\\ast\\) is entirely given by \\(P^\\ast[R_t = u] = p\\). Indeed, since \\(\\{Y_t = 1\\} =\\{R_t=u\\}\\) as well as \\(\\{Y_t = -1\\} = \\{R_t = d\\}\\) it follows that \\((Y_1, \\ldots, Y_T)\\) are iid. Hence, for \\(\\omega = (\\omega_1, \\ldots, \\omega_T)\\) it holds that</p> \\[     \\begin{align*}       P^\\ast\\left[ \\{\\omega\\} \\right] &amp; = P^\\ast\\left[ Y_1 = \\omega_1, \\ldots, Y_T = \\omega_T \\right]\\\\       &amp; = P^\\ast\\left[ \\cap_{t=1}^T \\{Y_t = \\omega_t\\} \\right]\\\\       &amp; = \\prod_{t=1}^T P^\\ast[Y_t = \\omega_t] &amp; &amp; \\text{Independence of }Y_1, \\ldots, Y_T\\\\       &amp; = \\prod_{t=1}^T P^\\ast[Y_1 = \\omega_t] &amp; &amp; \\text{Identical distribution of }Y_1, \\ldots, Y_T\\\\        &amp; = p^l (1-p)^{T-l}     \\end{align*} \\] <p>where \\(l = \\# \\{t \\colon \\omega_t = 1\\}\\) is the number of \\(1\\) in the sequence of coin tosses \\(\\omega = (\\omega_1, \\ldots, \\omega_T)\\).</p> <p>Since the market is complete, we can therefore:</p> <ul> <li> <p>Price any European contingent claim \\(C\\) uniquely by means of</p> \\[   \\pi(C) = E^\\ast\\left[ \\frac{C}{(1 + r)^T} \\right] \\] </li> <li> <p>Replicate (or hedge) the claim by means of a strategy \\(\\eta\\) such that</p> \\[   \\pi(C) + \\sum_{s=1}^T \\eta_s \\Delta X_s = \\frac{C}{(1 + r)^T} \\] </li> </ul> <p>In particular, given the replicating strategy \\(\\eta\\), we can even provide the (discounted) price at any time by means of</p> \\[     V_t = \\pi(C) + \\sum_{s=1}^t \\eta_s \\Delta X_s = E^\\ast\\left[ \\frac{C}{(1 + r)^T} \\mid \\mathcal{F}_t \\right] \\] <p>Pricing (i.e., computing \\(\\pi(C)\\)) and Hedging (i.e., finding \\(\\eta\\)) are two of three (managing risk is a fundamental component) main activities of financial institutions engaging in derivative trading.</p> <p>Before any further assumptions, as for the price, since we have an explicit expression for \\(P^\\ast\\), it holds that</p> \\[ \\pi(C) = \\sum_{\\omega \\in \\Omega} \\frac{C(\\omega)}{(1+r)^T} P^\\ast[\\{\\omega\\}] = \\sum_{\\omega \\in \\Omega}\\frac{C(\\omega)}{(1+r)^T}p^{l(\\omega)}(1-p)^{T-l(\\omega)} \\] <p>where \\(l(\\omega) = \\# \\{t \\colon \\omega_t = 1\\}\\).</p> <p>Warning</p> <p>This simple expression that can be implemented easily programatically hides however a very important fact. The sum is processed over all possible paths of coin tosses, and the cardinality of which is equal to \\(2^T\\). Even with such a very simple market, if we consider one year maturity by daily trading, it amounts for \\(2^{260}\\) paths which largely outmatch any computational power. Hence, it is not efficiently implementable, and without further assumptions, this curse of dimensionality can only be overcome through Monte-Carlo methods, sampling a large number, though less than \\(2^{260}\\) of paths and approximating the expectation.</p>"},{"location":"lecture/06-Stochastic-Exponential/032-crr-model/#reducing-complexity-vanilla-derivatives","title":"Reducing Complexity: Vanilla Derivatives","text":"<p>Many of the derivatives are plain vanilla European options, that is derivatives which discounted value only depends on the last value of the underlying:</p> \\[ H = \\frac{C}{(1+r)^T} = h(S_T) \\] <p>where \\(h\\colon \\mathbb{R} \\to \\mathbb{R}\\). For instance \\(h(x) = (x-K)^+/(1+r)^T\\) the discounted profile of a call option. However this option can only take \\(T+1\\) values \\(S_0(1+u)^l(1+d)^{T-l}\\) for \\(l = 0, \\ldots, T\\) with each values corresponding to \\(C_T^l=T!/(l!(T-l)!)\\) different possible paths. The computation of the price at time \\(0\\) therefore simplifies to</p> \\[ \\begin{align*}   \\pi(C) &amp; = \\sum_{\\omega \\in \\Omega} H(\\omega)P^\\ast[\\{\\omega\\}]\\\\          &amp; = \\sum_{l=0}^T  h\\left(S_0 (1+u)^l(1+d)^{T-l}\\right) C_T^l p^l (1-p)^{T-l} \\end{align*} \\] <p>which turns the computation of the price from a \\(2^T\\) to a \\(T+1\\) sum.</p>"},{"location":"lecture/06-Stochastic-Exponential/032-crr-model/#dynamic-pricing","title":"Dynamic Pricing","text":"<p>A further particularity of those options, is that the value of the discounted portfolio can also be computed explicitely through a simple backward technique. This relies on the following mathematical result</p> <p>Proposition</p> <p>Let \\(X\\) and \\(Y\\) be two random variable where \\(X\\) is \\(\\mathcal{F}_t\\)-measurable and \\(Y\\) is independent of \\(\\mathcal{F}_t\\). Then for every function \\(h:\\colon \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}\\) it holds (modulo integrability)</p> \\[   E^{P^\\ast}\\left[ h(X, Y) |\\mathcal{F}_t \\right](\\omega) = v(X(\\omega)) \\] <p>where \\(v\\colon \\mathbb{R} \\to \\mathbb{R}\\) is a function defined as</p> \\[   v(x) = E^{P^\\ast}\\left[ h(x, Y) \\right] \\] <p>Remark</p> <p>This proposition basically says that since \\(Y\\) is independent of \\(\\mathcal{F}_t\\) and \\(X\\) is \\(\\mathcal{F}_t\\) measurable, then computing the conditional expectation corresponds to freezing \\(X(\\omega)\\) and computing the expectation. In not so adequate notations, </p> \\[   E^{P^\\ast}\\left[ h(X, Y) | \\mathcal{F}_t \\right](\\omega) = E^{P^\\ast}\\left[ h(x, Y) | x = X(\\omega) \\right] \\] <p>This allows us to state the discrete formulation of partial differential equation type for the portfolio value:</p> <p>Proposition</p> <p>For a european vanilla option \\(H = h(S_T)\\), it holds that that the discounted value of the hedging portfolio is equal to</p> \\[   V_t = v_t(S_t) \\] <p>where \\(v_t \\colon \\mathbb{R} \\to \\mathbb{R}\\) for \\(t=T, \\ldots, 0\\) is recursively defined as</p> \\[   \\begin{equation*}     \\begin{cases}       v_T(x)  = h(S_T)\\\\       \\\\       v_t(x)  = p v_{t+1}(x(1+u)) + (1-p)v_t(x(1+d)) &amp; \\text{for }t = T-1, \\ldots, 0     \\end{cases}   \\end{equation*} \\] <p>Proof</p> <p>By finite inverse induction.</p> <ul> <li>For \\(t=T\\) it holds that \\(V_t = H = h(S_T) = v_T(S_T)\\) by definition.</li> <li> <p>For \\(t=T-1\\), by the martingale property, it holds that \\(V_{T-1} = E^{P^\\ast}[V_T |\\mathcal{F}_{T-1}] = E^{P^\\ast}[v_T(S_T)|\\mathcal{F}_{T-1}]\\).     Now using the proposition above, since \\(S_{T-1}\\) is \\(\\mathcal{F}_{T-1}\\)-measurable and \\(R_T\\) is independent to \\(\\mathcal{F}_{T-1}\\) we get</p> \\[   \\begin{equation*}     V_T  = E^{P^\\ast}\\left[ v_T(S_{T-1}(1+R_T)) |\\mathcal{F}_{T-1} \\right] = v_{T-1}(S_{T-1})    \\end{equation*} \\] <p>for</p> \\[   \\begin{align*}     v_{T-1}(x) &amp; = E^{P^\\ast}\\left[ v_T(x (1+R_T)) \\right]\\\\                &amp; = E^{P^\\ast}\\left[ v_T(x(1+R_1)) \\right] &amp;&amp; R_1, \\ldots, R_T \\text{ are iid}\\\\                &amp; = pv_T(x(1+u)) + (1-p)v_T(x(1+d))   \\end{align*} \\] </li> </ul> <p>The next steps \\(T-2, \\ldots, 0\\) follows the same argumentation.</p>"},{"location":"lecture/06-Stochastic-Exponential/032-crr-model/#dynamic-hedging","title":"Dynamic Hedging","text":"<p>As for the second part of the job, hedging, we know that there exists a predictable strategy \\(\\eta = (\\eta_t)_{t=1, \\ldots, T}\\) that will hedge the claim. Again, in the setting of plain vanilla european option, these ones can be computed backwardly as soon as you computed the sequence of functions \\(v_0, \\ldots, v_T\\).</p> <p>Proposition</p> <p>For a european vanilla option \\(H= h(S_T)\\) the hedging strategy \\(\\eta = (\\eta_t)_{t=1, T}\\) is given by</p> \\[   \\eta_t = \\Delta_t(S_{t-1}) \\] <p>where \\(\\Delta_t \\colon \\mathbb{R} \\to \\mathbb{R}\\) for \\(t=1, \\ldots, T\\) are functions recursively computed as follows</p> \\[     \\displaystyle \\Delta_t(x) = (1+r)^t \\frac{v_t(x(1+u)) - v_t(x(1+d))}{x(1+u) - x(1+d)} \\] <p>Remark</p> <p>The function \\(\\Delta_t\\) is called the delta hedge in finance. The notation is a bit unfortunate in regards to our notation for difference, but is makes sense if you notice that this is the discrete version of the derivative of the portfolio value with respect to the underlying asset \\(\\partial v_t/\\partial S_t\\) coinciding with the Black and Sholes framework.</p> <p>Proof</p> <p>Let us consider a given time \\(1\\leq t\\leq T\\). The hedging strategy \\(\\eta = (\\eta_t)_{t=1, \\ldots, T}\\) is such that</p> \\[     V_{t} - V_{t-1} = \\eta_t (X_t - X_{t-1})  \\] <p>On the one hand, knowing \\(S_0, \\ldots, S_{t-1}\\), since \\(\\eta_t\\) is predictable the right hand side can only take two values</p> \\[   \\begin{align*}     \\eta_t (X_t - X_{t-1}) &amp; = \\displaystyle \\eta_t \\frac{S_{t-1}}{(1+r)^t}\\left( R_t - r  \\right)     \\\\     &amp; =\\displaystyle \\eta_t \\frac{S_{t-1}}{(1+r)^t}       \\begin{cases}         u-r &amp; \\text{if } \\omega_t = 1\\\\         \\\\         d-r &amp; \\text{if } \\omega_{t} = -1       \\end{cases}   \\end{align*} \\] <p>On the other hand, since \\(V_t = v_t(S_t) = v_t(S_{t-1}(1+R_t))\\) as well as \\(V_{t-1} = v_{t-1}(S_{t-1}) = p v_t(S_{t-1}(1+u)) + (1-p) v_t(S_{t-1}(1+d))\\), it follows that the left hand side can only take two values given \\(S_0, \\ldots, S_{t-1}\\).</p> \\[   \\begin{align*}     V_t - V_{t-1} &amp; = \\displaystyle v_t(S_{t-1}(1+R_t)) - p v_t(S_{t-1}(1+u)) - (1-p) v_t(S_{t-1}(1+d))\\\\     \\\\                   &amp; = \\displaystyle     \\begin{cases}      (1-p) \\left(v_t(S_{t-1}(1+u)) - v_t(S_{t-1}(1+d))\\right) &amp; \\text{if }\\omega_t =1\\\\     \\\\      p \\left(v_t(S_{t-1}(1+u)) - v_t(S_{t-1}(1+d))\\right) &amp; \\text{if }\\omega_t =-1\\\\     \\end{cases}   \\end{align*} \\] <p>Puting these two equation together and solving for \\(\\eta_t\\), knowing that \\(p = (r-d)/(u-d)\\) yields that knowing \\(S_0, \\ldots, S_{t-1}\\) we have</p> \\[   \\eta_t = (1+r)^t \\frac{v_t(S_{t-1}(1+u)) - v_t(S_{t-1}(1+d))}{S_{t-1}(1+u) - S_{t-1}(1+d)} = \\Delta_t(S_{t-1}) \\] <p>Remark</p> <p>The derivation of the functions \\(v_t\\) and \\(\\Delta_t\\) for \\(t=0, \\ldots, T\\) can be extended to the general case of options depending on the full path, that is with discounted formulation \\(H = h(S_0, \\ldots, S_T)\\) for some function \\(h:\\mathbb{R}^{T+1} \\to \\mathbb{R}\\) following the same argumentation where</p> \\[     \\begin{equation*}         \\begin{cases}             v_T(x_0, \\ldots, x_{T}) &amp; = h(x_0, \\ldots, x_T)\\\\             \\\\             v_t(x_0, \\ldots, x_t) &amp; = p v_{t+1}(x_0, \\ldots, x_{t}, x_t(1+u)) + (1-p)v_{t+1}(x_0, \\ldots, x_{t}, x_t(1+d))         \\end{cases}     \\end{equation*} \\] <p>as well as</p> \\[     \\Delta_t(x_0, \\ldots, x_{t-1}) = (1+r)^t \\frac{v_{t}(x_0, \\ldots, x_{t-1}, x_{t-1}(1+u)) - v_{t}(x_0, \\ldots, x_{t-1}, x_{t-1}(1+d))}{x_{t-1}(1+u) - x_{t-1}(1+d)} \\] <p>Though those expression are valid and simple to write down mathematically, to solve those numerically hits the problem of the curse of dimensionality, since those depends on every possible combinations of paths for the price \\((x_0, \\ldots, x_t)\\) at each time \\(t\\), which amounts to \\(2^T\\).</p>"},{"location":"lecture/06-Stochastic-Exponential/032-crr-model/#implementation","title":"Implementation","text":"<p>The implementation in a binomial model(1) for plain vanilla options is classical and relatively straightforward after reformulating the problem in terms of matrices. </p> <ol> <li>the BS model turns into resolving pde which unless you have an explicit expression turns into finite difference methods or alike kinds which are similar.</li> </ol> <p>The main computational issue is to derive \\(v_t(x)\\) for every attainable \\(x=S_t\\) and every \\(t\\). To do so as in numerical methods for PDE we consider matrix/vector notations(1) of values</p> <ol> <li>In terms of computer memory and efficiency, this certainly not the best approach. But in that case you don't use python but rather low level programming languages like rust or C and look at structures like Btreemaps and the likes. In python <code>numpy</code> does just fine and the dimension is no longer an issue for modern computers.</li> </ol> <p>We will store the value of the portfolio and the delta hedging into a diagonal matrices \\((T+1)\\times (T+1)\\) and \\(T\\times T\\) with column vectors \\(\\boldsymbol{v}_t\\) in \\(\\mathbb{R}^{T+1}\\) and \\(\\boldsymbol{\\Delta}_t\\) as follows</p> \\[ \\begin{equation*} \\boldsymbol{v}_t =  \\begin{bmatrix}     v_t(S_0(1+u)^t)\\\\     v_t(S_0(1+u)^{t-1}(1+d))\\\\     \\vdots      \\\\     v_t(S_0(1+d)^t)\\\\     0     \\\\     \\vdots     \\\\     0 \\end{bmatrix} \\quad \\text{and}\\quad \\boldsymbol{\\Delta}_t =  \\begin{bmatrix} \\Delta_t(S_0(1+u)^{t-1})\\\\ \\Delta_t(S_0(1+u)^{t-2}(1+d))\\\\ \\vdots \\\\ \\Delta_t(S_0(1+d)^{t-1}) \\\\ 0 \\\\ \\vdots 0 \\end{bmatrix} \\end{equation*} \\] <p>for \\(t=0, \\ldots, T\\).  and we apply the recursion up to \\(0\\). Once \\(\\boldsymbol{v}_t\\) are calculated, and stored, the heding strategy can be computed directly.</p> <pre><code>import numpy as np\n\n# Computation of the discounted portfolio value\n# claim is the function C = f(S_T)\n# not efficient in this form (python is bad with loops) but clear in terms of loops\n# does not verify if -1 &lt; d&lt; r&lt; u!!!\ndef portfolio_value(claim, S0, r, u, d, T):\n    # value of the portfolio\n    v = np.zeros((T+1, T+1))\n    # stock prices tree\n    s = np.zeros((T+1, T+1))\n    # hedging strategy\n    delta = np.zeros((T, T))\n\n    # initialize the stock tree\n    s[0, 0] = S0\n    for t in range(1, T+1):\n        for l in range(t+1):\n            s[t, j] = S0 * (1+u)**(t-l) * (1+d)**l\n\n    # risk neutral probability\n    p = (r - d)/(u - d)\n\n    # initialization of the portfolio value at time T\n    v[T, :] = claim(s[T,:]) / (1+r) ** T\n\n    # Compute backwardly the discounted portfolio value and the hedging\n    for t in range(T-1, -1, -1):\n        for l in range(t+1):\n            vup = v[t+1, l]\n            vdown = v[t+1, l+1]\n            sup = s[t, l] * (1+u)\n            sdown = s[t,l] * (1+d)\n            # delta hedging\n            delta[t, l] = (1+r) ** t * (vup - vdown) / (sup - sdown)\n            # portfolio value\n            v[t,j] = p * vup + (1-p) * vdown\n\n    return {\n        \"Stock\": s,\n        \"portfolio\": v,\n        \"delta\": delta\n    }\n</code></pre>"},{"location":"lecture/06-Stochastic-Exponential/033-exotic-options/","title":"Exotic Options","text":"<p>We saw in the previous simple binomial model that for options that depends on the path, the computational complexity increases exponentially when derivatives can depends on the path. This is for instance the case for exotic options such as asian options and barrer options:</p> \\[ C^{call}_{asian} = \\left( \\frac{1}{T}\\sum_{t=1}^T S_t - K \\right)^+ \\quad \\text{or}\\quad C^{call}_{up\\&amp;out} = \\left( S_T - K \\right)^+ 1_{\\{S_t &lt; B \\colon t=0, \\ldots, T\\}} \\] <p>This usually would require Monte Carlo methods which are less accurate and slower to compute. However, in the case of barrer option, a mathematical result known as the relexion principle will reduce the dimension of the integration from \\(T+1\\) to \\(2\\) which does not depends in complexity on the number of steps and is easy to compute.</p> <p>The key to price barrier options is to know the joint distribution of \\(S_T\\) and the running maximum \\(\\overline{S}_T\\). In order to keep the formulas easy, we make the following assumption:</p> \\[ 1+u=\\frac{1}{1+d} \\] <p>In this case, the price process takes the form:</p> \\[ S_t=S_0\\left( 1+u \\right)^{Z_t} \\] <p>where the stochastic process \\(Z=(Z_t)_{0\\leq t\\leq T}\\) is given by:</p> \\[ Z_0=0, \\quad \\text{and} \\quad Z_t=\\sum_{s=1}^t Y_s \\] <p>which is the so-called random walk. We also consider that \\(P\\) is the uniform distribution, that is:</p> \\[ P\\left[ \\{\\omega\\} \\right]=\\frac{1}{\\# \\Omega} \\] <p>Under this measure, the random variables \\(Y_t\\) are independent, and therefore, it holds:</p> \\[ P[Z_t=k]= \\begin{cases} 2^{-t} C_t^{(t+k)/2} &amp; \\text{if } t+k \\text{ is even}, \\\\\\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\] <p>Denoting by:</p> \\[ \\overline{Z}_t=\\sup_{s\\leq t}Z_s \\] <p>the following reflection principle holds:</p> <p>Proposition: Reflexion Principle</p> <p>For all \\(k\\geq 1\\) and \\(l\\geq 0\\), it holds:</p> \\[   P\\left[ \\overline{Z}_T \\geq k \\text{ and } Z_T=k-l\\right]=P[Z_T=k+l] \\] <p>and</p> \\[   P\\left[ \\overline{Z}_T=k \\text{ and } Z_T=k-l \\right]=\\frac{k+l+1}{T+1}2P[Z_{T+1}=1+k+l] \\] <p>Proof</p> <p>Define \\(\\tau(\\omega)=\\inf\\{t\\colon Z_t(\\omega)=k\\}\\wedge T\\) and define:</p> \\[ f(\\omega)=(\\omega_1,\\ldots, \\omega_{\\tau(\\omega)}, -\\omega_{\\tau(\\omega)+1},\\ldots,-\\omega_T) \\] <p>The trajectories of \\(Z(\\omega)\\) and \\(Z(f(\\omega))\\) coincide up to the moment where \\(Z(\\omega)\\) reaches for the first time the level \\(k\\). From there on, the trajectories are mirrored with respect to the level \\(k\\). Since the random variables \\(Y_t\\) are independent of each other, it follows that \\(f\\) is a bijection between the set:</p> \\[   \\{\\omega \\in \\Omega\\colon \\overline{Z}_T(\\omega)\\geq k \\text{ and } Z_T(\\omega)=k-l\\} \\] <p>and the set:</p> \\[   \\{\\omega \\in \\Omega\\colon \\overline{Z}_T(\\omega)\\geq k \\text{ and } Z_T(\\omega)=k+l\\}=\\{\\omega \\in \\Omega \\colon Z_T=k+l\\} \\] <p>Since \\(P\\) is uniformly distributed, it follows that the first relation of the proposition holds.</p> <p>As for the second relation, if \\(T+k+l\\) is not even, suppose therefore that \\(j=(T+k+l)/2\\) is an integer. Applying the first relation, it holds:</p> \\[   P\\left[ \\overline{Z}_T=k;Z_T=k-l \\right] = P\\left[ Z_T=k+l \\right]-P\\left[ Z_T=k+l+2 \\right] = 2^{-T}C_T^j - 2^{-T} C_{T}^{j+1} \\] <p>which simplifies to:</p> \\[   P\\left[ \\overline{Z}_T=k;Z_T=k-l \\right]=\\frac{k+l+1}{T+1}2P[Z_{T+1}=1+k+l] \\] <p>The rest of the proof follows analogously.</p> <p>We can derive the same kind of formulas if we replace \\(P\\) by the equivalent martingale measure \\(P^\\ast\\). In that case, it holds:</p> \\[ P^\\ast\\left[ Z_t=k \\right]= \\begin{cases} p^{(t+k)/2}(1-p)^{(t-k)/2} C_{t}^{(t+k)/2} &amp; \\text{if } t+k \\text{ is even}, \\\\\\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\] <p>Proposition</p> <p>For all \\(k\\geq 1\\) and \\(l\\geq 0\\), it holds:</p> \\[   P^\\ast\\left[ \\overline{Z}_T \\geq k \\text{ and } Z_T=k-l\\right]=\\left( \\frac{1-p}{p} \\right)^lP^\\ast[Z_T=k+l]=\\left( \\frac{p}{1-p} \\right)^kP^\\ast[Z_T=-k-l] \\] <p>and</p> \\[   P^\\ast\\left[ \\overline{Z}_T=k \\text{ and } Z_T=k-l \\right]=\\frac{1}{p}\\left( \\frac{1-p}{p} \\right)^l\\frac{k+l+1}{T+1}P^\\ast[Z_{T+1}=1+k+l]   =\\frac{1}{1-p}\\left( \\frac{p}{1-p} \\right)^k\\frac{k+l+1}{T+1}P^\\ast[Z_{T+1}=-1-k-l]. \\] Proof <p>First, inspection shows that:</p> \\[ \\frac{dP^\\ast}{dP}=2^T p^{\\frac{Z_T+T}{2}}(1-p)^{\\frac{T-Z_T}{2}}. \\] <p>From the density formula, we get:</p> \\[ P^\\ast\\left[ \\overline{S}_T \\geq k \\text{ and } Z_T=k-l\\right]=2^T p^{(T+k-l)/2}(1-p)^{(T+l-k)/2}P\\left[ \\overline{S}_T \\geq k \\text{ and } Z_T=k-l\\right]. \\] <p>As well as:</p> \\[ P\\left[ Z_T=k+l\\right]=2^T p^{-(T+l+k)/2}(1-p)^{-(T-k-l)/2}P^\\ast[Z_T=k+l]. \\] <p>Which combine with:</p> \\[ P\\left[ \\overline{S}_T \\geq k \\text{ and } Z_T=k-l\\right]=P[Z_T=k+l]. \\] <p>This yields the first relation, the second one being analogous.</p> <p>Example: Up &amp; In Call Option</p> <p>Let:</p> \\[ C^{call}_{up\\&amp;in}= \\begin{cases} (S_T-K)^+ &amp; \\text{if } \\max_{0\\leq t\\leq T}S_t=\\overline{S}_T \\geq B, \\\\\\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\] <p>Where \\(K\\) is the strike and \\(B&gt;S_0\\vee K\\) is a barrier. Note that:</p> \\[ S_t=S_0(1+u)^{Z_t}, \\quad \\text{and} \\quad \\overline{S}_t=S_0(1+u)^{\\overline{Z}_t}. \\] <p>It holds:</p> \\[ \\pi(C)=\\frac{1}{(1+r)^T} E^\\ast\\left[C^{call}_{up\\&amp;in}\\right]. \\] <p>However:</p> \\[ E^\\ast\\left[C^{call}_{up\\&amp;in}\\right]= E^\\ast\\left[ (S_T-K)^+;\\overline{S}_T\\geq B \\right]= E^\\ast\\left[ (S_T-K)^+;S_T\\geq B \\right] + E^\\ast\\left[ (S_T-K)^+;\\overline{S}_T\\geq B;S_T&lt;B \\right]. \\] <p>The first expectation can be entirely computed using the distribution of \\(P^\\ast\\) since it only depends on the distribution of \\(S_T\\). As for the second expectation, without loss of generality, let \\(B=S_0(1+u)^k\\) and using our reflection principle:</p> \\[ E^\\ast\\left[ (S_T-K)^+;\\overline{S}_T\\geq B;S_T&lt;B \\right]= \\sum_{l\\geq 1} E^\\ast\\left[ (S_T-K)^+;\\overline{S}_T\\geq B;Z_T=k-l \\right]. \\] <p>This becomes:</p> \\[   \\begin{align*}     \\sum_{l\\geq 1}\\left( S_0(1+u)^{k-l}-K \\right)^+ P^\\ast\\left[ \\overline{Z}_T \\geq k \\text{ and } Z_T=k-l\\right]     &amp;=\\sum_{l\\geq 1}\\left( S_0(1+u)^{k-l}-K \\right)^+\\left( \\frac{p}{1-p} \\right)^kP^\\ast[Z_T=-k-l]     \\\\     &amp; =\\left( \\frac{p}{1-p} \\right)^k(1+u)^{2k}\\sum_{l\\geq 1}\\left( S_0(1+u)^{-k-l}-\\tilde{K} \\right)^+P^\\ast[Z_T=-k-l]   \\end{align*} \\] <p>where \\(\\tilde{K}=K(1+u)^{-2k}=K(S_0/B)^2\\). Hence, it follows that:</p> \\[   \\pi(C)=\\frac{1}{(1+r)^T} \\left( E^\\ast\\left[ (S_T-K)^+;S_T\\geq B \\right] + \\left( \\frac{p}{1-p} \\right)^k\\left( \\frac{B}{S_0} \\right)^2E^\\ast\\left[ (S_T-\\tilde{K})^+;S_T&lt;B \\right] \\right). \\] <p>Plugging in this distribution yields the explicit formula:</p> \\[ \\pi(C)=\\frac{1}{(1+r)^T} \\left[ \\sum_{n=0}^{n_k}\\left( S_0(1+u)^{T-2n}-K \\right)^+ p^{T-n}(1-p)^n C_{T}^{T-n} + \\left( \\frac{p}{1-p} \\right)^k\\left( \\frac{B}{S_0} \\right)^2 \\sum_{n=n_k+1}^T \\left( S_0(1+u)^{T-2n}-\\tilde{K} \\right)^+p^{T-n}(1-p)^n C_T^{T-n} \\right]. \\]"},{"location":"lecture/06-Stochastic-Exponential/034-american-options/","title":"American Options","text":"<p>So far, even if the outcome of European contingent claims may depend on the sequence of events previous to the maturity, the contract is settled for a given time horizon. In American type of contingent claim, the buyer can claim the payoff at any time before the settlement.</p> <p>Definition</p> <p>An American contingent claim is a non-negative adapted stochastic process:</p> \\[ C=(C_t)_{0\\leq t\\leq T}. \\] <p>The value \\(C_t(\\omega)\\) represents the outcome that a buyer can claim if he/she exercises at time \\(t\\) in state \\(\\omega\\).</p> <p>Example</p> <p>The most classical example of American contingent claims are the American call and put options:</p> \\[ C_t^{call}=\\left( S_t^i-K \\right)^+ \\quad \\text{and} \\quad C_t^{put}=\\left( K-S_t^i \\right)^+, \\] <p>for \\(t=0,\\ldots, T\\).</p> <p>Remark</p> <p>Note that a European contingent claim \\(C\\) can be viewed as a special case of an American one. Indeed, it would correspond to the stochastic process:</p> \\[ C_t= \\begin{cases} 0 &amp; \\text{if } t=0,\\ldots, T-1, \\\\\\\\ C &amp; \\text{if } t=T. \\end{cases} \\]"},{"location":"lecture/06-Stochastic-Exponential/034-american-options/#hedging-capital-for-the-seller","title":"Hedging Capital for the Seller","text":"<p>We first consider the problem of a hedging strategy for the seller of an American contingent claim \\(C\\). Hereby, we denote by \\(H\\) the discounted contingent process, that is:</p> \\[ H_t=\\frac{C_t}{B_t}, \\quad t=0,\\ldots, T. \\] <p>Warning</p> <p>We suppose throughout that the financial market is arbitrage-free and, even more, complete. This means there exists only one pricing measure \\(P^\\ast\\) equivalent to \\(P\\).</p> <p>We want to compute the minimal amount of capital \\(U_t\\) that the seller at time \\(t\\) should have in order to pay the buyer of the contingent claim in case this person exercises their claim. We make this computation backward:</p> <ul> <li> <p>Time \\(T\\):     Suppose that at time \\(T\\), the buyer did not exercise its claim previously.     Then the discounted amount of capital needed to satisfy the buyer is exactly:</p> \\[ U_T=H_T \\] </li> <li> <p>Time \\(T-1\\):     Suppose that we are at time \\(T-1\\). We face two situations.</p> <p>Either the buyer decides to exercise now, and we need at least:</p> \\[ U_{T-1} \\geq H_{T-1} \\] <p>Or it decides to wait another time period, and we have to hedge against the capital we need in the next period \\(U_T\\). Since the market is complete with a value \\(E^{P^\\ast}[U_T|\\mathcal{F}_{T-1}]\\) at time \\(T-1\\) I can find a strategy \\(\\boldsymbol{\\eta}_T\\) which will replicate \\(U_T\\), that is</p> \\[     E^{P^\\ast}[U_T |\\mathcal{F}_{T-1}] + \\boldsymbol{\\eta}_T\\cdot \\Delta \\boldsymbol{X}_T = U_T \\] <p>It follows that the capital required today to hedge this case must be:</p> \\[     U_{T-1} \\geq E^{P^\\ast}\\left[ U_T |\\mathcal{F}_{T-1} \\right] \\] <p>Altogether, this means:</p> \\[ U_{T-1} = \\max\\left\\{ H_{T-1}, E^{\\ast}\\left[ U_T | \\mathcal{F}_{T-1} \\right] \\right\\} = H_{T-1} \\vee E^{P^\\ast}\\left[ U_T | \\mathcal{F}_{T-1} \\right]. \\] </li> <li> <p>Time \\(t \\leq T-1\\):     The same argumentation means we have to reserve at least:</p> \\[     U_t \\geq H_t, \\] <p>as well as the minimum amount of capital \\(U_{t+1}\\) needed from tomorrow in expectation under the pricing measure, that is:</p> \\[ U_t \\geq E^{P^\\ast}\\left[ U_{t+1} | \\mathcal{F}_t \\right]. \\] <p>Altogether, it follows that:</p> \\[ U_t = \\max\\left\\{ H_t, E^{P^\\ast}\\left[ U_{t+1} | \\mathcal{F}_t \\right] \\right\\} = H_t \\vee E^{P^\\ast}\\left[ U_{t+1} | \\mathcal{F}_t \\right]. \\] </li> </ul> <p>This recursive scheme is called the Snell Envelope.</p> <p>Definition: The Snell Enveloppe</p> <p>Let \\(H\\) be an adapted process, \\(P^\\ast\\)-integrable. The Snell Envelope of \\(H\\) is defined inverse recursively as follows:</p> \\[ \\begin{equation*}     \\begin{cases}         U_T =H_T\\\\         \\\\         U_t=H_t \\vee E^{P^\\ast}\\left[ U_{t+1} | \\mathcal{F}_t \\right] &amp;\\text{for } t=T-1,\\ldots, 0.     \\end{cases} \\end{equation*} \\] <p>The Snell envelope satisfies the following inequality:</p> \\[ \\begin{align*}    E^{P^\\ast}\\left[ U_{t+1} | \\mathcal{F}_t \\right] &amp; \\leq  H_t \\vee E^{P^\\ast}\\left[ U_{t+1} | \\mathcal{F}_t \\right] \\\\       &amp; = U_t \\end{align*} \\] <p>Processes satisfying this inequality are called super-martingales:</p> <p>Definition: Super/Sub Martingales</p> <p>A process \\(X\\) is called a \\(P^\\ast\\)-super-martingale if:</p> <ol> <li>\\(X\\) is adapted;</li> <li>\\(X\\) is \\(Q\\)-integrable;</li> <li>\\(E^{P^\\ast}[X_{t+1}|\\mathcal{F}_t] \\leq X_t\\) for every \\(t=0,\\ldots,T-1\\).</li> </ol> <p>A process \\(X\\) is called a \\(Q\\)-sub-martingale if \\(-X\\) is a \\(Q\\)-super-martingale.</p> <p>The Snell envelope is an example of \\(P^\\ast\\)-super-martingale with particular property</p> <p>Proposition</p> <p>Let \\(H\\) be an adapted and \\(P^\\ast\\)-integrable process. The Snell envelope \\(U\\) of \\(H\\) is a \\(P^\\ast\\)-super-martingale and the smallest \\(P^\\ast\\)-super-martingale among those dominating \\(H\\). That is, if \\(V\\) is a \\(P^\\ast\\) super-martingale such that \\(V_t \\geq H_t\\) for all \\(t\\), then \\(V\\geq U\\).</p> <p>Proof</p> <p>From the definition of \\(U\\), it follows immediately that \\(U\\) is a \\(P^\\ast\\)-super-martingale. Let \\(V\\) be another \\(P^\\ast\\)-super-martingale such that \\(V_t \\geq H_t\\) for every \\(t\\). By backward induction we show that \\(V_t \\geq U_t\\).</p> <ul> <li> <p>For \\(t = T\\), by definition we have \\(V_T \\geq H_T = U_T\\)</p> </li> <li> <p>For \\(t = T-1\\), we have</p> <ul> <li>\\(V_{T-1} \\geq H_{T-1}\\) since \\(V\\) dominates \\(H\\)</li> <li>\\(V_{T-1}\\geq E^{P^\\ast}[V_T|\\mathcal{F}_{T-1}] = E^{P^\\ast}[U_T|\\mathcal{F}_{T-1}]\\) since \\(V\\) is a super martingale and \\(V_T \\geq U_T\\).</li> </ul> <p>All together it follows that \\(V_{T-1}\\geq H_{T-1}\\vee E^{P^\\ast}[U_T|\\mathcal{F}_{T-1}] = U_{T-1}\\).</p> </li> </ul> <p>The argumentation follows the same logic for every other step.</p> <p>Example</p> <p>Consider the CRR model where the American contingent claim is path-independent, that is:</p> \\[ H_t = h_t(S_t), \\] <p>for some functions \\(h_t:\\mathbb{R}\\to \\mathbb{R}\\). This is particularly the case for American put and call options since \\(S^0\\) does not depend on the states \\(\\omega \\in \\Omega\\). The American option scheme for the computation of the Snell envelope \\(U_t=u_t(S_t)\\) reads as follows:</p> \\[   u_T(x)=h_T(x), \\quad \\text{and} \\quad u_t(x)= h_t(x) \\vee \\left( u_{t+1}(x(1+u) )p + u_{t+1}(x(1+d))(1-p) \\right), \\] <p>for \\(t=0,\\ldots, T-1\\), where \\(p=(r-d)/(u-d)\\).</p>"},{"location":"lecture/06-Stochastic-Exponential/034-american-options/#execution-time-for-the-buyer","title":"Execution Time for the Buyer","text":"<p>The buyer of an American claim is no longer a passive owner of a contract for which he waits until the end of the period the result of the outcome. He can also strategically decide in any state \\(\\omega\\) to exercise its contract at a time \\(\\tau(\\omega)\\). In general, this stopping strategy is a set of conditions which triggered, yields the exercise of the claim. However the triggering conditions whether to stop or not at time \\(t\\) should only rely on the available information at time \\(t\\).</p> <p>Definition</p> <p>A stopping time is a random variable \\(\\tau:\\Omega \\to \\{0,1,\\ldots, T\\}\\cup\\{\\infty\\}\\) such that \\(\\{\\tau =t\\} = \\{\\omega \\in \\Omega\\colon \\tau(\\omega)=t\\}\\) is in \\(\\mathcal{F}_t\\) for every \\(0\\leq t\\leq T\\).</p> <p>In other terms, \\(\\{ \\tau=t\\}\\) represents the event on which the buyer will decide to exercise its American contingent claim. The event \\(\\{\\tau=\\infty\\}\\) represents the event where the triggering conditions have not been met before the time horizon, and therefore the buyer will exercise it at time \\(T\\).</p> <p>Remark</p> <p>Note that the following facts hold true (see math suplement on processes):</p> <ul> <li>Deterministic times are stopping times:* The constant random variable \\(\\tau(\\omega)=t\\) for every \\(\\omega\\) and a given \\(t\\) is a stopping time;</li> <li>A random variable \\(\\tau:\\Omega \\to \\{0,1,\\ldots,T\\}\\) is a stopping time if and only if \\(\\{\\tau\\leq t\\}\\) is in \\(\\mathcal{F}_t\\) for every \\(t=0,\\ldots, T\\).</li> <li>If \\(\\tau\\) is a stopping time, then \\(\\{t\\leq \\tau\\} = \\{\\tau \\leq t-1\\}^c\\) is in \\(\\mathcal{F}_{t-1}\\) for every \\(t=1,\\ldots, T\\);</li> <li>If \\(\\sigma\\) and \\(\\tau\\) are two stopping times, then so are \\(\\sigma \\vee \\tau\\), \\(\\tau\\wedge \\sigma\\). In particular \\(\\tau \\wedge t\\).</li> </ul> <p>Example</p> <p>Let \\(S\\) be an adapted process, for instance the price evolution of an asset. Given a threshold \\(c\\), the random variable</p> \\[     \\tau(\\omega)=\\inf\\left\\{ t\\colon S_t(\\omega)\\geq c \\right\\} \\] <p>is a stopping time.</p> Proof <p>It holds that</p> \\[   \\begin{align*}     \\{\\tau \\leq t\\} &amp; = \\left\\{\\omega \\in \\Omega\\colon S_s(\\omega) \\geq c \\text{ for some }0 \\leq s\\leq t\\right\\}\\\\                     &amp; = \\underbrace{\\cup_{s=0}^t \\underbrace{\\{S_s \\geq t\\}}_{\\in \\mathcal{F}_s \\subseteq \\mathcal{F}_t}}_{\\in \\mathcal{F}_t}   \\end{align*} \\] <p>Definition</p> <p>Let \\(S\\) be a stochastic process and \\(\\tau\\) a stopping time. We denote by \\(S^\\tau\\) the stopped process</p> \\[     \\begin{equation*}       S_t^\\tau(\\omega)=S_{t\\wedge \\tau(\\omega)}(\\omega) =       \\begin{cases}         S_t(\\omega) &amp;\\text{if }t&lt; \\tau(\\omega)\\\\         S_{\\tau(\\omega)}(\\omega) &amp; \\text{if }\\tau(\\omega)\\leq t       \\end{cases}     \\end{equation*} \\] <p> </p> <p>Theorem: Doob's Optional Sampling</p> <p>Let \\(M\\) be an adapted process and \\(Q\\) a probability measure such that \\(M_t\\) is \\(Q\\)-integrable for all \\(t=0,\\ldots,T\\). The following assertions are equivalent:</p> <ol> <li>\\(M\\) is a \\(Q\\)-martingale;</li> <li>For every stopping time \\(\\tau\\), the process \\(M^\\tau\\) is a martingale;</li> </ol> Proof <p>As for 1. implies 2, we already know that if \\(M\\) is a martingale and \\(\\eta = (\\eta_t)_{t=1, \\ldots, T}\\) is a predictable process, then the portfolio</p> \\[   V_t = V_0 + \\sum_{s=1}^t \\eta_s (M_s - M_{s-1}) \\] <p>is a martingale. Taking the portfolio \\(V_0 = M_0\\) and the strategy</p> \\[   \\begin{equation*}    \\eta_t = 1_{\\{t\\leq \\tau\\}} = \\begin{cases}     1 &amp; \\text{if }t\\leq \\tau\\\\     0 &amp; \\text{if }t&gt;\\tau   \\end{cases}   \\end{equation*} \\] <p>where you buy and hold \\(M\\) until time \\(\\tau\\), it follows that \\(\\eta\\) is predictable since \\(\\{t\\leq \\tau\\}=\\{\\tau \\leq t-1\\}^c\\) is in \\(\\mathcal{F}_{t-1}\\). Furthermore, by definition </p> \\[   \\begin{equation*}     V_t = \\begin{cases}       M_t &amp; \\text{if }t&lt; \\tau\\\\       M_\\tau &amp; \\text{if }\\tau \\leq t     \\end{cases}     = M_{t\\wedge \\tau} = M^\\tau_t   \\end{equation*} \\] <p>showing that \\(M^\\tau\\) is a martingale.</p> <p>As for 2. implies 1., it is immediate by considering the stopping time \\(\\tau = T\\).</p> <p>Remark</p> <p>Note that if \\(\\eta\\) is a positive predictable process and \\(M\\) is a super-/sub-martingale, then it is easy to show that the portfolio \\(V_t = V_0 + \\sum_{s=1}^t\\eta_s (M_s - M_{s-1})\\) is a super-/sub-martingale. It follows from the proof that Doob's optional sampling theorem also holds for super-/sub-martingales.</p> <p>Concerning our buyer of an American option. He will choose its exercise strategy among the following set</p> \\[     \\mathcal{T}=\\left\\{ \\tau\\colon \\tau \\text{ stopping time with } \\tau \\leq T \\right\\} \\] <p>Suppose now that the goal of the buyer is to maximize the expectation of its outcome under the pricing measure \\(P^\\ast\\), he therefore faces the following optimization problem</p> \\[     \\max\\left\\{ E^{\\ast}\\left[ H_{\\tau} \\right]\\colon \\tau \\in \\mathcal{T}\\right\\} \\] <p>Its goal is therefore to find an optimal stopping time \\(\\tau^\\ast\\) such that</p> \\[ E^\\ast\\left[ H_{\\tau^\\ast}  \\right] \\geq E^{\\ast}\\left[ H_{\\tau} \\right] \\] <p>for any other stopping strategy \\(\\tau\\).</p> <p>Remark</p> <p>Note that such a problem even if intuitive is highly non trivial. Indeed, the maximization is done among the set of possible stopping times which is usually infinite dimensional and difficult to describe or parametrize. Secondly, the optimization function \\(\\tau \\mapsto E^{\\ast}[H_{\\tau}]\\) is nothing close to be something like convex or even smooth (smooth in which sense). The classical optimization theory here do not apply at all.</p> <p>We start by showing that the optimization problem from the buyer's side is always smaller than the minimum hedging capital for the seller.</p> <p>Proposition</p> <p>For any stopping time \\(\\tau\\) in \\(\\mathcal{T}\\), it holds</p> \\[     U_0 \\geq E^\\ast\\left[ H_{\\tau} \\right] \\] <p>In particular,</p> \\[     \\underbrace{U_0}_{\\text{Seller's minimum hedging capital}} \\geq \\underbrace{\\sup \\left\\{ E^\\ast\\left[ H_\\tau \\right] \\colon \\tau \\in \\mathcal{T} \\right\\}}_{\\text{Buyer's maximal expected revenue}} \\] <p>Proof</p> <p>Let \\(\\tau\\) in \\(\\mathcal{T}\\) be any stopping time strategy for the buyer. We know that the Snell envelope is a super-martingale such that \\(U_t\\geq H_t\\) for every \\(t\\). In particular, on the one hand, it holds that \\(U_\\tau \\geq H_\\tau\\) and therefore \\(E^\\ast[U_\\tau] \\geq E^\\ast[H_\\tau]\\). On the other hand, \\(U^\\tau\\) is also a super-martingale due to Doob's optional sampling theorem. Hence \\(U_0 \\geq E^\\ast\\left[ U_T^\\tau \\right] = E^\\ast\\left[ U_{T\\wedge \\tau} \\right]=E^\\ast[U_\\tau]\\). Both together we deduce that</p> \\[     U_0 \\geq E^\\ast\\left[ H_\\tau \\right] \\] <p>This proposition shows that whatever stopping strategy the buyer is choosing, it will always reach less in (risk-neutral) expectation than what the seller is asking as minimum capital to hedge its contingent claim. If the buyer reaches equality, then it is an optimal strategy. Therefore, the following definition.</p> <p>Definition</p> <p>A stopping time \\(\\tau^\\ast\\) in \\(\\mathcal{T}\\) is called optimal if \\(U_0=E^\\ast\\left[ H_{\\tau^\\ast} \\right]\\).</p> <p>The question is whether, at least one, optimal stopping time is available to the buyer. Before addressing this question, we can obtain a better characterization of optimal stopping times.</p> <p>Proposition</p> <p>A stopping time \\(\\tau^\\ast\\) is optimal if and only if the two following conditions are satisfied</p> <ol> <li>\\(U_{\\tau^\\ast}=H_{\\tau^\\ast}\\);</li> <li>\\(U^{\\tau^\\ast}\\) is a \\(P^\\ast\\)-martingale.</li> </ol> <p>Proof</p> <p>Suppose that a stopping time \\(\\tau^\\ast\\) is such that:</p> <ul> <li>\\(U_{\\tau^\\ast}=H_{\\tau^\\ast}\\)</li> <li>\\(U^{\\tau^\\ast}\\) is a \\(P^\\ast\\)-martingale</li> </ul> <p>From the second point and Doob's optional sampling theorem, we deduce that</p> \\[     U_0 = E^\\ast\\left[ U^\\tau_T \\right] = E^\\ast\\left[ U_{\\tau^\\ast} \\right] \\] <p>And from the first, we get that \\(E^\\ast[U_{\\tau^\\ast}] = E^\\ast[H_{\\tau^\\ast}]\\). Hence \\(U_0 = E^\\ast[H_{\\tau^\\ast}]\\), showing that \\(\\tau^\\ast\\) is an optimal stopping time.</p> <p>Reciprocally, suppose that \\(\\tau^\\ast\\) is an optimal stopping time. Since \\(U\\) is a Snell envelope, and \\(\\tau^\\ast\\) is an optimal stopping time (last equality), we know that</p> \\[     U_0 \\geq E^\\ast\\left[ U_{\\tau^\\ast} \\right] \\geq E^\\ast\\left[ H_{\\tau^\\ast} \\right] = U_0 \\] <p>and \\(U_{\\tau^\\ast}\\geq H_{\\tau^\\ast}\\). If \\(U^{\\tau^\\ast}\\) were a strict martingale, then the first inequality would be strict. If \\(U_{\\tau^\\ast}\\) were strictly greater than \\(H_{\\tau^\\ast}\\) on a set of positive probability, the second inequality would be strict. Since both sides of the inequalities are equal to \\(U_0\\), we conclude that \\(U^{\\tau^\\ast}\\) is a martingale and \\(U_{\\tau^\\ast} = H_{\\tau^\\ast}\\).</p> <p>This mathematical characterization of optimal stopping times gives us a hint as how to derive an optimal stopping strategy.</p> <p>Given the Snell envelope \\(U\\) of \\(H\\) under \\(P^\\ast\\) with Doob-Meyer decomposition \\(U = M + A\\) where \\(M\\) is a martingale and \\(A\\) is a predictable decreasing process starting at \\(0\\). On the one hand, according to the first condition we define </p> \\[     \\tau_{\\min}=\\inf\\left\\{ t\\colon U_t=H_t \\right\\} \\] <p>which is a member of \\(\\mathcal{T}\\). Per definition, it is the smallest stopping time such that \\(U_{\\tau_{\\min}}=H_{\\tau_{\\min}}\\).</p> <p>On the other hand, according to the first condition we define</p> \\[     \\tau_{\\max}=\\inf \\left\\{ t\\colon E^{\\ast}\\left[ U_{t+1}-U_t |\\mathcal{F}_t \\right]\\neq 0 \\right\\}\\wedge T=\\inf\\left\\{ t\\colon A_{t+1}\\neq 0 \\right\\}\\wedge T \\] <p>which is also a stopping time in \\(\\mathcal{T}\\). By definition is this the largest stopping time for which \\(U^\\tau\\) is still a martingale before it starts to be a strict super martingale.</p> <p>According to the previous proposition, if the buyer is choosing a stopping time that might stop earlier than \\(\\tau_{\\min}\\) or stop after \\(\\tau_{\\max}\\) then is has to be sub-optimal.</p> <p>In other terms, any optimal stopping time \\(\\tau^\\ast\\) must satisfy</p> \\[ \\tau_{\\min} \\leq \\tau^\\ast \\leq \\tau_{\\max} \\] <p>The following proposition shows that eventually \\(\\tau_{\\min}\\) as well as \\(\\tau_{\\max}\\), which are explicit, are both optimal stopping times</p> <p>Proposition</p> <p>The stopping times \\(\\tau_{\\min}\\) and \\(\\tau_{\\max}\\) are optimal stopping times. Furthermore, any other optimal stopping time \\(\\tau^\\ast\\) must satisfy \\(\\tau_{\\min} \\leq \\tau^\\ast \\leq \\tau_{\\max}\\).</p> Proof <p>We first show that \\(\\tau_{\\min}\\) is a optimal stopping time. By the characterization, since by definition \\(U_{\\tau_{\\min}} = H_{\\tau_\\min}\\) we just need to show that \\(U^{\\tau_\\min}\\) is a martingale. By Doob's optional sampling theorem, the stopped process \\(U^{\\tau_{\\min}}_t=U_{t\\wedge \\tau_{\\min}}\\) is a \\(P^\\ast\\)-super-martingale. For a fixed \\(0\\leq t\\leq T-1\\), define \\(A=\\{t&lt;\\tau_{\\min}\\}\\). For \\(\\omega\\in A\\), it holds that \\(t&lt;\\tau_{\\min}(\\omega)\\) and therefore \\(U_t(\\omega)&gt;H_t(\\omega)\\). Hence,</p> \\[     U_t^{\\tau_{\\min}}(\\omega)=U_{t}(\\omega)=H_t(\\omega)\\vee E^{\\ast}\\left[ U_{t+1}|\\mathcal{F}_t \\right](\\omega)=E^{\\ast}\\left[ U_{t+1}|\\mathcal{F}_t \\right](\\omega)     =E^{\\ast}\\left[ U_{t+1}^{\\tau_{\\min}}|\\mathcal{F}_t \\right](\\omega) \\] <p>But for \\(\\omega \\in A^c\\), it holds that \\(t\\wedge \\tau_{\\min}(\\omega)=(t+1)\\wedge\\tau_{\\min}(\\omega)=\\tau_{\\min}(\\omega)\\) and therefore</p> \\[      U_t^{\\tau_{\\min}}(\\omega)=E^{\\ast}[U_{t+1}^{\\tau_{\\min}}|\\mathcal{F}_t](\\omega) \\] <p>All together with the fact that \\(A \\in \\mathcal{F}_t\\), it follows that</p> \\[     U_{t}^{\\tau_{\\min}}     =1_AU_{t}^{\\tau_{\\min}}+1_{A^c}U_{t}^{\\tau_{\\min}}     =1_A E^{\\ast}\\left[ U_{t+1}^{\\tau_{\\min}}|\\mathcal{F}_t \\right]+1_{A^c}E^{\\ast}\\left[ U_{t+1}^{\\tau_{\\min}}|\\mathcal{F}_t \\right]     =E^{\\ast}\\left[ U_{t+1}^{\\tau_{\\min}}|\\mathcal{F}_t \\right] \\] <p>showing that \\(U^{\\tau_{\\min}}\\) is a \\(P^\\ast\\)-martingale.</p> <p>We now show that \\(\\tau_{\\max}\\) is an optimal stopping time. By the characterization, since by definition \\(U^{\\tau_\\max}\\) is a martingale, we just need to show that \\(U_{\\tau_\\max} = H_{\\tau_\\max}\\). Let \\(U = M + A\\) be the Doob-Meyer decomposition of \\(U\\) into a martingale \\(M\\) and a predictable decreasing process \\(A\\) starting at \\(0\\). For \\(\\omega \\in \\{\\tau_{\\max}=T\\}\\) it is clear. For \\(\\omega \\in \\{\\tau_{\\max}=t\\}\\) for \\(t&lt;T\\), it follows that \\(A_t(\\omega)=0\\) and \\(A_{t+1}(\\omega)&gt;0\\). Hence,</p> \\[     E^{\\ast}\\left[ U_{t+1}-U_t|\\mathcal{F}_t \\right](\\omega)=-\\left( A_{t+1}(\\omega)-A_t(\\omega) \\right)&lt;0 \\] <p>showing that \\(U_{t}(\\omega)&gt;E^{\\ast}[U_{t+1}|\\mathcal{F}_t](\\omega)\\) and by the definition of the Snell envelope it follows that \\(U_t(\\omega)=H_t(\\omega)\\vee E^{\\ast}[U_{t+1}|\\mathcal{F}_t](\\omega) = H_t(\\omega)\\). Hence \\(H_{\\tau_{\\max}}=U_{\\tau_{\\max}}\\).</p>"},{"location":"lecture/06-Stochastic-Exponential/035-ruin-probability/","title":"Default (Ruin) Probability","text":"<p>We saw that the random walk often generates the evolution of the stock prices in the CRR model. In this section, we are interested in finding the probability of the ruin event. During the study of American Options, we introduced the concept of stopping times. Stopping times are also used to define ruin events, and the martingales with the help of Doob's Optional Sampling Theorem will provide a very elegant way to study the properties of this ruin event.</p> <p>We consider the following random walk of the CRR model starting at \\(0\\):</p> \\[     Z_0=0\\quad \\text{and}\\quad Z_t=\\sum_{s=1}^t Y_s,\\quad t\\geq 1 \\] <p>where \\(Y\\) are i.i.d. with \\(P[Y_t = 1] = p\\) and \\(P[Y_t = -1] = 1 - p = q\\).</p> <p>As for the filtration, we take</p> \\[     \\mathcal{F}_0=\\{\\emptyset,\\Omega\\}\\quad \\text{and}\\quad \\mathcal{F}_t=\\sigma(Z_s\\colon 1\\leq s\\leq t),\\quad t\\geq 1 \\] <p>We define \\(\\tau_{a}=\\inf\\left\\{ t\\colon Z_t=a \\right\\}\\), \\(\\tau_{b}=\\inf\\left\\{ t\\colon Z_t =-b \\right\\}\\), and \\(\\tau=\\tau_a\\wedge \\tau_b =\\inf\\{t\\colon Z_t=a\\text{ or }Z_t=-b\\}\\) for two integers \\(a,b\\).</p> <p>Remark</p> <p>As we saw in the CRR model, we often describe the stock price as:</p> \\[     S_t=S_0(1+u)^{Z_t} \\] <p>so that the stopping time \\(\\tau\\) can be interpreted as the first time that the stock price reaches the level \\(S_0(1+u)^a\\) or drops to the level \\(S_0/(1+u)^{b}\\).</p> <p>We are interested in the following probabilities:</p> \\[     P\\left[ Z_{\\tau}=a \\right], \\quad \\text{and}\\quad P \\left[ Z_{\\tau}=-b \\right] \\] <p>which are, respectively, the probability that \\(Z\\) reaches the level \\(a\\) before running bankrupt at level \\(-b\\), and the probability of running bankrupt at level \\(-b\\) before reaching level \\(a\\).</p>"},{"location":"lecture/06-Stochastic-Exponential/035-ruin-probability/#symmetric-random-walk","title":"Symmetric Random Walk","text":"<p>Suppose first that \\(p=q=1/2\\), the case of the symmetric random walk. It follows that \\(Z\\) is a martingale. Hence, due to the Optional Sampling Theorem, the stopped process \\(Z^\\tau\\) is a martingale, showing that:</p> \\[     0=Z_0=E\\left[ Z_t^\\tau \\right]=E\\left[ Z_{t\\wedge \\tau} \\right] \\] <p>for every \\(t\\). An intuitive inspection shows that for almost all \\(\\omega\\), the hitting time of one of the barriers will occur in a finite amount of time almost surely, that is \\(P[\\tau &lt;\\infty]=1\\). Hence,</p> \\[     \\lim_{t\\to \\infty}Z_{t\\wedge \\tau}=Z_\\tau \\] <p>\\(P\\)-almost surely. Furthermore, \\(|Z^\\tau_t|\\leq a\\wedge b\\) for every \\(t\\), and therefore, by Lebesgue's Dominated Convergence Theorem, it follows that:</p> \\[     0=\\lim_{t\\to \\infty}E\\left[ Z_t^\\tau \\right]=E\\left[ \\lim_{t\\to \\infty} Z_{t\\wedge \\tau} \\right]=E[Z_{\\tau}] \\] <p>On the other hand, it holds:</p> \\[     Z_{\\tau}=a1_{\\{Z_\\tau =a\\}}-b1_{\\{Z_\\tau=-b\\}}=a1_{\\{Z_\\tau=a\\}}-b(1-1_{\\{Z_\\tau=a\\}}) \\] <p>showing that:</p> \\[     E\\left[ Z_\\tau \\right]=(a+b)P\\left[ Z_\\tau=a \\right]-b \\] <p>Together with the fact that \\(E[Z_\\tau]=0\\), it follows that:</p> \\[     P\\left[ Z_\\tau=a \\right]=\\frac{b}{a+b}, \\quad \\text{and}\\quad P\\left[ Z_\\tau=-b \\right]=\\frac{a}{a+b} \\]"},{"location":"lecture/06-Stochastic-Exponential/035-ruin-probability/#asymmetric-random-walk","title":"Asymmetric Random Walk","text":"<p>Suppose now that \\(1&gt;p&gt;1/2\\). Then, \\(Z\\) is no longer a martingale, but a sub-martingale. Indeed, \\(Y\\) being independent and identically distributed, it follows that:</p> \\[     E\\left[ Z_{t+1}-Z_t|\\mathcal{F}_t \\right]=E\\left[ Y_{t+1}|\\mathcal{F}_t \\right]=E\\left[ Y_{1} \\right]= P\\left[ Y_1=1 \\right]-P\\left[ Y_1=-1 \\right]=2p-1&gt;0 \\] <p>We can therefore no longer apply the same strategy as before. However, a similar inspection as in the previous case shows that \\(\\tau(\\omega)&lt;\\infty\\) for \\(P\\)-almost all \\(\\omega \\in \\Omega\\). Despite the fact that \\(Z\\) is no longer a martingale, the process:</p> \\[     M_t=\\left(\\frac{q}{p}\\right)^{Z_t}, \\quad t=0,1,\\ldots \\] <p>is a martingale where \\(q = 1-p\\). Indeed, it is clearly adapted and integrable. Furthermore, using the independent and identically distributed assumption for \\(Y\\), it holds:</p> \\[     E\\left[ M_{t+1}-M_t|\\mathcal{F}_t \\right]     =M_tE\\left[ \\left( \\frac{q}{p} \\right)^{Y_{t+1}}-1|\\mathcal{F}_t \\right]     =M_tE\\left[ \\left( \\frac{q}{p} \\right)^{Y_{1}}-1 \\right]     =M_t\\left( p\\frac{q}{p}+q\\frac{p}{q}-1 \\right)=0 \\] <p>Considering the stopped process \\(M^\\tau\\), it follows from Doob's Optional Sampling Theorem that:</p> \\[     1=M_0=E\\left[M_{t}^\\tau  \\right]=E\\left[ M_{t\\wedge \\tau} \\right] \\] <p>for every \\(t\\). However, since \\(\\tau(\\omega)&lt;\\infty\\) for every \\(\\omega\\), it follows that \\(\\lim_{t\\to \\infty}M_{t\\wedge \\tau}=M_{\\tau}\\) \\(P\\)-almost surely. Furthermore \\(|M_{t\\wedge \\tau}|\\leq 1\\), allowing us to apply Lebesgue's Dominated Convergence:</p> \\[     1=\\lim_{t\\to \\infty}E\\left[ M_{t\\wedge \\tau} \\right]=E\\left[ \\lim_{t\\to \\infty}M_{t\\wedge \\tau} \\right]=E\\left[ M_\\tau \\right] \\] <p>On the other hand, it holds:</p> \\[     E\\left[ M_\\tau \\right]     =\\left(\\frac{q}{p}\\right)^aP\\left[ Z_\\tau=a \\right]+\\left( \\frac{q}{p} \\right)^{-b} P\\left[ Z_\\tau=-b \\right]     =\\left(\\frac{q}{p}\\right)^{-b}\\left(\\left(\\frac{q}{p}\\right)^{a+b}P[Z_\\tau=a]+(1-P[ Z_\\tau=a]) \\right) \\] <p>Combined with \\(E[M_\\tau]=1\\), it yields:</p> \\[     P\\left[ Z_\\tau=a \\right]=\\frac{(q/p)^b-1}{(q/p)^{a+b}-1}, \\quad \\text{and}\\quad P\\left[ Z_{\\tau}=-b \\right]=\\frac{(q/p)^{a}-1}{(q/p)^{a}-(q/p)^{-b}} \\]"},{"location":"lecture/06-Stochastic-Exponential/035-ruin-probability/#application-to-credit-rating","title":"Application to Credit Rating","text":"<p>Given a company, we want to estimate its probability to default at some point. In our simple framework, we define:</p> \\[     \\inf\\left\\{ t: b+Z_t =0 \\right\\}=\\inf\\left\\{ t \\colon Z_t = -b \\right\\}=\\tau_b \\] <p>which is the first time that a firm with cumulative returns \\(Z\\) and start capital \\(b\\) reaches bankruptcy. We also assume here that \\(1/2 &lt;p&lt;1\\). We want to estimate:</p> \\[     P\\left[ \\tau_b &lt;\\infty \\right] \\] <p>which is the probability that the firm goes bankrupt at some point in time. Since \\(\\tau_a \\to \\infty\\) as \\(a\\to \\infty\\), it follows that:</p> \\[     P\\left[ \\tau_b&lt;\\infty \\right]=\\lim_{a \\to \\infty}P\\left[ Z_{\\tau}=-b \\right]=\\lim_{a\\to \\infty}\\frac{(q/p)^{a}-1}{(q/p)^{a}-(q/p)^{-b}}=\\left( \\frac{q}{p} \\right)^b \\] <p>We see therefore two factors impacting the probability of default of the company:</p> <ul> <li>The returns of the company (\\(p\\)): Since \\(E[Z_{t+1}-Z_t|\\mathcal{F}_t]=2p-1&gt;0\\), if \\(p\\) is very close to \\(1\\), that is strong returns, then \\(q/p\\) is very small and so is the probability of default \\((q/p)^b\\).</li> <li>The present capital of the company (\\(b\\)): Since \\(0&lt;q/p&lt;1\\), it follows that the higher \\(b\\), the smaller \\((q/p)^b\\) in an exponential way.</li> </ul>"},{"location":"lecture/07-SDE/021-what-is-risk/","title":"What is Risk","text":"<p>Even if the notion of risk is colloquial and everyone intuitively understands it, it is far from clear what it is the exact definition.</p> <p>We saw in the previous chapter how to price contingent claims in a \"risk-neutral way\" ensured by an arbitrage-free financial market. However, such pricing does not tell us much about the amount of \"risk\" one undertakes when investing in one product or another.</p> <p>Let us consider the following example.</p> <p>Example</p> <p>Let \\(\\Omega=\\{\\omega_1,\\omega_2,\\omega_3\\}\\), \\(\\mathcal{F}=2^\\Omega\\), and the \"objective probability\" measure \\(P\\) given by \\(P[\\{\\omega_1\\}]=0.1\\), \\(P[\\{\\omega_2\\}]=0.85\\), and \\(P[\\{\\omega_3\\}]=0.05\\). Our bank account \\(B_0=1\\) and \\(B_1=(1+r)\\). We have two stocks with the same start price \\(S_0^1=S_0^2=100\\) and prices tomorrow:</p> \\[ S_1^1(\\omega)= \\begin{cases}     110 &amp; \\text{if } \\omega = \\omega_1 \\\\     105 &amp; \\text{if } \\omega = \\omega_2 \\\\     100 &amp; \\text{if } \\omega = \\omega_3 \\end{cases} \\quad \\text{and} \\quad S_1^2(\\omega)= \\begin{cases}     160 &amp; \\text{if } \\omega = \\omega_1 \\\\     110 &amp; \\text{if } \\omega = \\omega_2 \\\\     0   &amp; \\text{if } \\omega = \\omega_3 \\end{cases} \\] <p>Simple computation shows that for \\(r=\\frac{1}{15} \\approx 6.66\\%\\), there exists a unique risk-neutral pricing measure \\(P^\\ast\\) given by:</p> \\[ P^\\ast[\\{\\omega_1\\}] = p_1^\\ast = \\frac{2}{3}, \\quad P^\\ast[\\{\\omega_3\\}] = p_3^\\ast = \\frac{1}{3}, \\quad \\text{and} \\quad P^\\ast[\\{\\omega_2\\}] = p_2^\\ast = 0 \\] <p>Now, as a portfolio manager, you face the dilemma of which stock you would choose or what proportion you would allocate to one or the other. If the only rationale underlying your decision process is given in terms of the risk-neutral pricing, there is no difference between the two stocks, and you are indifferent.</p> <p>However, you intuitively see that the first stock is a blue chip, whereas the second one is rather the hot but \"risky\" kid on the playground\u2014a startup or so. Your decision process would likely be driven by a \"risk/reward\" analysis in the face of \"uncertainty,\" whatever that means.</p> <p>We make the analysis even simpler with the following second example.</p> <p>Example</p> <p>You own 1,000 RMB and have the choice between the following games:</p> <ol> <li> <p>Pay 1,000 and get immediately:</p> \\[ \\begin{cases}     2,000 &amp; \\text{with probability } 50\\% \\\\     0     &amp; \\text{otherwise} \\end{cases} \\] </li> <li> <p>Pay 1,000 and get immediately:</p> \\[  \\begin{cases}      1,200 &amp; \\text{with probability } \\frac{5}{6} \\approx 83.33\\% \\\\      0     &amp; \\text{otherwise}  \\end{cases} \\] </li> <li> <p>Pay 1,000 and get immediately:</p> \\[  \\begin{cases}      1,300 &amp; \\text{with probability } 25\\% \\\\      900   &amp; \\text{otherwise}  \\end{cases} \\] </li> <li> <p>Pay 1,000 and get immediately:</p> \\[  \\begin{cases}      1,100 &amp; \\text{with probability } 50\\% \\\\      900   &amp; \\text{otherwise}  \\end{cases} \\] </li> <li> <p>Do nothing and keep your 1,000.</p> </li> </ol> <p>All these games have an expected return of 0. However, you would likely have a preference regarding which one is the best. Considering their standard deviations\u2014that is, \\(E[(X - E[X])^2]^{1/2}\\)\u2014it holds:</p> \\[   \\begin{aligned}       \\mathrm{STD}(\\text{game 1}) &amp; \\approx 1,000, \\\\       \\mathrm{STD}(\\text{game 2}) &amp; \\approx 447.21, \\\\       \\mathrm{STD}(\\text{game 3}) &amp; \\approx 173.21, \\\\       \\mathrm{STD}(\\text{game 4}) &amp; \\approx 100, \\\\       \\mathrm{STD}(\\text{game 5}) &amp; \\approx 0.   \\end{aligned} \\] <p>Remark</p> <p>Risk perception is a subjective view of how you assess uncertain prospective outcomes. This may differ from one person to another as well as from one context to another. How can we model this fact mathematically?</p>"},{"location":"lecture/07-SDE/021-what-is-risk/#two-examples-for-risk-assessment-instruments","title":"Two Examples for Risk Assessment Instruments","text":""},{"location":"lecture/07-SDE/021-what-is-risk/#markowitz-mean-variance","title":"Markowitz Mean Variance","text":"<p>The deviation from the mean appears to be a good indicator of our aversion to uncertainty. This is why Markowitz introduced the following criterion to assess the trade-off between risk and rewards in terms of variance and means.</p> <p>Definition</p> <p>Given a square integrable random variable \\( X \\)\u2014modeling some payoff such as a portfolio strategy, industrial projects, or any management decision\u2014the Markowitz mean/variance measure is defined as:</p> \\[ MV_{\\alpha}(X) = E[X] - \\frac{\\alpha}{2} \\text{VAR}(X) \\] <p>where:</p> \\[ \\text{VAR}(X) = E\\left[(X - E[X])^2\\right] \\] <p>is the variance of the random variable, and \\( \\alpha \\) is a positive number.</p> <p>For any value of \\( \\alpha \\), you can check that assessing previous games in terms of mean and variance will rank them, with the largest standard deviation corresponding to the worst game and the smallest standard deviation corresponding to the best game. </p> <p>The Markowitz mean-variance approach was a highly successful instrument for finding optimal portfolio strategies. It can also be used as a risk assessment measure. However, since we are more interested in the downside risks, we consider risk measures defined for the random variable \\( L = -X \\), where \\( X \\) represents returns.</p> <p>Definition</p> <p>The Markowitz risk measure is defined as:</p> \\[   RMV_{\\alpha}(L) = E[L] + \\frac{\\alpha}{2} \\text{VAR}(L) \\] <p>where \\( L \\) is a square integrable loss profile.</p> <p>Proposition</p> <p>The Markowitz risk measure satisfies the following properties:</p> <ol> <li> <p>Cash-invariance: For every loss profile \\( L \\) and \\( m \\in \\mathbb{R} \\),</p> \\[   RMV_{\\alpha}(L - m) = RMV_{\\alpha}(L) - m. \\] </li> <li> <p>Convexity: For any two loss profiles \\( L_1, L_2 \\) and \\( \\lambda \\in [0, 1] \\),</p> \\[   RMV_{\\alpha}(\\lambda L_1 + (1 - \\lambda)L_2) \\leq \\lambda RMV_{\\alpha}(L_1) + (1 - \\lambda)RMV_{\\alpha}(L_2) \\leq \\max \\left\\{ RMV_{\\alpha}(L_1), RMV_{\\alpha}(L_2)\\right\\}. \\] </li> <li> <p>Law Invariance: If two loss profiles \\( L_1 \\) and \\( L_2 \\) have the same CDF, then:</p> \\[   RMV_{\\alpha}(L_1) = RMV_{\\alpha}(L_2). \\] </li> </ol> Proof <ol> <li> <p>Cash-invariance: For every \\( m \\in \\mathbb{R} \\), and loss \\( L \\), it holds:</p> \\[   \\begin{align*}      RMV_{\\alpha}(L-m) &amp; = E[L-m] + \\frac{\\alpha}{2} E\\left[\\left(L-m-E[L-m]\\right)^2\\right]\\\\                        &amp; = E[L] + \\frac{\\alpha}{2} E\\left[\\left(L - E[L]\\right)^2\\right] - m = RMV_{\\alpha}(L) - m   \\end{align*} \\] </li> <li> <p>Convexity: Let \\( 0 \\leq \\lambda \\leq 1 \\) and \\( L_1 \\) and \\( L_2 \\) be two loss profiles. </p> <p>Since the function \\( x \\mapsto x^2 \\) is convex, it follows that:</p> \\[   \\begin{align*}      \\left(\\lambda L_1 + (1-\\lambda)L_2 - E[\\lambda L_1 + (1-\\lambda)L_2]\\right)^2        &amp;=\\left(\\lambda(L_1 - E[L_1]) + (1-\\lambda)(L_2 - E[L_2])\\right)^2\\\\       &amp;\\leq \\lambda \\left(L_1 - E[L_1]\\right)^2 + (1-\\lambda)\\left(L_2 - E[L_2]\\right)^2   \\end{align*} \\] <p>Taking expectation, it follows that:</p> \\[   \\text{VAR}(\\lambda L_1 + (1-\\lambda)L_2) \\leq \\lambda \\text{VAR}(L_1) + (1-\\lambda) \\text{VAR}(L_2) \\] <p>showing that:</p> \\[   \\begin{align*}      RMV_{\\alpha}(\\lambda L_1 + (1-\\lambda)L_2) &amp; = \\lambda E[L_1] + (1-\\lambda)E[L_2] + \\frac{\\alpha}{2} \\text{VAR}(\\lambda L_1 + (1-\\lambda)L_2)\\\\       &amp;\\leq \\lambda\\left(E[L_1] + \\frac{\\alpha}{2} \\text{VAR}(L_1)\\right) + (1-\\lambda)\\left(E[L_2] + \\frac{\\alpha}{2} \\text{VAR}(L_2)\\right)\\\\       &amp; = \\lambda RMV_{\\alpha}(L_1) + (1-\\lambda)RMV_{\\alpha}(L_2)\\\\       &amp; \\leq \\max \\left\\{ RMV_{\\alpha}(L_1), RMV_{\\alpha}(L_2) \\right\\}   \\end{align*} \\] </li> <li> <p>Law Invariance: For the last assertion, let \\( L_1 \\) and \\( L_2 \\) be such that \\( F_{L_1} = F_{L_2} \\).   It follows that:</p> \\[ \\begin{align*}    RMV_{\\alpha}(L_1) &amp; = \\int_{\\mathbb{R}} x dF_{L_1}(x) + \\frac{\\alpha}{2}\\int_{\\mathbb{R}} \\left[x - \\int_{\\mathbb{R}}x dF_{L_1}(x)\\right]^2 dF_{L_1}(x) \\\\       &amp; = \\int_{\\mathbb{R}} x dF_{L_2}(x) + \\frac{\\alpha}{2}\\int_{\\mathbb{R}} \\left[x - \\int_{\\mathbb{R}}x dF_{L_2}(x)\\right]^2 dF_{L_2}(x) \\\\       &amp; = RMV_{\\alpha}(L_2) \\end{align*} \\] </li> </ol>"},{"location":"lecture/07-SDE/021-what-is-risk/#value-at-risk-vr","title":"Value at Risk (V@R)","text":"<p>The value at risk (V@R) is a widely used risk assessment measure introduced in the finance industry by JP Morgan around 1995. However as an instrument for measuring risk, it has been used for centuries in insurrance industry and turns out to be a very well known mathematical concept. It measures downside risk as follows:</p> <p>Definition</p> <p>Let \\( L \\) be a loss profile. The value at risk (\\( V@R_{\\alpha} \\)) with parameter \\( 0 &lt; \\alpha &lt; 1 \\) is defined as:</p> \\[   V@R_{\\alpha}(L) = \\inf\\{m \\in \\mathbb{R} : P[L &gt; m] \\leq \\alpha\\}. \\] <p>This can also be expressed as:</p> \\[   \\begin{align*}     V@R_{\\alpha}(L) &amp; = \\inf\\{m \\in \\mathbb{R} : P[L &gt; m] \\leq \\alpha\\}\\\\                     &amp; = \\inf\\{m \\in \\mathbb{R} : 1-P[L\\leq m] \\leq \\alpha\\}\\\\                     &amp; = \\inf\\{m \\in \\mathbb{R} : F_L(m) \\geq 1-\\alpha\\}   \\end{align*} \\] <p>where \\( F_L(m):= P[L\\leq m] \\) is the cumulative distribution function (CDF) of \\( L \\).</p> <p> </p> <p>Note: Quantile Function</p> <p>Note that the CDF \\(F_L\\) is an increasing function from \\(0\\) to \\(1\\). Furthermore, it is right continuous meaning that \\(F_L(m_n)\\downarrow F_L(m)\\) for any sequence \\(m_n \\downarrow m\\). Indeed, let \\(A_n = \\{L \\leq m_n\\}\\) and \\(A =\\{L\\leq m\\}\\), it follows that \\(A_1\\supseteq A_2 \\ldots \\supseteq A_n \\supseteq \\ldots\\) with \\(\\cap A_n =A\\). As a consequence of the \\(\\sigma\\)-additivity of the probability measure, it follows that \\(P[A_n]\\downarrow P[A]\\).</p> <p>Now, if \\(F_L\\) is strictly increasing and continuous, it has an inverse \\(F_L^{-1}\\colon (0, 1)\\to \\mathbb{R}\\) which is also strictly increasing and continuous. Such an inverse is called the quantile of \\(L\\) and denoted by \\(q_L\\colon (0,1)\\to \\mathbb{R}\\). It follows that we can write the value at risk in terms of quantile:</p> \\[   \\begin{align*}     V@R_{\\alpha}(L) &amp; = \\inf\\{m \\in \\mathbb{R} : F_L(m) \\geq 1-\\alpha\\}\\\\                     &amp; = \\inf\\{m \\in \\mathbb{R} : F^{-1}_L(F_L(m)) \\geq F^{-1}_L(1-\\alpha)\\}\\\\                     &amp; = \\inf\\{m \\in \\mathbb{R} : m \\geq F^{-1}_L(1-\\alpha)\\}\\\\                     &amp; = F^{-1}_L(1-\\alpha)   \\end{align*} \\] <p>In other terms, \\(V@R_{\\alpha}(L)=q_L(1-\\alpha)\\) is the \\(1-\\alpha\\) quantile of the distribution.</p> <p>In the case where \\(F_L\\) is not strictly increasing and continuous, we can still define the so called (left) pseudo-inverse or quantile as</p> <p>Definition: Quantile</p> <p>The quantile of the random variable \\(L\\) is defined as</p> \\[ \\begin{equation*}   \\begin{split}     q_L \\colon (0,1) &amp; \\longrightarrow \\mathbb{R}\\\\                 s &amp; \\longmapsto q_L(\\alpha) = \\inf\\left\\{ m \\in \\mathbb{R}\\colon P\\left[ L\\leq m \\right] \\geq s\\right\\}   \\end{split} \\end{equation*} \\] <p>The quantile is an increasing and left continuous function for which holds</p> \\[   F_L(q_L(s)-) \\leq s\\leq F_L(q_L(s))   \\] <p>The value at risk indicates the amount of cash or liquidity needed to reduce the loss size so that the probability of making losses exceeds \\( \\alpha \\) is small. Typical values for \\( \\alpha \\) are 5%, 1%, or 0.5%, depending on the horizon.</p> <p>Example</p> <p>Let \\( \\Omega = \\{\\omega_1, \\omega_2, \\omega_3, \\omega_4\\} \\) with probabilities \\( p = (0.7\\%, 3.3\\%, 46\\%, 50\\%) \\), and let \\( L \\) be a loss profile defined as:</p> \\[ L(\\omega) = \\begin{cases} 10,000 &amp; \\text{if } \\omega = \\omega_1, \\\\ -50    &amp; \\text{if } \\omega = \\omega_2, \\\\ -200   &amp; \\text{if } \\omega = \\omega_3, \\\\ -1,000 &amp; \\text{if } \\omega = \\omega_4. \\end{cases} \\] <p>The corresponding CDF \\( F_L(m) \\) is:</p> \\[ F_L(m) = \\begin{cases} 0       &amp; \\text{if } m &lt; -1,000, \\\\ 50\\%    &amp; \\text{if } -1,000 \\leq m &lt; -200, \\\\ 96\\%    &amp; \\text{if } -200 \\leq m &lt; -50, \\\\ 99.3\\%  &amp; \\text{if } -50 \\leq m &lt; 10,000, \\\\ 1       &amp; \\text{if } m \\geq 10,000. \\end{cases} \\] <p>From this, the quantile function is:</p> \\[ q_L(x) = \\begin{cases} -1,000 &amp; \\text{if } 0 &lt; x \\leq 50\\%, \\\\ -200   &amp; \\text{if } 50\\% &lt; x \\leq 96\\%, \\\\ -50    &amp; \\text{if } 96\\% &lt; x \\leq 99.3\\%, \\\\ 10,000 &amp; \\text{if } 99.3\\% &lt; x \\leq 1. \\end{cases} \\] <p>Thus: [ V@R_{5\\%} = q_L(95\\%) = -200, \\quad V@R_{1\\%} = q_L(99\\%) = -50, \\quad V@R_{0.5\\%} = q_L(99.5\\%) = 10,000. ]</p> <p>Note: Practical Computation of Value at Risk</p> <p>Unlike mean-variance risk measures, which involve computing expectations (analytical or via Monte Carlo methods, for instance), the computation of Value at Risk (VaR) is slightly more complex.  Even when a random variable has a probability density function, there is generally no analytical form for its quantile function. In cases where the cumulative distribution function (CDF) is strictly increasing and continuous, computing VaR requires inverting the function \\( m \\mapsto F_L(m) \\). This involves solving the equation:</p> \\[   F_L(m^\\ast) = s \\] <p>where \\( m^\\ast \\) is the quantile we are seeking. This is a classical root-finding problem.</p> <p>Most scientific libraries provide methods for root finding, such as Newton's method, the secant method, or more advanced mixed approaches like Brent's method.  It is important to note, however, that VaR often focuses on high quantiles (e.g., 0.99 or 0.999) of the CDF, which lie near the boundary of the inverse. This makes the problem particularly challenging, as the derivative of the CDF approaches zero near these limits, testing the bounds of numerical precision.  Fortunately, most scientific libraries with statistical functions provide predefined and highly optimized methods for quantile computation. </p> <p>Below, we illustrate this using Python, specifically with <code>scipy.optimize</code> and <code>scipy.stats</code>.</p> <p>Computation of value at risk<pre><code># import libraries\nimport numpy as np\nfrom scipy.stats import norm, t             # Normal and Student distribution\nfrom scipy.optimize import root, brentq     # root-&gt;newton method, brentq-&gt;bissecant flavor\nimport plotly.graph_objs as go              # professional but easy plotting\n\n# Straightforward quantile computation implementation\ndef quantile(cdf, s):\n  # definition of the root function(1) \n  def fun(m):\n    result = cdf(m) - s\n    return result\n\n  # return the root(2)\n  result = root(fun, 0)\n  return result.x[0]\n\n# Define two random variables\nX = norm()      # standard normal\nY = t(df = 2)   # student with 2 as degree of freedom\n\n# plot the cdf of both\n\nx = np.linespace(-4, 4, 100)\ny1 = X.cdf(x)\ny2 = Y.cdf(x)\n\nfig = go.Figure()\nfig.add_scatter(x=x, y=y1, name=\"normal distribution\")\nfig.add_scatter(x=x, y=y2, name=\"student distribution\")\nfig.update_layout(title = 'CDF of normal and student')\nfig.show()\n\n# compute the var 0.01 and 0.01 for each\n\nprint(f\"\"\"\n1% V@R for Normal:\\t{quantile(X.cdf, 0.99)}\n0.01% V@R for Normal:\\t{quantile(X.cdf, 0.999)}\n1% V@R for Student:\\t{quantile(Y.cdf, 0.99)}\n0.01% V@R for Student:\\t{quantile(Y.cdf, 0.999)}\n\"\"\")\n\n\n# using the pre programmed `ppf` functions\nprint(f\"\"\"\n1% V@R for Normal:\\t{X.ppf(0.99)}\n0.01% V@R for Normal:\\t{X.ppf(0.999)}\n1% V@R for Student:\\t{Y.ppf(0.99)}\n0.01% V@R for Student:\\t{Y.ppf(0.999)}\n\"\"\")\n\n# You can compare the speed between your implementation and the pre programmed using %timeit\n</code></pre></p> <ol> <li>Find <code>m</code> such that <code>F(m) = s</code> is equivalent to finding <code>m</code> such that <code>F(m) - s = 0</code> which is the usual implementation.</li> <li>We use here the Newton variant of root optimization problem. It only requires a start point and is usually fast.      However it might not converge if the derivative is quite close to <code>0</code> so it might not always be adequate.     Using <code>brentq</code>, as a bissecant type, requires to provide two bounds <code>a&lt;b</code> within which that root shall be found. In particular it should hold that <code>fun(a)</code> has a different sign as <code>fun(b)</code>.     Both have advantages and inconvenience.</li> </ol> <p>As for mean-variance, value at risk also fulfills some properties</p> <p>Proposition</p> <p>The Value at Risk V@R satisfies the following properties:</p> <ol> <li> <p>Cash-invariance: For every loss profile \\( L \\) and \\( m \\in \\mathbb{R} \\),</p> \\[   V@R_{\\alpha}(L - m) = V@R_{\\alpha}(L) - m. \\] </li> <li> <p>Monotonicity: For any two loss profiles \\( L_1, L_2 \\) with \\(L_1(\\omega) \\leq L_2(\\omega)\\) it holds,</p> \\[   V@R_{\\alpha}(L_1) \\leq V@R_{\\alpha}(L_2). \\] </li> <li> <p>Law Invariance: If two loss profiles \\( L_1 \\) and \\( L_2 \\) have the same CDF, then:</p> \\[   V@R_{\\alpha}(L_1) = V@R_{\\alpha}(L_2). \\] </li> </ol> Proof <ol> <li> <p>Cash-invariance: For every \\( m \\in \\mathbb{R} \\), and loss \\( L \\), it holds with variable change \\(\\hat{m} = \\tilde{m} - m\\):</p> \\[   \\begin{align*}      V@R_{\\alpha}(L-m) &amp; = \\inf\\left\\{ \\tilde{m} \\in \\mathbb{R} \\colon P[L-m&gt;\\tilde{m}] \\leq \\alpha \\right\\}\\\\                     &amp; = \\inf \\left\\{ \\hat{m} - m \\colon P[L&gt;\\hat{m}] \\leq \\alpha  \\right\\}\\\\                     &amp; = \\inf \\left\\{ \\hat{m} \\colon P[L&gt;\\hat{m}] \\leq \\alpha  \\right\\} - m = V@R_{\\alpha}(L) - m   \\end{align*} \\] </li> <li> <p>Monotonicity: Let \\( L_1 \\leq L_2 \\) be two loss profiles.</p> <p>For any \\(m\\), it holds</p> \\[     \\left\\{ \\omega \\colon L_1(\\omega)\\leq m \\right\\} \\supseteq \\left\\{ \\omega \\colon L_2(\\omega)\\leq m \\right\\} \\] <p>showing that for every \\(m\\) we have \\(P[L_1\\leq m] \\geq P[L_2 \\leq m]\\). Hence, we have</p> \\[     \\left\\{ m \\in \\mathbb{R} \\colon P\\left[ L_1\\leq m \\right]\\geq 1-\\alpha \\right\\} \\subseteq  \\left\\{ m \\in \\mathbb{R} \\colon P\\left[ L_2\\leq m \\right]\\geq 1-\\alpha \\right\\} \\] <p>showing that the infimum of the the left handside is smaller than the infimum of the right hand side, that is \\(V@R_{\\alpha}(L_1)\\leq V@R_{\\alpha}(L_2)\\). </p> </li> <li> <p>Law Invariance: This follows immediately since the value at risk depends only on the CDF.</p> </li> </ol>"},{"location":"lecture/07-SDE/021-what-is-risk/#sound-properties","title":"Sound Properties?","text":"<p>Both risk assessments make sense and have a certain appeal. Let us discuss some of the properties they fulfill:</p> <ul> <li> <p>Cash Invariance: Given a risk assessment instrument \\( L \\mapsto R(L) \\), being cash invariant means that \\( R(L - m) = R(L) - m \\).     This property is appreciated by economists and regulators as it confers a clear monetary interpretation to risk assessment.     Regulators typically require financial institutions to maintain their total risk below zero.     For a financial institution with a loss exposure \\( L \\), the question is how much liquidity (or cash) must be held to reduce the overall risk to below zero.     With cash \\( m \\) and risky exposure \\( L \\), the resulting loss profile is \\( L - m \\), with a risk equal to \\( R(L - m) \\).     A risk assessment below zero implies that \\( m \\geq R(L) \\). In other words, the minimal cash requirement to make the risky exposure acceptable is \\( m = R(L) \\).</p> </li> <li> <p>Law Invariance: Law invariance is important because, even though we work with random variables, in practice we observe only their realizations and approximate their CDF.     Hence, a risk assessment instrument should depend solely on the CDF of the loss profile.</p> </li> <li> <p>Monotonicity: Monotonicity means that if the loss profile of one position is always greater than another, i.e., it results in higher losses in all scenarios, then its risk should also be higher.</p> </li> <li> <p>Diversification (Convexity): Diversification implies that combining two risky assets (a convex combination) should result in a risk lower than the worst of the two individual risks.</p> </li> </ul> Property \\( MVR_{\\alpha} \\) \\( V@R_{\\alpha} \\) Cash Invariance Law Invariance Monotonicity Diversification <p>Warning: Value at Risk might lead to Concentration</p> <p>Value at Risk (VaR) can, in some cases, counteract diversification. The primary reason is that the quantile is just a single point on the CDF of the loss distribution and does not account for the full risk in the tail. The following example illustrates this concentration issue:</p> <p>In the first scenario, you lend \\( 1000 \\) RMB to a friend, expecting repayment in one year with \\( 4\\% \\) interest. - If the friend repays the loan, you gain \\( 40 \\) RMB. - If the friend defaults, you lose \\( 1000 \\) RMB. - Assume the probability of default is \\( 4\\% \\).  </p> <p>The loss profile can be represented as:  </p> \\[     L =      \\begin{cases}         -40 &amp; \\text{with probability } 96\\% \\\\         1000 &amp; \\text{with probability } 4\\%     \\end{cases} \\] <p>This results in the following CDF and quantile function:  </p> \\[     \\begin{align*}         F_L(m) &amp; = \\begin{cases}             0 &amp; \\text{for } m &lt; -40 \\\\             96\\% &amp; \\text{for } -40 \\leq m &lt; 1000 \\\\             100\\% &amp; \\text{for } m \\geq 1000         \\end{cases} \\\\         q_L(s) &amp; = \\begin{cases}             -40 &amp; \\text{for } 0 &lt; s \\leq 96\\% \\\\             1000 &amp; \\text{for } s &gt; 96\\%         \\end{cases}     \\end{align*} \\] <p>The Value at Risk at the \\( 5\\% \\) level is:  </p> \\[     V@R_{5\\%}(L) = q_L(95\\%) = -40 \\] <p>Now, consider diversifying your exposure by lending \\( 500 \\) RMB to two independent friends:  </p> \\[     L =      \\begin{cases}         -40 &amp; \\text{with probability } 92.16\\% \\\\         480 &amp; \\text{with probability } 7.68\\% \\\\         1000 &amp; \\text{with probability } 0.16\\%     \\end{cases} \\] <p>This diversification reduces the probability of large losses to \\( 0.16\\% \\), but introduces a medium loss of \\( 480 \\) RMB with a \\( 7.68\\% \\) probability.  </p> <p>The CDF and quantile function for this case are:  </p> \\[     \\begin{align*}         F_L(m) &amp; = \\begin{cases}             0 &amp; \\text{for } m &lt; -40 \\\\             92.16\\% &amp; \\text{for } -40 \\leq m &lt; 480 \\\\             99.84\\% &amp; \\text{for } 480 \\leq m &lt; 1000 \\\\             100\\% &amp; \\text{for } m \\geq 1000         \\end{cases} \\\\         q_L(s) &amp; = \\begin{cases}             -40 &amp; \\text{for } 0 &lt; s \\leq 92.16\\% \\\\             480 &amp; \\text{for } 92.16\\% &lt; s \\leq 99.84\\% \\\\             1000 &amp; \\text{for } s &gt; 99.84\\%         \\end{cases}     \\end{align*} \\] <p>The Value at Risk at the \\( 5\\% \\) level is:  </p> \\[     V@R_{5\\%}(L) = q_L(95\\%) = 480 \\] <p>In this case, the Value at Risk increases from \\( -40 \\) to \\( 480 \\), contradicting the expectation that diversification reduces risk. The primary reason is that, in the non-diversified scenario, all potential losses are concentrated in the tail of the distribution beyond the chosen quantile. In other words, Value at Risk is blind to the magnitude of losses beyond the selected quantile level.</p> <p>Even though both instruments (mean-variance risk and Value at Risk) have intuitive appeal and practical applications, closer scrutiny reveals that each violates one or more fundamental properties expected of a robust risk measure.</p>"},{"location":"lecture/07-SDE/022-risk-preferences/","title":"Risk Preferences and Measures","text":"<p>So far, we have introduced potential examples of risk measures and discussed their shortcomings regarding properties deemed sound for risk assessment. Additionally, the selection of these measures might appear arbitrary. In the following, we aim to formalize the concepts of risk and uncertainty.</p> <p>On the one hand, uncertainty refers to the possibility of multiple outcomes. In other words, it considers the set \\( \\Omega \\) within a probability space. This concept is inherently an objective matter, tied to the nature of the world.</p> <p>On the other hand, risk represents a subjective or personal perception of uncertainty. It depends on an individual's viewpoint and can be understood as a cautious response to uncertainty. To model this consistently within a mathematical framework, we rely on decision theory, which captures preferences among various choices. The set of possible choices is denoted by \\( \\mathcal{X} \\). In this context, uncertain outcomes are modeled as random variables, meaning we work with a vector space \\( \\mathcal{X} \\) of random variables (primarily bounded for mathematical convenience).</p> <p>Definition: Preference Order and Numerical Representation</p> <p>A preference order \\( \\preccurlyeq \\) on \\( \\mathcal{X} \\) is a binary relation \\( x \\preccurlyeq y \\) indicating that choice \\( y \\) is preferred to choice \\( x \\). We assume this relation satisfies the following normative properties:</p> <ul> <li>Transitivity: \\( x \\preccurlyeq y \\) and \\( y \\preccurlyeq z \\) imply \\( x \\preccurlyeq z \\);</li> <li>Completeness: For any two possible choices \\( x \\) and \\( y \\), either \\( x \\preccurlyeq y \\) or \\( y \\preccurlyeq x \\).</li> </ul> <p>A function \\( U\\colon \\mathcal{X} \\to \\mathbb{R} \\) is called a numerical representation (or utility) of a preference order \\( \\preccurlyeq \\) if:</p> \\[     x \\preccurlyeq y \\quad \\text{if and only if} \\quad U(x) \\leq U(y) \\] <p>Preference orders are a generic way to represent subjective views on outcomes. The first property, transitivity, ensures consistency: if \\( y \\) is preferred to \\( x \\), and \\( z \\) is preferred to \\( y \\), then \\( z \\) must also be preferred to \\( x \\). This property appears quite intuitive. The second property, completeness, requires that, for any two elements, one must always express a preference between them. This is a strong assumption, as it mandates the ability to decide between any two elements in a potentially infinite set \\( \\mathcal{X} \\).  </p> <p>These two rational (or normative, as decision theorists would say) assumptions often fail in empirical decision-making. However, they are intended to model fully rational behavior in decision-making processes involving prospective outcomes.  </p> <p>A numerical representation maps the preference ranking into \\( \\mathbb{R} \\), providing a quantitative measure of preferences.</p> Note <p>Note first that if we have a numerical representation \\( U \\) for a preference order \\( \\preccurlyeq \\), it is not unique. Any strictly increasing function \\( \\phi \\colon \\mathbb{R} \\to \\mathbb{R} \\) defines another numerical representation \\( \\tilde{U} = \\phi \\circ U \\). Indeed, \\( x \\preccurlyeq y \\) is equivalent to \\( U(x) \\leq U(y) \\), which is equivalent to \\( \\phi(U(x)) = \\tilde{U}(x) \\leq \\tilde{U}(y) = \\phi(U(y)) \\).</p> <p>Second, starting directly with a function \\( U \\colon \\mathcal{X} \\to \\mathbb{R} \\), it defines a preference order \\( \\preccurlyeq \\) by:</p> \\[     x \\preccurlyeq y \\Leftrightarrow U(x) \\leq U(y) \\] <p>As an exercise, show that \\( \\preccurlyeq \\), so defined through a function \\( U \\), is a preference order, satisfying transitivity and completeness.</p> <p>Third, even if a numerical function defines a preference order, the reciprocal is not necessarily true. Additional assumptions are required to ensure that, for a given preference order, a numerical representation \\( U \\) exists. However, under reasonable assumptions, this is often the case.</p> <p>Proposition</p> <p>If the set \\( \\mathcal{X} \\) is countable (1), then any preference order \\( \\preccurlyeq \\) on \\( \\mathcal{X} \\) admits a numerical representation.</p> <ol> <li>Meaning that \\( \\mathcal{X} \\) can be enumerated as a subset of \\( \\mathbb{N} \\).</li> </ol> <p>Proof</p> <p>Without loss of generality, assume \\( \\mathcal{X} = \\{x_1, x_2, \\ldots\\} \\). On \\( \\mathbb{N} \\), define a probability measure \\( P[\\{n\\}] = p_n = 1 / 2^n \\), since \\( \\sum p_n = 1 \\). For each \\( x_n \\), define \\( A_n = \\{k \\colon x_k \\preccurlyeq x_n\\} \\), the set of indices \\( k \\) for elements in \\( \\mathcal{X} \\) that are less preferred than \\( x_n \\). By definition, \\( x_n \\preccurlyeq x_m \\) if and only if \\( A_n \\subseteq A_m \\). The function:</p> \\[     \\begin{equation*}         \\begin{split}             U \\colon \\mathcal{X} &amp;\\longrightarrow \\mathbb{R}\\\\                 x_n &amp; \\longmapsto U(x_n) = P[A_n] = \\sum_{\\{k \\colon x_k \\preccurlyeq x_n\\}} p_k         \\end{split}     \\end{equation*} \\] <p>defines a numerical representation of \\( \\preccurlyeq \\). Indeed, \\( x_n \\preccurlyeq x_m \\) if and only if \\( A_n \\subseteq A_m \\). Since \\( P \\) assigns a unique probability to each element of \\( \\mathbb{N} \\), it follows that \\( A_n \\subseteq A_m \\) if and only if \\( U(x_n) = P[A_n] \\leq P[A_m] = U(x_m) \\). This completes the proof.</p> <p>This proposition uses probability measures to define a numerical representation. The argument extends to more general sets, provided you can relate sublevel sets \\( \\{\\tilde{x} \\colon \\tilde{x} \\preccurlyeq x\\} \\) using topological arguments like smoothness. If such smoothness is absent, preference orders may exist without a numerical representation.</p> <p>The Lexicographical Order Does Not Admit a Numerical Representation</p> <p>Consider \\( \\mathcal{X} = [0, 1] \\times [0, 1] \\) and define the lexicographical order as:</p> \\[     x = (x_1, x_2) \\preccurlyeq y = (y_1, y_2) \\quad \\text{if and only if} \\quad      \\begin{cases}       \\text{either } &amp; x_1 &lt; y_1, \\\\       \\text{or } &amp; x_1 = y_1 \\text{ and } x_2 \\leq y_2.     \\end{cases} \\] <p>This is a preference order (similar to library book ordering). However, since \\( \\mathcal{X} \\) is uncountable and the preference order lacks smoothness, it can be shown that no numerical representation exists. Try this as an exercise: assume a numerical representation exists and derive a contradiction.</p> <p>Decision theory typically frames preferences and utilities (where higher values are better). However, when discussing risk, we consider possible loss profiles \\( \\mathcal{L} \\)\u2014random variables representing losses. For simplicity, we consider complete binary relations \\( \\preccurlyeq \\), where \\( L_1 \\preccurlyeq L_2 \\) means \"\\( L_1 \\) is less risky than \\( L_2 \\).\" Thus, loss profiles are ranked by \\( \\preccurlyeq \\) based on perceived risk. However, the basic properties of a preference order \\( \\preccurlyeq \\) on \\( \\mathcal{L} \\) do not inherently convey insights about risk perception.</p> <p>Definition: Risk Order and Risk Measures</p> <p>A preference order \\( \\succcurlyeq \\) on \\( \\mathcal{L} \\) is called a risk order if the following two additional assumptions are satisfied:</p> <ul> <li> <p>Diversification: If \\( L_1 \\) is more risky than \\( L_2 \\), then any diversified position between the two is less risky than the worse one:</p> \\[     \\text{if } L_1 \\succcurlyeq L_2, \\quad \\text{then}\\quad L_1 \\succcurlyeq \\lambda L_1 + (1-\\lambda) L_2, \\quad \\text{for every } 0 \\leq \\lambda \\leq 1. \\] </li> <li> <p>Monotonicity (worse for sure is more risky): If the losses of \\( L_1 \\) are worse than those of \\( L_2 \\) in every state of the world, then \\( L_1 \\) is more risky than \\( L_2 \\):</p> \\[     \\text{if } L_1(\\omega) \\geq L_2(\\omega) \\text{ for every } \\omega, \\quad \\text{then } L_1 \\succcurlyeq L_2. \\] </li> </ul> <p>A numerical representation \\( R \\colon \\mathcal{L} \\to \\mathbb{R} \\) of a risk order is called a risk measure.</p> <p>These two additional properties express reasonable and intuitive features of risk perception. They also have implications for the properties of risk measures.</p> <p>Proposition</p> <p>Let \\( R \\) be a numerical representation of a preference order \\( \\succcurlyeq \\) on \\( \\mathcal{L} \\). Then the following assertions are equivalent:</p> <ul> <li>\\( \\succcurlyeq \\) is a risk order;</li> <li>\\( R \\) satisfies:<ul> <li>Quasi-convexity: \\( \\max\\{R(L_1), R(L_2)\\} \\geq R(\\lambda L_1 + (1-\\lambda) L_2) \\) for every \\( 0 \\leq \\lambda \\leq 1 \\);</li> <li>Monotonicity: If \\( L_1(\\omega) \\geq L_2(\\omega) \\) for every \\( \\omega \\), then \\( R(L_1) \\geq R(L_2) \\).</li> </ul> </li> </ul> Proof <p>Let \\( L_1 \\) and \\( L_2 \\) be two loss profiles. Assume that \\( \\succcurlyeq \\) is a risk order.  </p> <p>For quasi-convexity, due to the completeness of the relation, assume without loss of generality that \\( L_1 \\succcurlyeq L_2 \\), which is equivalent to \\( R(L_1) = \\max\\{R(L_1), R(L_2)\\} \\). For any \\( 0 \\leq \\lambda \\leq 1 \\), the diversification property implies \\( L_1 \\succcurlyeq \\lambda L_1 + (1-\\lambda) L_2 \\), which gives:</p> \\[     \\max\\{R(L_1), R(L_2)\\} = R(L_1) \\geq R(\\lambda L_1 + (1-\\lambda) L_2), \\] <p>showing quasi-convexity of \\( R \\).</p> <p>For monotonicity, assume \\( L_1(\\omega) \\geq L_2(\\omega) \\) for every \\( \\omega \\). By the monotonicity assumption, \\( L_1 \\succcurlyeq L_2 \\), which implies \\( R(L_1) \\geq R(L_2) \\).  </p> <p>The reverse implication\u2014that a numerical representation being quasi-convex and monotone implies \\( \\succcurlyeq \\) is a risk order\u2014is straightforward to verify.</p> <p>This proposition shows that neither the mean-variance risk measure nor the Value at Risk represents a risk order. Additional properties may be required of a risk measure, but they might not always align with the underlying risk order.</p> <p>Definition</p> <p>A risk measure \\( R \\) is called:</p> <ul> <li>Cash-Invariant: if \\( R(L-m) = R(L) - m \\) for every \\( m \\in \\mathbb{R} \\);</li> <li>Positive-Homogeneous: if \\( R(\\lambda L) = \\lambda R(L) \\) for every \\( \\lambda &gt; 0 \\);</li> <li>Law-Invariant: if \\( R(L) = R(\\tilde{L}) \\) whenever the CDFs of \\( L \\) and \\( \\tilde{L} \\) coincide.</li> </ul> <p>Aside from law invariance, the other two properties do not hold if the risk measure is transformed by a strictly increasing function. Nevertheless, they are commonly used and practical.</p> <p>Cash-Invariance</p> <p>Cash-invariance is typically required from regulatory or financial perspectives. For instance, consider a financial institution with a risky position \\( X \\). The question is how much liquidity \\( m \\) is needed in the bank account to ensure the overall position (cash plus risky assets) has acceptable risk. The threshold is that the total risk must be below zero. The total loss profile is \\( L - m \\), where \\( L = -X \\). By cash-invariance:</p> \\[     0 \\geq R(L - m) = R(L) - m \\implies m \\geq R(L). \\] <p>Thus, the minimal liquidity required to make the risky position acceptable is \\( m = R(L) \\). This interpretation ties the risk measure to capital requirements.</p> <p>Moreover, cash-invariance, combined with quasi-convexity, implies convexity.</p> <p>Lemma</p> <p>If \\( R \\) is a cash-invariant risk measure, then \\( R \\) is convex.</p> Proof <p>Let \\( R \\) be a cash-invariant risk measure, \\( 0 \\leq \\lambda \\leq 1 \\), and \\( L_1, L_2 \\) be loss profiles. To prove \\( R(\\lambda L_1 + (1-\\lambda) L_2) \\leq \\lambda R(L_1) + (1-\\lambda) R(L_2) \\), define \\( m_1 = R(L_1) \\) and \\( m_2 = R(L_2) \\). By cash-invariance and quasi-convexity:</p> \\[     \\begin{align*}         R(\\lambda L_1 + (1-\\lambda) L_2) - \\lambda m_1 - (1-\\lambda)m_2 &amp; = R\\left( \\lambda L_1 + (1-\\lambda) L_2 - \\lambda m_1 - (1-\\lambda)m_2 \\right) \\\\         &amp; = R\\left( \\lambda (L_1 - m_1) + (1-\\lambda)(L_2 - m_2) \\right) \\\\         &amp; \\leq \\max\\{R(L_1 - m_1), R(L_2 - m_2)\\} \\\\         &amp; = \\max\\{0, 0\\} = 0.     \\end{align*} \\] <p>Positive Homogeneity</p> <p>Positive homogeneity has a financial interpretation: if \\( L \\) represents the loss exposure of an investment with risk \\( R(L) \\), scaling the investment by \\( \\lambda &gt; 0 \\) scales the corresponding risk by \\( \\lambda \\). While desirable for mathematical reasons, super-linear scaling might be expected in some contexts. Positive homogeneity also implies sub-additivity, ensuring that risk is not exacerbated by combining positions.</p> <p>Lemma</p> <p>Let \\( R \\) be a cash-invariant risk measure. If \\( R \\) is positive homogeneous, then:</p> \\[     R(L_1 + L_2) \\leq R(L_1) + R(L_2). \\] Proof <p>Let \\( R \\) be a cash-invariant and positive-homogeneous risk measure. By convexity:</p> \\[     \\begin{align*}         R(L_1 + L_2) &amp; = R\\left( 2 \\cdot \\frac{1}{2}(L_1 + L_2) \\right) \\\\         &amp; = 2 R\\left(\\frac{1}{2}L_1 + \\frac{1}{2}L_2 \\right) \\quad \\text{(Positive Homogeneity)} \\\\         &amp; \\leq 2 \\left( \\frac{1}{2}R(L_1) + \\frac{1}{2}R(L_2) \\right) \\quad \\text{(Convexity)} \\\\         &amp; = R(L_1) + R(L_2).     \\end{align*} \\]"},{"location":"lecture/07-SDE/023-oce/","title":"Expected Shortfall","text":"<p>We have thus far explored the fundamentals of risk assessment, focusing on the key principles it must satisfy to achieve sound quantification: monotonicity, diversification, and, for financial purposes, cash-invariance. This foundation has enabled us to highlight the fundamental flaws of mean-variance analysis and value at risk (VaR) in meeting these criteria. However, from a practical standpoint, we are still far from identifying a fully satisfactory approach.</p> <p>When considering a risk quantification instrument \\( R \\), the following points are crucial:</p> <ol> <li>Soundness: The instrument \\( R \\) must satisfy the properties of diversification and monotonicity to ensure robust risk quantification.</li> <li>Understandability: \\( R \\) should be intuitively comprehensible from a financial perspective, even for individuals not deeply versed in the intricacies of mathematics. Ultimately, you need to convince your boss, the regulator, and the public that the methodology you employ is sensible and reliable.</li> <li>Implementability: The computation of \\( R \\) must be feasible. At the end of the day, you need to produce a quantifiable result. This means it should be possible to create a programmatic function, based on available data, to compute the value of your risk measure (prototyping).</li> <li>Efficiency and Robustness: The implementation of \\( R \\) should meet industry standards\u2014being fast, reliable, and free of bugs. Risk computations are not a one-time experiment; they need to be conducted daily. Large financial institutions, by regulatory requirement, must aggregate and assess vast and complex positions to provide timely results on a daily basis.</li> </ol> <p>As for now, our focus has been primarily on the first point\u2014establishing the groundwork for soundness. However, the other points are equally vital in practice. Since the 2008 financial crisis, the shortcomings of value at risk (VaR) have been widely acknowledged. While these shortcomings (particularly related to soundness) were long known to academics, addressing the other points took time before a new industry standard could emerge. This standard is the expected shortfall (also known under equivalent terms such as average value at risk or conditional value at risk).</p>"},{"location":"lecture/07-SDE/023-oce/#expected-shortfall_1","title":"Expected Shortfall","text":"<p>As the main issue of value at risk being the fact that it only provides information at one point of the CDF and being blind beyond it, the idea is to consider the tail beyond value at risk</p> <p>Definition: Expected Shortfall</p> <p>The expected shortfall of a random variable (integrable) at level \\(\\alpha\\) is defined as</p> \\[     ES_{\\alpha}(L) = \\frac{1}{\\alpha}\\int_0^\\alpha V@R_{s}(L) ds = \\frac{1}{\\alpha}\\int_{1-\\alpha}^1 q_L(s) ds \\] <p>Note</p> <p>The Expected Shortfall (ES) was introduced by Artzner, Delbaen, Eber, and Heath in 1999 to address the shortcomings of Value at Risk (V@R). Expected Shortfall is known by several other names (with equivalent definitions, modulo some subtleties), including:  </p> <ul> <li>Average Value at Risk (AV@R),  </li> <li>Conditional Value at Risk (CV@R),  </li> <li>Expected Tail Loss (ETL), and  </li> <li>Superquantile.</li> </ul> <p> </p> <p>As shown in the figure, the expected shortfall (ES) addresses the shortcomings of value at risk (V@R) by considering the loss area beyond V@R. Specifically, if two loss distributions, \\( \\tilde{L} \\) and \\( L \\), share the same V@R but \\( \\tilde{L} \\) exhibits larger losses beyond the V@R (i.e., has fatter tails than \\( L \\)), then\u2014even with identical V@R values\u2014the expected shortfall (the area beyond V@R) of \\( \\tilde{L} \\) will exceed that of \\( L \\).</p> <p>This observation addresses the second point on our wish list, as ES naturally rectifies V@R's limitations regarding tail risk. However, it does not resolve the first issue on our list. Specifically, while it is clear that V@R fails to satisfy diversification, it remains puzzling why ES should satisfy this property. The desirable properties of V@R\u2014monotonicity, law invariance, cash-invariance, and positive homogeneity\u2014extend to ES through its integral formulation. However, since V@R is not convex, it is unclear why ES should exhibit convexity based on this representation.</p> <p>Furthermore, while this formulation satisfies the third point (as ES is computed as an integral of a quantifiable object), doubts remain about its efficiency. Calculating the integral of the quantile involves evaluating numerous quantiles between \\( 1-\\alpha \\) and \\( 1 \\), which is computationally intensive and prone to error. This challenge is especially pronounced for extreme quantiles (e.g., \\( 99.999\\% \\) or \\( 99.99999\\% \\)), where sampling the distribution in highly unlikely regions becomes unstable.</p> <p>To address these issues, we now explore another class of risk assessment instruments introduced by operations research scientists Ben-Tal and Teboulle: the optimized certainty equivalent.</p>"},{"location":"lecture/07-SDE/023-oce/#optimized-certainty-equivalent","title":"Optimized Certainty Equivalent","text":"<p>At the core of the definition of the optimized certainty equivalent is a special penalization function called loss function.</p> <ul> <li> <p>Definition: Loss Function</p> <p>A function \\(\\ell \\colon \\mathbb{R} \\to \\mathbb{R}\\) is called a loss function if</p> <ul> <li>\\(\\ell\\) is convex</li> <li>\\(\\ell\\) is increasing    </li> <li> <p>\\(\\ell(0) = 0\\) and \\(\\ell^\\prime(0) = 1\\)(1)</p> <ol> <li>Note that \\(\\ell\\) does not necessarily need to be differentiable such as \\(\\ell(x) = x^+/\\alpha\\) for \\(0&lt; \\alpha &lt;1\\). It just needs to have \\(\\ell^{\\prime}_-(0) \\leq 1 \\leq \\ell^\\prime_+(0)\\) where \\(\\ell_-^\\prime\\) and \\(\\ell^\\prime_+\\) are the left and right derivative that always exists for convex functions.</li> </ol> </li> <li> <p>\\(\\lim_{x \\to \\infty}\\ell(x)/x &gt;1\\) and \\(\\lim_{x \\to -\\infty} \\ell(x)/x &lt;1\\).</p> </li> </ul> <p>Classical examples following this definition</p> <ul> <li>piecewise linear: \\(\\ell(x)= x^+/ \\alpha\\) with \\(0&lt; \\alpha &lt;1\\);</li> <li>quadratic: \\(\\ell(x)=x^++(x^+)^2/2\\);</li> <li>exponential: \\(\\ell(x)=e^x-1\\)</li> </ul> </li> </ul> <p> </p> <p>The loss function penalizes a loss (losses are considered positive in our case) \\( x \\geq 0 \\) by assigning a value \\( \\ell(x) \\geq x \\). For gains (negative values), it also penalizes by assigning an amount smaller than the gain itself.</p> <p>Thus, given a loss profile \\( L \\), you compute \\( E[\\ell(L)] \\geq E[L] \\), which represents the penalized loss estimation of the loss profile. The idea introduced by Ben-Tal and Teboulle is to reduce the value of these penalized losses by allocating some cash \\( m \\), transitioning from \\( E[\\ell(L)] \\) to \\( E[\\ell(L-m)] \\). However, in terms of total costs, you must account for the cash allocated, leading to the total cost valuation:</p> \\[ m + E[\\ell(L-m)]. \\] <p>With the decision variable being the amount of cash allocated, minimizing the total cost gives rise to the definition of the optimized certainty equivalent.</p> <p>Definition: Optimized Certainty Equivalent</p> <p>Given a loss function \\( \\ell \\), the optimized certainty equivalent \\( R \\) of a bounded random variable (under appropriate integrability conditions) is defined as:</p> \\[   R(L) = \\inf \\left\\{ m + E\\left[ \\ell(L - m) \\right] \\colon m \\in \\mathbb{R} \\right\\}. \\] <p>Proposition</p> <p>Given a loss function \\( \\ell \\), the optimized certainty equivalent \\( R \\) is a cash-invariant and law-invariant risk measure.</p> <p>Furthermore, it holds that:</p> \\[   R(L) = m^\\ast + E\\left[ \\ell(L - m^\\ast) \\right], \\] <p>where: (1)</p> <ol> <li> <p>If \\( \\ell \\) is not differentiable at \\( 0 \\), the condition changes to:</p> \\[ E[\\ell^\\prime_-(L-m^\\ast)] \\leq 1 \\leq E\\left[ \\ell^\\prime_+(L-m^\\ast) \\right], \\] </li> </ol> \\[     E[\\ell^\\prime(L - m^\\ast)] = 1. \\] Proof <p>We show that \\( R \\), as defined, is monotone, cash-invariant, and convex.</p> <ul> <li> <p>Monotonicity:      Suppose \\( L_1(\\omega) \\geq L_2(\\omega) \\) for all \\( \\omega \\).     Since \\( \\ell \\) is increasing:</p> \\[     m + \\ell\\left( L_1 - m \\right) \\geq m + \\ell\\left( L_2 - m \\right). \\] <p>Taking the expectation:</p> \\[     m + E\\left[\\ell\\left( L_1 - m \\right)\\right] \\geq m + E\\left[\\ell\\left( L_2 - m \\right)\\right]. \\] <p>Since \\( m + E[\\ell(L_2 - m)] \\geq R(L_2) \\), it follows that:</p> \\[     m + E\\left[\\ell\\left( L_1 - m \\right)\\right] \\geq R(L_2). \\] <p>Taking the infimum over \\( m \\) yields:</p> \\[     R(L_1) = \\inf\\left\\{ m + E\\left[\\ell\\left( L_1 - m \\right)\\right] \\colon m \\in \\mathbb{R} \\right\\} \\geq R(L_2). \\] </li> <li> <p>Cash-Invariance:     Let \\( m \\in \\mathbb{R} \\).     Then:</p> \\[     \\begin{align*}         R(L - m) &amp; = \\inf\\left\\{ \\tilde{m} + E\\left[\\ell\\left( L - m - \\tilde{m} \\right)\\right] \\colon \\tilde{m} \\in \\mathbb{R} \\right\\} \\\\             &amp; = \\inf\\left\\{ \\hat{m} - m + E\\left[\\ell\\left( L - \\hat{m} \\right)\\right] \\colon \\hat{m} \\in \\mathbb{R} \\right\\} \\quad \\text{(change of variable \\( \\hat{m} = m + \\tilde{m} \\))} \\\\             &amp; = R(L) - m.     \\end{align*} \\] </li> <li> <p>Convexity:   Let \\( L_1 \\) and \\( L_2 \\) be two loss profiles, and \\( 0 \\leq \\lambda \\leq 1 \\).     For \\( m_1, m_2 \\in \\mathbb{R} \\), define \\( m = \\lambda m_1 + (1-\\lambda)m_2 \\) and \\( L = \\lambda L_1 + (1-\\lambda)L_2 \\).     Since \\( \\ell \\) is convex:</p> \\[     m + \\ell(L - m) \\leq \\lambda\\left( m_1 + E\\left[ \\ell(L_1 - m_1) \\right] \\right) + (1-\\lambda)\\left( m_2 + E[\\ell(L_2 - m_2)] \\right). \\] <p>Since \\( R(L) \\leq m + E[\\ell(L - m)] \\), taking the infimum over \\( m_1 \\) and \\( m_2 \\) sequentially yields:</p> \\[     R(\\lambda L_1 + (1-\\lambda)L_2) = R(L) \\leq \\lambda R(L_1) + (1-\\lambda)R(L_2). \\] </li> <li> <p>Law-Invariance:   Law-invariance follows directly, as \\( R \\) depends only on the expectation \\( E[\\ell(\\cdot)] \\), which depends on the CDF of \\( L \\).</p> </li> </ul> <p>To show the final assertion: Define:</p> \\[     g(m) = m + E\\left[ \\ell(L - m) \\right], \\] <p>for which \\( R(L) = \\inf g(m) \\). Since \\( \\ell \\) is convex, \\( g \\) is also convex. It follows from \\(\\ell\\) being increasing and the asymptotic assumptions on \\(\\ell\\) that \\(\\ell(x) \\geq a_1 x -c_1\\) for \\(x\\) positively large enough with \\(a_1&gt;1\\) and \\(\\ell(x)\\geq a_2 x -c_2\\) for \\(x\\) negatively large enough and \\(a_2&lt;1\\). Since \\(L\\) is bounded, it follows that for \\(m\\) positively large enough (more than the bounds of \\(L\\) at least) we have</p> \\[     g(m) = m + E[\\ell(L-m)] \\geq m + a_2E\\left[ L -m \\right] - c_2 = \\underbrace{(1-a_2)}_{&gt;0} \\underbrace{m}_{&gt;0} + a_2 E[L] - c_2 \\xrightarrow[m \\to \\infty]{} \\infty \\] <p>The same argumentation for large enough negative values of \\(m\\) yields</p> \\[     g(m) = m + E[\\ell(L-m)] \\geq m + a_1E\\left[ L -m \\right] - c_1 = \\underbrace{(1-a_1)}_{&lt;0} \\underbrace{m}_{&lt;0} + a_1 E[L] - c_1 \\xrightarrow[m \\to -\\infty]{} \\infty \\] <p>All together, it shows that \\(g(m) \\to \\infty\\) for \\(m\\to \\pm \\infty\\), that is, in mathematical terms, \\(g\\) is coercive.</p> <p> </p> <p>This ensures that \\( g \\) attains its minimum at \\( m^\\ast \\), satisfying the first-order condition:</p> \\[     E\\left[ \\ell^\\prime_-(L-m^\\ast) \\right] \\leq 1 \\leq E\\left[ \\ell^\\prime_+(L-m^\\ast) \\right]. \\] <p>If \\( \\ell \\) is differentiable, this simplifies to:</p> \\[     E\\left[ \\ell^\\prime(L - m^\\ast) \\right] = 1. \\] <p>This completes the proof.</p> <p>This proposition provides several key takeaways:</p> <ol> <li>The optimized certainty equivalent (OCE) is a risk measure independent of the specific definition of \\( \\ell \\), as long as \\( \\ell \\) is a valid loss function.  </li> <li>By its definition and the convexity of the problem, the computation of OCE is straightforward, reducing to a one-dimensional unconstrained convex optimization problem. This allows for the application of efficient, state-of-the-art algorithms.  </li> <li>The simplicity of this optimization problem allows for circumventing classical gradient descent by providing an explicit expression for the first-order condition.</li> </ol> <p>The Exponential Function: Entropic Risk Measure</p> <p>Consider the loss function \\( \\ell(x) = (e^{\\gamma x} - 1)/\\gamma \\). By the first-order condition:</p> \\[   1 = E[\\ell^\\prime(L-m^\\ast)] = E[e^{\\gamma (L - m^\\ast)}] = e^{-\\gamma m^\\ast}E\\left[ e^{\\gamma L} \\right]. \\] <p>Solving for \\( m^\\ast \\):</p> \\[   m^\\ast = \\frac{\\ln\\left(E[e^{\\gamma L}]\\right)}{\\gamma}. \\] <p>Substituting \\( m^\\ast \\) back into \\( R \\) yields:</p> \\[   R(L) = \\frac{1}{\\gamma} \\ln \\left( E\\left[ e^{\\gamma L} \\right] \\right). \\] <p>Hence, for the exponential loss function, the OCE can be computed explicitly, and the resulting risk measure is known as the entropic risk measure.  </p> <p>While this measure is prevalent in other domains (e.g., statistical mechanics, physics, and machine learning) and is computationally efficient, it is unsuitable as a financial risk measure. The exponential penalization assigns extremely high values to large losses, making it impractical for scenarios with rare but severe losses.  </p> <p>For example, consider the loss profile:</p> \\[   \\begin{cases}       1,000,000,000 &amp; \\text{with probability } 0.00001, \\\\       -10,000 &amp; \\text{otherwise}.   \\end{cases} \\] <p>Despite the low probability of the extreme loss, the exponential penalization makes the risk computation infeasible due to numerical instability and even with exact values, the resulting risk would be stratospherical.</p> <p>The Piecewise Linear Function</p> <p>The exponential function example demonstrates how a strong penalization can lead to explicit representations but may not be practical for financial risk measures. Let us now consider the opposite extreme: a function that penalizes less, specifically a piecewise linear loss function:</p> \\[     \\ell(x) = \\frac{1}{\\alpha}x^+, \\] <p>where \\( 0 &lt; \\alpha &lt; 1 \\).  </p> <p>Since \\( \\ell \\) is not differentiable, the characterization uses the left and right derivatives:</p> \\[   \\ell_-^\\prime(x) = \\frac{1}{\\alpha} 1_{(0, \\infty)}(x) =        \\begin{cases}         \\frac{1}{\\alpha} &amp; x &gt; 0, \\\\         0 &amp; \\text{otherwise}.       \\end{cases}   \\quad \\text{and} \\quad   \\ell_+^\\prime(x) = \\frac{1}{\\alpha} 1_{[0, \\infty)}(x) =        \\begin{cases}         \\frac{1}{\\alpha} &amp; x \\geq 0, \\\\         0 &amp; \\text{otherwise}.       \\end{cases} \\] <p>Applying the first-order condition:</p> \\[   E[\\ell^\\prime_-(L-m^\\ast)] \\leq 1 \\leq E[\\ell^\\prime_+(L-m^\\ast)]. \\] <p>Substituting the derivatives:</p> \\[   \\frac{1}{\\alpha}P[L &gt; m^\\ast] \\leq 1 \\leq \\frac{1}{\\alpha}P[L \\geq m^\\ast]. \\] <p>This simplifies to:</p> \\[   P[L &lt; m^\\ast] \\leq 1-\\alpha \\leq P[L \\leq m^\\ast]. \\] <p>Thus, \\( m^\\ast \\) is the \\( 1-\\alpha \\) quantile of \\( L \\):</p> \\[   m^\\ast = q_L(1-\\alpha) = V@R_{\\alpha}(L). \\] <p>Therefore, for the piecewise linear loss function:</p> \\[     R(L) = \\inf\\left\\{ m + \\frac{1}{\\alpha}E\\left[ (L-m)^+ \\right] \\right\\} = V@R_{\\alpha}(L) + \\frac{1}{\\alpha}E\\left. \\]"},{"location":"lecture/07-SDE/023-oce/#expected-shortfall-and-optimized-certainty-equivalent","title":"Expected Shortfall and Optimized Certainty Equivalent","text":"<p>On one hand, we previously noted that it is not entirely clear how to show that Expected Shortfall (ES) is a risk measure when derived as the integral of V@R. On the other hand, the optimized certainty equivalent (OCE) with a piecewise linear loss function shows some structural similarities to V@R. It turns out that these two concepts are strongly connected, as demonstrated by the following proposition:</p> <p>Proposition</p> <p>For bounded loss profiles (or even integrable ones), the Expected Shortfall with confidence level \\( 0 &lt; \\alpha &lt; 1 \\) coincides with the optimized certainty equivalent using a piecewise linear loss function with a factor of \\( 1/\\alpha \\).  </p> <p>In other words:</p> \\[   ES_{\\alpha}(L) = \\frac{1}{\\alpha}\\int_{0}^\\alpha V@R_{s}(L)ds = \\inf \\left\\{ m +\\frac{1}{\\alpha}E[(L-m)^+] \\colon m \\in \\mathbb{R} \\right\\} = V@R_{\\alpha}(L) + \\frac{1}{\\alpha}E\\left. \\] <p>In particular, Expected Shortfall is a cash-invariant and law-invariant risk measure.</p> <p>This remarkable result addresses the key questions about ES: it confirms that ES is a sound risk measure and provides a computationally efficient approach. Instead of directly computing the integral of the quantile (which can be computationally intensive and error-prone), ES can be expressed as the sum of V@R (already an industry standard) and the expected loss beyond V@R, which can be computed easily either using the PDF of \\( L \\) or Monte Carlo methods with importance sampling.</p> Proof <p>The connection between ES and OCE arises from the fact that the quantile function \\( q_L(s) \\) of \\( L \\) shares the same CDF as \\( L \\) itself. Formally, given a loss profile (random variable) \\( L \\) with CDF \\( F_L(m) = P[L \\leq m] \\) and quantile function \\( q_L(s) = \\inf\\{m \\colon F_L(m)\\geq s\\} \\), the quantile \\( q_L(s) \\) can be viewed as a random variable defined on the probability space \\( (\\tilde{\\Omega}, \\tilde{\\mathcal{F}}, \\tilde{P}) \\), where:</p> <ul> <li>\\( \\tilde{\\Omega} = (0,1) \\),  </li> <li>\\( \\tilde{\\mathcal{F}} \\) is the \\( \\sigma \\)-algebra generated by intervals of \\((0,1)\\), and  </li> <li>\\( \\tilde{P} \\) is the Lebesgue measure \\( dx \\) (the measure of interval lengths).</li> </ul> <p>It can be shown that \\( q_L(s) \\) has the same CDF as \\( L \\), i.e., \\( F_{q_L}(m) = F_L(m) \\). Indeed, by the definition of \\( q_L(s) \\):</p> \\[ (0, F_L(m)) \\subseteq \\{s \\colon q_L(s) \\leq m\\} \\subseteq (0, F_L(m)], \\] <p>and under \\( \\tilde{P} \\), these sets yield:</p> \\[     F_L(m) = \\tilde{P}[(0, F_L(m))] \\leq \\tilde{P}[q_L \\leq m] \\leq \\tilde{P}[(0, F_L(m)]] = F_L(m). \\] <p>Therefore, \\( F_{q_L}(m) = F_L(m) \\).</p> <p>Using this fact, and noting that \\( q_L(s) \\geq q_L(1-\\alpha) \\) for \\( s \\geq 1-\\alpha \\):</p> \\[ \\begin{align*}   ES_{\\alpha}(L) &amp; = \\frac{1}{\\alpha} \\int_{0}^\\alpha V@R_{s}ds \\\\     &amp; = \\frac{1}{\\alpha} \\int_{1-\\alpha}^1 q_L(s) ds \\\\     &amp; = q_L(1-\\alpha) + \\frac{1}{\\alpha}\\int_{1-\\alpha}^1 \\left( q_L(s) - q_{L}(1-\\alpha) \\right)ds \\\\     &amp; = V@R_{\\alpha}(L) + \\frac{1}{\\alpha}\\int_{\\mathbb{R}} \\left( m - V@R_{\\alpha}(L) \\right)^+ dF_{q_L}(m) \\\\     &amp; = V@R_{\\alpha}(L) + \\frac{1}{\\alpha}\\int_{\\mathbb{R}} \\left( m - V@R_{\\alpha}(L) \\right)^+ dF_{L}(m) \\\\     &amp; = V@R_{\\alpha}(L) + \\frac{1}{\\alpha}E \\left. \\end{align*} \\] <p>Remark on the Distribution of the Quantile and Random Sampling</p> <p>This kind of magic trick to show the relationship between the piecewise linear optimized certainty equivalent and the expected shortfall relies on the fundamental fact that the distribution of a random variable \\(X\\) on some probability space \\((\\Omega, \\mathcal{F}, P)\\) is the same as the distribution of its quantile \\(q_X\\) on \\((\\tilde{\\Omega}, \\tilde{\\mathcal{F}}, \\tilde{P})\\) where \\(\\tilde{\\Omega} = (0,1)\\), \\(\\tilde{F} = \\mathcal{B}((0,1))\\) the \\(\\sigma\\)-algebra generated by intervals and \\(\\tilde{P}\\) is the lebesgue measure \\(dx\\) that measure interval length, that it \\(\\tilde{P}[(a, b]] = b-a\\).</p> <p>This result is widely known and extensively used, particularly for random sampling. Suppose you want to sample \\( x_1, \\ldots, x_N \\) from the distribution of a random variable \\( X \\) (e.g., normal, Student's t, gamma). A computer, however, generates (quasi-)random numbers \\( u_1, \\ldots, u_N \\) uniformly distributed between \\( 0 \\) and \\( 1 \\). By the equivalence between the distributions of \\( X \\) and \\( q_X \\), defining \\( x_n = q_X(u_n) \\) for \\( n = 1, \\ldots, N \\) produces a random sample \\( x_1, \\ldots, x_N \\) from the distribution of \\( X \\).</p> <pre><code>import numpy as np\nfrom scipy.stats import norm      # (1)\nimport plotly.graph_objs as go    # (2)\n\nN = 10000\nu = np.random.rand(N)             # uniform sample\nx0 = norm.ppf(u)                  # quantile of normal distribution of u\nx1 = norm.rvs(size=N)             # sample from normal\n\n# Plot the two histograms\nfig = go.Figure()\nfig.add_histogram(\n    x=x0,\n    histnorm='probability',\n    name='Quantile of Uniform Sample',\n)\nfig.add_histogram(\n    x=x1,\n    histnorm='probability',\n    name='Standard Normal Sample',\n)\nfig.show()\n</code></pre> <ol> <li>The <code>scipy.stats</code> library provides access to many distributions, including their <code>cdf</code>, <code>pdf</code>, and <code>ppf</code> (quantile function).  </li> <li><code>plotly</code> is used here for plotting; alternatively, <code>matplotlib</code> can be used.</li> </ol> <p>This principle underpins Monte Carlo integration, where the goal is to compute \\( E[f(X)] \\). By the law of large numbers and the central limit theorem, it holds that:</p> \\[     \\frac{1}{N}\\sum_{n=1}^N f(x_n) \\xrightarrow[N \\to \\infty]{} E[f(X)], \\] <p>where \\( x_1, \\ldots, x_N \\) is a random sample from the distribution of \\( X \\). In practice, a random sample \\( u_1, \\ldots, u_N \\) is drawn from a uniform distribution on \\( (0, 1) \\), and then \\( x_n = q_X(u_n) \\) is computed and used in the arithmetic mean of \\( f(x_n) \\) for \\( n = 1, \\ldots, N \\).</p> <p>As of now, we know that Expected Shortfall (ES) is a sound risk measure: it is understandable, implementable, and, due to its representation, efficient to compute. Prior to the introduction of ES, financial institutions commonly computed \\( V@R \\). To transition to ES, they only need to compute the additional term \\( E/\\alpha \\), which is computationally efficient (either analytically or via Monte Carlo methods).</p> <p>The computation of ES in simple cases is demonstrated below:</p> <pre><code>import numpy as np\nfrom scipy.stats import norm, t       # Normal and Student's t distributions\nfrom scipy.optimize import root       # Root finding\nfrom scipy.integrate import quad      # One-dimensional integration\nimport plotly.graph_objs as go        # Plotting library\n\n# Define the basic computation of the quantile (X is a random variable)\ndef quantile(X, s):\n    def fun(m):\n        return X.cdf(m) - s\n    result = root(fun, 0)  # Find the root\n    return result.x[0]\n\n# Compute ES using the integral of quantile representation\ndef ES1(X, alpha):\n    def fun(s):\n        return quantile(X, s)\n    result, err = quad(fun, 1 - alpha, 1)  # Integrate quantile between 1-alpha and 1\n    return result / alpha\n\n# Compute ES using the OCE representation\ndef ES2(X, alpha):\n    var = quantile(X, 1 - alpha)\n    def fun(x):\n        return (x - var) * X.pdf(x)\n    result, err = quad(fun, var, np.Inf)  # Integrate beyond V@R\n    return var + result / alpha\n\n# Define distributions\nX1 = norm\nX2 = t(df=2)  # Student's t distribution with df=2 (variance = 1)\n\nalpha = 0.01  # Confidence level (1%)\n\n# Display results\nprint(f\"\"\"\nV@R (Normal):\\t{quantile(X1, 1 - alpha)}\nES (slow, Normal):\\t{ES1(X1, alpha)}\nES (fast, Normal):\\t{ES2(X1, alpha)}\n\nV@R (Student):\\t{quantile(X2, 1 - alpha)}\nES (slow, Student):\\t{ES1(X2, alpha)}\nES (fast, Student):\\t{ES2(X2, alpha)}\n\"\"\")\n\n# Exercise:\n# Compare and plot the differences between V@R and ES for Normal and Student's t distributions for 0.0001 &lt; alpha &lt; 0.05.\n# Use %timeit to compare the computation times of ES1 and ES2.\n</code></pre> <p>We saw that the expected shortfall has multiple representations, and simple transformations can yield additional formulations.</p> <p>The expected shortfall has the following representations</p> \\[   \\begin{align*}     ES_{\\alpha}(L)  &amp; = \\frac{1}{\\alpha}\\int_0^\\alpha V@R_{s}(L)ds &amp;&amp; \\text{Quantile representation}\\\\                     &amp; = \\inf\\{m + \\frac{1}{\\alpha}E[(L-m)^+]\\colon m \\in \\mathbb{R}\\} &amp;&amp; \\text{OCE representation}\\\\                     &amp; = V@R_{\\alpha}(L) + \\frac{1}{\\alpha}E\\left \\\\                     &amp; = \\frac{1}{\\alpha}\\int_{V@R_{\\alpha}(L)}^\\infty x dF_L(x)   \\end{align*} \\] <p>Furthermore, the expected shortfall is positive homogeneous, that is</p> \\[ ES_{\\alpha}(\\lambda L) = \\lambda ES_{\\alpha}(L)\\] <p>for every \\(\\lambda&gt;0\\). In particular \\(ES_{\\alpha}(L_1 + L_2)\\leq ES_{\\alpha}(L_1) + ES_{\\alpha}(L_2)\\).</p>"},{"location":"material/ex01/","title":"Exercise: One Period Financial Market","text":"<p>Exercise</p> <p>We consider the following two financial market models with state space \\( \\Omega=\\{\\omega_1,\\omega_2, \\omega_3\\} \\) and a probability measure \\( P \\) with</p> \\[     P\\left[ \\{\\omega_i\\} \\right] = p_i \\quad \\text{for} \\quad          \\begin{cases}             p_i  &gt; 0 &amp; \\text{for every } i=1,2,3, \\\\             p_1 + p_2 + p_3 = 1         \\end{cases} \\] <ol> <li> <p>Financial Market I: \\( B_0 = 1 \\) and \\( B_1 = 2 \\) as bank account and three stocks given by</p> \\[     \\begin{align*}         \\boldsymbol{S}_0 &amp; = (S_0^1, S_0^2, S_0^3) = (7, 31, 62)\\\\         \\boldsymbol{S}_1 &amp; =          \\begin{bmatrix}             S^1_1(\\omega_1) &amp; S^2_1(\\omega_1) &amp; S^3_1(\\omega_1) \\\\             S^1_1(\\omega_2) &amp; S^2_1(\\omega_2) &amp; S^3_1(\\omega_2) \\\\             S^1_1(\\omega_3) &amp; S^2_1(\\omega_3) &amp; S^3_1(\\omega_3)         \\end{bmatrix} =          \\begin{bmatrix}             40 &amp; 60 &amp; 120 \\\\             0 &amp; 40 &amp; 80 \\\\             20 &amp; 100 &amp; 200         \\end{bmatrix}     \\end{align*} \\] </li> <li> <p>Financial Market I: \\( B_0 = 1 \\) and \\( B_1 = 1 \\) as bank account and two stocks given by</p> \\[     \\begin{align*}         \\boldsymbol{S}_0 &amp; = (S_0^1, S_0^2) = (8, 10)\\\\         \\boldsymbol{S}_1  &amp;=             \\begin{bmatrix}                 S^1_1(\\omega_1) &amp; S^2_1(\\omega_1) \\\\                 S^1_1(\\omega_2) &amp; S^2_1(\\omega_2) \\\\                 S^1_1(\\omega_3) &amp; S^2_1(\\omega_3)             \\end{bmatrix} =              \\begin{bmatrix}             6 &amp; 11 \\\\             5 &amp; 11 \\\\             12 &amp; 9             \\end{bmatrix}     \\end{align*} \\] </li> </ol> <p>Are these models arbitrage-free? If yes, give all risk-neutral pricing measures. Otherwise, provide an arbitrage strategy.</p> <p>Exercise</p> <p>Given a generic financial market as in the lecture on some probability space \\( (\\Omega, \\mathcal{F},P) \\). Recall that the vector of returns of the financial assets is the random variable given by</p> \\[ \\boldsymbol{R}_1 = \\left( \\frac{S_1^1-S_0^1}{S_0^1}, \\ldots, \\frac{S_1^d-S_0^d}{S_0^d} \\right) \\] <p>Show that the following assertions are equivalent:</p> <ol> <li>The financial market is arbitrage-free;</li> <li> <p>There exists no \\( \\boldsymbol{\\eta}\\) in \\(\\mathbb{R}^d \\) such that</p> \\[     P\\left[ \\boldsymbol{\\eta} \\cdot \\boldsymbol{R}_1 \\geq r \\sum_{k=1}^d \\eta^k \\right]=1 \\quad \\text{and} \\quad P\\left[ \\boldsymbol{\\eta} \\cdot \\boldsymbol{R}_1 &gt; r \\sum_{k=1}^d \\eta^k \\right]&gt;0 \\] </li> <li> <p>For any strategy \\( \\boldsymbol{\\eta}\\) in \\(\\mathbb{R}^d \\), it holds that</p> \\[     P\\left[ \\boldsymbol{\\eta} \\cdot \\Delta \\boldsymbol{X}_1 \\geq 0 \\right]=1 \\quad \\text{implies} \\quad P\\left[ \\boldsymbol{\\eta} \\cdot \\Delta \\boldsymbol{X}_1 = 0 \\right]=1 \\] </li> </ol> <p>Exercise</p> <p>We consider a binomial financial market model with interest rate \\( r \\geq 0 \\),</p> \\[     \\Omega=\\{\\omega^+,\\omega^-\\}, \\quad p:=P(\\{\\omega^+\\})=\\frac{1}{2} \\] <p>and one risky asset with initial value \\( S_0=100 \\) and at time 1,</p> \\[     S_1(\\omega) =          \\begin{cases}             120 &amp; \\text{if } \\omega = \\omega^+ \\\\             90 &amp; \\text{if } \\omega = \\omega^-         \\end{cases} \\] <p>Let \\( C=(S_1-K)^+ \\) be a call option on \\( S \\) with strike price \\( K=100 \\).</p> <ol> <li>For which \\( r \\) is the model arbitrage-free?     For those \\( r \\) for which the model is arbitrage-free, give the risk-neutral pricing measure \\( P^* \\) by finding \\( p^*=P^*(\\{\\omega^+\\}) \\).</li> <li> <p>If you compute the call option's price as the expectation \\( E\\left[\\frac{C}{1+r}\\right] \\) under the objective measure \\( P \\), then there exists an arbitrage in the model.     Show that the risk-free arbitrage gain equals the difference</p> \\[     E\\left[\\frac{C}{1+r}\\right] - E^*\\left[\\frac{C}{1+r}\\right]. \\] </li> <li> <p>For the call option, find a portfolio with start value \\( V_0 \\) and hedging strategy \\( \\eta \\) in \\(\\mathbb{R}\\) such that</p> \\[     \\frac{C}{1+r}=V_0 + \\eta \\Delta X_1 \\] <p>Show that the necessary amount of money to finance this strategy is the risk-neutral price \\( V_0=E^*\\left[\\frac{C}{1+r}\\right] \\).</p> </li> </ol> <p>Exercise: Put/Call Parity</p> <p>On an arbitrage-free financial market, we consider a call and a put</p> \\[ C^{call}=(S_1-K)^+, \\quad \\text{and} \\quad C^{put}=(K-S_1)^+ \\] <p>on the same financial asset \\( S^1 \\) and with the same strike \\( K \\). Show that if \\( \\pi(C^{call}) \\) and \\( \\pi(C^{put}) \\) are fair prices for the call and put respectively, then it has to hold</p> \\[ \\pi(C^{call})=\\pi(C^{put})+S_0-\\frac{K}{1+r} \\] <p>Exercise</p> <p>We consider a financial market with bank account \\( B_0 = 1 \\) and \\( B_1 = 1 + r \\) for some \\( r &gt; -1 \\). We have a single financial asset \\( S \\). We suppose that the financial market is arbitrage-free, and that on this market every call option \\( C(K) = (S_1 - K)^+ \\) is traded for a fair price \\( \\pi(K) \\). Using the \"law of one price,\" compute the prices of the following derivatives:</p> <ol> <li>\\( \\min(S_1, K) \\).</li> <li> <p>\"Butterfly spread\" with payoff \\( f(S_1) \\), whereby \\( f \\) is given by</p> \\[ f(x) =     \\begin{cases}         x-a &amp; \\text{if } a \\leq x \\leq \\frac{a+b}{2}, \\\\         b-x &amp; \\text{if } \\frac{a+b}{2} \\leq x \\leq b, \\\\         0 &amp; \\text{otherwise}     \\end{cases} \\] <p>for some \\( 0 \\leq a \\leq b \\).</p> </li> </ol> <p>Exercise</p> <p>On an arbitrage-free market, we consider a financial asset \\( S \\). A \\( S^2 \\)MART certificate with loss barrier \\( 0 &lt; K_1 \\), strike price \\( K &gt; K_1 \\), and participation rate \\( \\alpha \\) is a certificate given by:</p> <ul> <li>If the financial asset falls below the loss barrier \\( K_1 \\), the certificate pays \\( \\frac{K}{K_1} \\) times the price of the asset at this point.</li> <li>If the financial asset falls between \\( K_1 \\) and \\( K \\), the certificate pays \\( K \\).</li> <li>If the financial asset is above \\( K \\), the certificate pays a portion \\( \\alpha \\) of the asset plus a portion \\( (1-\\alpha) \\) of the strike price.</li> </ul> <p>Given this certificate:</p> <ol> <li>Write the index as a function of \\( S, K_1, K, \\) and \\( \\alpha \\).</li> <li>Show that this certificate can be written as a linear combination of the financial asset and adequate call options.</li> <li>Given \\( K_1, K \\), and the fair price of the call options, determine what should be the participation rate \\( \\alpha \\) such that the fair price of the certificate equals the price of the financial asset.</li> </ol>"},{"location":"material/ex02/","title":"Exercise: Risk Management","text":"<p>Exercise</p> <p>A random variable \\( L \\) is called normally distributed with mean \\( \\mu \\) in \\(\\mathbb{R} \\) and variance \\( \\sigma &gt; 0 \\) if it has a probability density given by</p> \\[   f_{\\mu,\\sigma}(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right) \\] <p>and we use the notation \\( L \\sim \\mathcal{N}(\\mu, \\sigma^2) \\). We denote by</p> \\[ F_{\\mu,\\sigma}(m) = P[L \\leq m] = \\int_{-\\infty}^{m} f_{\\mu,\\sigma}(y) \\, dy, \\quad x \\in \\mathbb{R} \\quad \\text{and} \\quad q_{\\mu,\\sigma}(s) = F^{-1}_{\\mu,\\sigma}(s), \\quad s \\in (0,1) \\] <p>the CDF and quantile of the normal distribution with mean \\( \\mu \\) and variance \\( \\sigma \\). We use the simplified notations for the standard normal:</p> \\[ f = f_{0,1}, \\quad F = F_{0,1}, \\quad q = q_{0,1}. \\] <ol> <li>Show that if \\( L \\sim \\mathcal{N}(0,1) \\), it holds \\( \\mu + \\sigma L \\sim \\mathcal{N}(\\mu, \\sigma^2) \\).</li> <li>Show that if \\( L \\sim \\mathcal{N}(\\mu, \\sigma^2) \\), it holds \\( -L \\sim \\mathcal{N}(-\\mu, \\sigma^2) \\).</li> <li> <p>Show that</p> \\[ f_{\\mu,\\sigma}(x) = \\frac{1}{\\sigma} f\\left( \\frac{x-\\mu}{\\sigma} \\right), \\quad \\sigma^2 f^\\prime_{\\mu,\\sigma} (x) = (\\mu-x)f_{\\mu,\\sigma}(x) \\] \\[ F_{\\mu,\\sigma}(x) = F\\left( \\frac{x-\\mu}{\\sigma} \\right), \\quad q_{\\mu,\\sigma}(s) = \\mu + \\sigma q(s). \\] </li> </ol> <p>Recall that </p> \\[   \\begin{align*}     V@R_{\\alpha}(L) &amp; = \\inf \\{m \\colon P[L&gt;m]\\geq \\alpha\\} = q_L(1-\\alpha)\\\\     ES_{\\alpha}(L) &amp; = \\frac{1}{\\alpha}\\int_0^\\alpha V@R_{\\alpha}(s) ds = \\frac{1}{\\alpha}\\int_{1-\\alpha}^1 q_L(s)ds   \\end{align*} \\] <p>and that both functionals are positive homogoneous.</p> <p>Show that for \\(L\\sim \\mathcal{N}(\\mu, \\sigma^2)\\)</p> \\[     V@R_{\\alpha}(L) = \\mu + \\sigma V@R_{\\alpha}(\\bar{L}) \\quad \\text{and} \\quad ES_{\\alpha}(L) = \\mu + \\sigma ES_{\\alpha}(\\bar{L}) \\] <p>where \\(\\bar{L} \\sim \\mathcal{N}(0, 1)\\) (in other terms, to compute the value at risk or expected shortfal of normal distribution, you just need the V@R and ES of the standard normal distribution).</p> <p>Deduce that for normally distributed random variable \\(L\\) with zero mean, \\(V@R\\) and \\(ES\\) are related through</p> \\[     ES_{\\alpha}(L) = C V@R_{\\alpha}(L) \\] <p>for some constant \\(C\\) which you provide explicitely.</p> <p>Dual Representation</p> <p>We already know that the expected shortfall has two possible representations:</p> \\[   \\begin{align*}      ES_\\alpha(L) &amp; = \\frac{1}{\\alpha} \\int_{1-\\alpha}^{1} q_L(s) ds\\\\                   &amp; = \\inf\\left\\{ m + \\frac{1}{\\alpha} E\\left[ (L-m)^+ \\right] \\right\\}\\\\                   &amp; = q_L(1-\\alpha) + \\frac{1}{\\alpha} E\\left[ \\left( L - q_L(1-\\alpha) \\right)^+ \\right]   \\end{align*} \\] <p>We derive an alternative formulation in terms of duality, namely:</p> \\[ ES_{\\alpha}(L) = \\sup\\left\\{ E^Q[L] \\colon 0 \\leq \\frac{dQ}{dP} \\leq \\frac{1}{\\alpha} \\right\\} \\] <p>This general statement says that the expected shortfall accounts for computing the expected loss under any alternative probability model \\( Q \\) such that \\( Q \\) is not \"too\" far away from \\( P \\) in the sense that the density is bounded by \\( 1/\\alpha \\).</p> <ol> <li> <p>Show that for every \\( x \\) and every \\( y \\) with \\( 0 \\leq y \\leq 1/\\alpha \\) it holds:</p> \\[ \\frac{1}{\\alpha} x^+ \\geq xy \\] <p>In other terms, \\( x^+ / \\alpha = \\sup\\{ xy \\colon 0 \\leq y \\leq 1/\\alpha \\} \\), which is called Fenchel-Moreau duality.</p> </li> <li> <p>Using the fact that \\( E[dQ/dP] = 1 \\), show that if \\( 0 \\leq dQ/dP \\leq 1/\\alpha \\), then it holds:</p> \\[ m + \\frac{1}{\\alpha} E\\left[ (L-m)^+ \\right] \\geq E^Q[L] \\] <p>and deduce that:</p> \\[ ES_{\\alpha}(L) \\geq \\sup\\left\\{ E^Q[L] \\colon 0 \\leq \\frac{dQ}{dP} \\leq \\frac{1}{\\alpha} \\right\\} \\] </li> <li> <p>Assuming that \\( F_L \\) is continuous and strictly increasing, show that if we define the random variable:</p> \\[ \\frac{dQ^\\ast}{dP} = \\frac{1}{\\alpha} 1_{\\{L \\geq q_L(1-\\alpha)\\}} \\] <p>then it defines a probability measure \\( Q^\\ast \\) such that \\( 0 \\leq dQ^\\ast/dP \\leq 1/\\alpha \\) and for which it holds:</p> \\[ ES_{\\alpha}(L) = E^{Q^\\ast}[L] \\] <p>and deduce the duality formula \u2014 for which you now have an explicit \\( Q^\\ast \\) that depends on \\( L \\).</p> </li> </ol>"},{"location":"material/ex03/","title":"Multiperiod Market","text":""},{"location":"material/ex03/#generic","title":"Generic","text":"<p>Exercise</p> <p>We consider a multi-period financial market with a bank account \\(B\\) and one stock \\(S^1 = S\\). Which of the following processes \\(\\eta = (\\eta_t)_{1 \\leq t \\leq T}\\), where \\(\\eta_t : \\Omega \\to \\mathbb{R}\\), are predictable?</p> <ol> <li>\\(\\eta_t = 1_{\\{S_t &gt; S_{t-1}\\}}\\) for \\(t = 1, \\ldots, T\\);</li> <li>\\(\\eta_1 = 1\\) and \\(\\eta_t = 1_{\\{S_{t-1} &gt; S_{t-2}\\}}\\) for \\(t = 2, \\ldots, T\\);</li> <li>\\(\\eta_t = 1_A 1_{\\{t &gt; t_0\\}}\\) for some \\(A \\in \\mathcal{F}_{t_0}\\) and some \\(t_0 \\in \\{0, \\ldots, T-1\\}\\);</li> <li>\\(\\eta_t = 1_{\\{S_t &gt; S_{S_0}\\}}\\);</li> <li>\\(\\eta_1 = 1\\) and \\(\\eta_t = 2 \\eta_{t-1} 1_{\\{S_t &lt; 1\\}}\\) for \\(t \\geq 2\\).</li> </ol> <p>Insider Problem</p> <p>Let \\(Y_1, \\ldots, Y_T\\) be independent identically distributed random variables such that \\(E[Y_t] = 0\\) for every \\(t\\) and not identically constant on some probability space \\((\\Omega, \\mathcal{F}, P)\\). We consider the filtration \\(\\mathcal{F}_0 = \\{\\emptyset, \\Omega\\}\\) and \\(\\mathcal{F}_t = \\sigma(Y_1, \\ldots, Y_t)\\) and the discounted price process:</p> \\[     X_0 := 0, \\quad X_t := X_0 + \\sum_{s=1}^t Y_s. \\] <p>We extend the filtration with the information provided by \\(X_T\\), that is:</p> \\[     \\tilde{\\mathcal{F}}_t = \\sigma(\\mathcal{F}_t, X_T), \\quad t = 0, \\ldots, T \\] <p>This is the information that an insider knows, for whatever reason, about the terminal value of the discounted price at time \\(T\\). We denote the non-insider filtration \\(\\mathbb{F}\\) and the insider filtration \\(\\tilde{\\mathbb{F}}\\).</p> <p>Show that:</p> <ol> <li> <p>If \\(Z_1\\) and \\(Z_2\\) are identically distributed integrable random variables, then it holds</p> \\[   E[Z_1 |\\sigma(Z_1+Z_2)] = \\frac{Z_1 + Z_2}{2} \\] </li> <li> <p>\\(X\\) is a martingale under the filtration \\(\\mathbb{F}\\).     Show that \\(X\\) cannot be a martingale under the insider filtration \\(\\tilde{\\mathbb{F}}\\).     However, the process:</p> \\[     \\tilde{X}_0 = X_0, \\quad \\text{and} \\quad \\tilde{X}_t = X_t - \\sum_{s=0}^{t-1} \\frac{X_T - X_s}{T - s}, \\quad t = 1, \\ldots, T \\] <p>is a martingale under \\(\\tilde{\\mathbb{F}}\\).</p> </li> <li> <p>With the information about the terminal discounted value \\(X_T\\) of the stock, it is possible to realize arbitrage gains.   Find a strategy \\(\\eta\\), predictable with respect to \\(\\tilde{\\mathbb{F}}\\), such that among all other strategies \\(\\mu\\), predictable with respect to \\(\\tilde{\\mathbb{F}}\\) with \\(|\\mu_t| \\leq 1\\), the expected gain \\(E[G_T]\\) is maximal, where:</p> \\[     G_T = \\sum_{s=1}^T \\eta_s \\Delta X_s. \\] </li> </ol> <p>Exercise</p> <p>In an arbitrage-free market with a bank account:</p> \\[       B_t = (1 + r)^t, \\quad r &gt; -1, \\, t = 0, \\ldots, T   \\] <p>Let \\(C_t^{call}\\) and \\(C_t^{put}\\) be the price processes of a call and put on a self-financed portfolio \\(V\\) with strike \\(K\\), that is:</p> \\[     C^{call} = (V_T - K)^+ \\quad \\text{and} \\quad C^{put} = (K - V_T)^+. \\] <p>Show the put-call parity:</p> \\[     C_t^{call} - C_t^{put} = V_t - K(1 + r)^{t-T}, \\quad t = 0, \\ldots, T.   \\]"},{"location":"material/ex03/#american-option","title":"American Option","text":"<p>Exercise</p> <p>Let \\(H\\) be the discounted price of an American option in a complete financial market, and denote by \\(U\\) the Snell envelope. Denote by \\(\\tilde{H} := H_T\\) the corresponding European option whose price process is given by:</p> \\[     V_t = E_{P^\\ast}\\left[ H_T \\mid \\mathcal{F}_t \\right], \\quad t = 0, \\ldots, T \\] <p>Show that:</p> <ol> <li>\\(U_t \\geq V_t\\) for every \\(t\\).</li> <li>If \\(V_t \\geq H_t\\) for all \\(t\\), then \\(U_t = V_t\\) for every \\(t\\).</li> <li>If \\(H\\) is a \\(P^\\ast\\)-sub-martingale, then \\(U = V\\).</li> <li>Show that if \\(H_t = f(X_t)\\) for some convex function \\(f\\) for every \\(t\\), for instance, an American call option, then \\(H\\) is a \\(P^\\ast\\)-sub-martingale.</li> </ol> <p>Exercise (Difficult)</p> <p>In a multi-period arbitrage-free and complete financial market with one stock \\(S\\), consider the following American put option:</p> \\[     H_t^K = \\frac{(K - S_t)^+}{(1 + r)^t}, \\quad t = 0, 1, \\ldots, T \\] <p>where \\(r &gt; 0\\) and \\(K\\) is a positive strike. We denote by \\(\\tau^K_{\\min}\\) the smallest optimal stopping time for the buyer, that is:</p> \\[     \\tau^K_{\\min}(\\omega) = \\inf\\{t : U_t^K(\\omega) = H_t^K(\\omega)\\}, \\] <p>where \\(U^K\\) is the corresponding Snell envelope of \\(H^K\\).</p> <p>Show that:</p> <ol> <li>\\(\\tau^K_{\\min} \\geq \\tau^{K^\\prime}_{\\min}\\) whenever \\(K \\leq K^\\prime\\).</li> <li>For \\(K\\) large enough, show that ultimately \\(\\tau^K_{\\min} = 0\\).</li> </ol>"},{"location":"material/ex03/#default-probability","title":"Default Probability","text":"<p>Exercise: First part is easy, second part less so</p> <p>Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space, \\(Y_1, Y_2, \\ldots\\) a sequence of independent identically distributed random variables taking values \\(\\pm 1\\) and such that:</p> \\[   P[Y_t = 1] = p, \\quad \\text{and} \\quad P[Y_t = -1] = 1 - p = q \\] <p>for \\(1 &gt; p \\geq 1/2\\). Consider the filtration \\(\\mathcal{F}_0 = \\{\\emptyset, \\Omega\\}\\) and \\(\\mathcal{F}_t = \\sigma(Y_1, \\ldots, Y_t)\\), \\(t \\geq 1\\). The random walk \\(Z\\) is given by:</p> \\[   Z_0 = 0 \\quad \\text{and} \\quad Z_t = \\sum_{s=1}^t Y_s, \\quad t \\geq 1. \\] <p>Finally, for two integers \\(a\\) and \\(b\\), we define the stopping time:</p> \\[   \\tau_a = \\inf\\{ t : Z_t = a \\}, \\quad \\tau_b = \\inf\\{ t : Z_t = -b \\}, \\quad \\text{and} \\quad \\tau = \\tau_a \\wedge \\tau_b. \\] <p>You can assume that for any value of \\(a\\) and \\(b\\), it holds \\(\\tau(\\omega) &lt; \\infty\\) for almost all \\(\\omega\\).</p> <ol> <li>Since \\(1 &gt; p \\geq 1/2\\), show that \\(Z\\) is a sub-martingale and a martingale if and only if \\(p = 1/2\\).</li> <li> <p>Suppose that \\(p = 1/2\\), and show that for every time horizon \\(T\\), it holds:</p> \\[       E\\left[ Z_{T \\wedge \\tau} \\right] = 0 \\] <p>Deduce that:</p> \\[       P[Z_\\tau = a] = \\frac{b}{a + b}. \\] </li> <li> <p>Always for \\(p = 1/2\\), show that the process \\(Z^2_t - t\\) is a martingale.    Show that \\(E[Z_\\tau^2 - \\tau] = 0\\), and using the previous point, show that the expected time to reach \\(a\\) or \\(-b\\) for the first time is given by:</p> \\[ E\\left[ \\tau \\right] = ab. \\] </li> <li> <p>For \\(1 &gt; p &gt; 1/2\\), show that the process:</p> \\[ M_0 = 1, \\quad M_t = \\left( \\frac{q}{p} \\right)^{Z_t}, \\quad t = 1, 2, \\ldots \\] <p>is a martingale.</p> </li> <li> <p>For \\(1 &gt; p &gt; 1/2\\), show that:</p> \\[ P\\left[ Z_\\tau = a \\right] = \\frac{(q/p)^b - 1}{(q/p)^{a + b} - 1}. \\] </li> </ol> <p>From now on, suppose that \\(p &gt; 1/2\\) and for \\(\\lambda \\in \\mathbb{R}\\), define:</p> \\[       \\phi(\\lambda) = \\ln(E[\\exp(\\lambda Y_t)]) = \\ln(E[\\exp(\\lambda Y_1)]) = \\ln(pe^{\\lambda} + qe^{-\\lambda})     \\] <p>as well as the process \\(L(\\lambda)\\) by:</p> \\[       L_t(\\lambda) = \\exp\\left( \\lambda Z_t - t \\phi(\\lambda) \\right), \\quad t = 0, 1, 2, \\ldots     \\] <ol> <li> <p>Show that:</p> \\[ \\phi^\\prime(0) = E\\left[ Y_1 \\right] \\] <p>and (with differentiation done with respect to \\(\\lambda\\)!)</p> \\[ L^\\prime_t := \\frac{d L_t}{d\\lambda}(0) = Z_t - t \\phi^\\prime(0). \\] </li> <li> <p>Show that \\(L(\\lambda)\\) is a martingale.    Sketch the reason why the process \\(L^\\prime\\) is then also a martingale.</p> </li> <li> <p>From the two previous points, show that:</p> \\[ E\\left[ Z_{t \\wedge \\tau} - t \\wedge \\tau \\phi^\\prime(0) \\right] = 0, \\quad \\text{for every time } t. \\] </li> <li> <p>From equation (4), deduce that the average time before reaching either \\(a\\) or \\(-b\\) is given by:</p> \\[ E[\\tau] = \\frac{a + b}{p - q} \\frac{(q/p)^b - 1}{(q/p)^{a + b} - 1} - \\frac{b}{p - q}. \\] </li> </ol>"},{"location":"material/ex_math/","title":"Exercises: Probability and Stochastic Processes","text":""},{"location":"material/ex_math/#probability","title":"Probability","text":"<p>Remark</p> <p>For the Radon Nykodym theorem, the notion of absolute continuity and equivalence between probability measure is central.</p> <p>Definition</p> <p>Given two probability measure \\(P\\) and \\(Q\\) we say that</p> <ol> <li> <p>\\(Q\\) is absolutely continuous with respect to \\(P\\) and denote \\(Q\\ll P\\) if </p> \\[ P[A] = 0 \\quad \\text{implies} \\quad Q[A] = 0\\] </li> <li> <p>\\(Q\\) is equivalent to \\(P\\) and denote \\(Q\\sim P\\) if both \\(Q\\ll P\\) and \\(P\\ll Q\\).     That is</p> \\[ P[A] = 0 \\quad \\text{if and only if} \\quad Q[A] = 0\\] </li> </ol> <p>By definition it clearly holds that</p> \\[     Q \\ll P \\quad \\text{if and only if} \\quad P[A] = 1 \\, \\text{implies}\\, Q[A] = 1 \\] <p>or</p> \\[     Q \\ll P \\quad \\text{if and only if} \\quad Q[A] &gt; 0 \\, \\text{implies}\\, P[A] &gt; 0 \\] <p>and in the equivalent case</p> \\[     Q \\sim P \\quad \\text{if and only if} \\quad P[A] = 1 \\, \\text{if and only if}\\, Q[A] = 1 \\] <p>or</p> \\[     Q \\sim P \\quad \\text{if and only if} \\quad P[A] &gt; 0 \\, \\text{if and only if}\\, Q[A] &gt; 0 \\] <p>Exercise</p> <p>The Radon-Nykodym theorem states that if a probability measure \\( Q \\ll P \\), then there exists a (\\( P \\)-almost surely) unique random variable \\(Z\\) such that</p> \\[   \\begin{equation*}     \\begin{cases}       Z &amp; \\geq 0 \\\\       E^P[Z] &amp; = 1 \\\\       E^Q[X] &amp; = E^P[Z X] \\quad \\text{ for any positive bounded random variable }X     \\end{cases}   \\end{equation*} \\] <p>This unique random variable is called the density of \\(Q\\) with respect to \\(P\\) and denoted by \\(dQ/dP\\).</p> <p>This density allows to compute expectation of random variable under \\(Q\\) in terms of expectation under \\(P\\). If \\(Q\\sim P\\) then \\(dQ/dP\\) is strictly positive and \\(dP/dQ = (dQ/dP)^{-1}\\).</p> <p>This fundamental theorem is complex to prove in the general case, relying on other fundamental theorems of functional analysis. However, you can prove it easily in the finite state setting.</p> <p>Let \\( \\Omega=\\{\\omega_1,\\ldots,\\omega_n\\} \\) be a finite state space with \\( \\sigma \\)-algebra \\( \\mathcal{F}=2^\\Omega \\) and probability measure \\( P \\) given by the vector \\( \\boldsymbol{p}=(p_1,\\ldots,p_n) \\) where \\( P[\\{\\omega_i\\}]=p_i&gt;0 \\) for every \\( i \\) and \\( \\sum p_i=1 \\).</p> <p>Let now \\( Q \\) be another probability measure on \\( (\\Omega,\\mathcal{F}) \\) given by the vector \\( \\boldsymbol{q}=(q_1,\\ldots,q_n) \\) where \\( Q[\\{\\omega_i\\}]=q_i\\geq 0 \\) and \\( \\sum q_i=1 \\). Since \\( P[A]=0 \\) implies \\( A=\\emptyset \\), it follows that \\( Q[A]=Q[\\emptyset]=0 \\). Hence, \\( Q \\) is absolutely continuous with respect to \\( P \\).</p> <p>Find a random variable \\( \\frac{dQ}{dP}:\\Omega \\to \\mathbb{R} \\) such that \\( \\frac{dQ}{dP}\\geq 0 \\), \\( E_P[\\frac{dQ}{dP}]=1 \\), and </p> \\[ E_Q[X]=E_P\\left[ \\frac{dQ}{dP}X \\right] \\] <p>for every random variable \\( X:\\Omega\\to \\mathbb{R} \\). Show that \\( \\frac{dQ}{dP} \\) is also unique.</p> <p>Note that since it is a finite setting, the random variable \\(dQ/dP\\) can be represented by an \\( n \\)-dimensional vector \\(\\boldsymbol{z} = (z_1, \\ldots, z_n)\\) with \\(z_i = dQ/dP(\\omega_i)\\). The conditions therefore translate into finding such a vector \\(\\boldsymbol{z}\\) with \\(z_i \\geq 0\\), \\(\\sum z_i p_i = 1\\) and such that for every vector \\(\\boldsymbol{x}=(x_1, \\ldots, x_n)\\) it holds</p> \\[   \\sum x_i q_i =E^Q[X] = E\\left[ \\frac{dQ}{dP}X \\right] = \\sum x_i z_i p_i \\] <p>Exercise</p> <p>Let \\( (\\Omega, \\mathcal{F}, P) \\) be a probability space. Given a positive random variable \\( X \\), we define </p> \\[ A = \\left\\{ X &gt; 0 \\right\\} := \\left\\{ \\omega \\colon X(\\omega) &gt; 0 \\right\\} \\quad \\text{and} \\quad A_n = \\left\\{ X &gt; \\frac{1}{n} \\right\\}, \\quad n \\in \\mathbb{N}. \\] <p>Show that:</p> <ol> <li>\\( A_n \\subseteq A_{n+1} \\) and \\( \\cup_{k \\leq n} A_k = A_n \\nearrow A \\).</li> <li> <p>\\( P[A_n] \\nearrow P[A] \\).</p> <p>Hint: Show that the sequence of events \\( B_1 = A_1 \\), \\( B_2 = A_2 \\setminus A_1 \\), \\( B_3 = A_3 \\setminus A_2 \\), \\( \\ldots \\) is such that:</p> \\[ B_k \\cap B_j = \\emptyset \\text{ for } k \\neq j, \\quad \\cup_{k=1}^n B_k = A_n, \\quad \\text{and} \\quad \\cup_{k=1}^\\infty B_k = A, \\] <p>and conclude with the property of a probability measure which implies:</p> \\[ P\\left[ \\cup_{k=1}^n B_k \\right] = \\sum_{k=1}^{n} P\\left[ B_k \\right] \\nearrow \\sum_{k=1}^\\infty P[B_k] = P\\left[\\cup_{k=1}^\\infty B_k \\right]. \\] </li> <li> <p>Show that \\( X \\leq Y \\) implies \\( E[X] \\leq E[Y] \\), and deduce:</p> \\[ \\frac{1}{n} P\\left[ A_n \\right] \\leq E\\left[ X 1_{A_n} \\right] \\leq E\\left[ X \\right]. \\] </li> <li> <p>Deduce that if \\( X \\geq 0 \\) and \\( E[X] = 0 \\), then \\( P[X &gt; 0] = 0 \\).</p> </li> <li>Deduce that if \\( X \\geq 0 \\) and \\( P\\left[ X &gt; 0 \\right] &gt; 0 \\), then \\( E\\left[ X \\right] &gt; 0 \\).</li> </ol> <p>Exercise</p> <p>Let \\( (\\Omega, \\mathcal{F}, P) \\) be a probability space. Let further \\( (A_n) \\) be a sequence of pairwise disjoint elements\\footnote{That is \\( A_n \\cap A_m = \\emptyset \\) for every \\( n \\neq m \\).} of \\( \\mathcal{F} \\) such that \\( P[A_n] &gt; 0 \\) for every \\( n \\). Define \\( \\mathcal{G} = \\sigma(A_n \\colon n) \\), the \\( \\sigma \\)-algebra generated by the sequence \\( (A_n) \\). That is,</p> \\[ A \\in \\mathcal{G} \\quad \\text{if and only if} \\quad A = \\cup_{i \\in I} A_i, \\quad I \\subseteq \\mathbb{N}. \\] <p>Show that:</p> <ol> <li> <p>For every \\( B \\in \\mathcal{F} \\), it holds</p> \\[ P\\left[ B|\\mathcal{G} \\right] := E\\left[ 1_B |\\mathcal{G} \\right] = \\sum P\\left[ B | A_n \\right] 1_{A_n} \\] <p>where \\( P[B|A_n] := \\frac{P[B \\cap A_n]}{P[A_n]} \\).</p> </li> <li> <p>For every \\( X \\), a bounded random variable, it holds</p> \\[ E\\left[ X|\\mathcal{G} \\right] = \\sum \\frac{E\\left[ 1_{A_n} X \\right]}{P[A_n]} 1_{A_n}. \\] </li> </ol>"},{"location":"material/ex_math/#stochastic-processes","title":"Stochastic Processes","text":"<p>Exercise</p> <ol> <li> <p>Let \\( X \\) and \\( Y \\) be two identically distributed random variables. Show that</p> \\[ E\\left[ X \\big| \\sigma(X+Y) \\right] = \\frac{X+Y}{2}. \\] <p>Hint: Note that \\( E[X+Y | \\sigma(X+Y)] = X+Y \\).</p> </li> <li> <p>Let \\( X = (X_t)_{0 \\leq t \\leq T} \\) be a martingale on a probability space \\( (\\Omega, \\mathcal{F}, P) \\) with a filtration \\( \\mathbb{F} = (\\mathcal{F}_t)_{0 \\leq t \\leq T} \\). Show that \\( E[X_s|\\mathcal{F}_t]=X_t \\) for every \\( 0 \\leq t \\leq s \\leq T \\).</p> </li> <li> <p>Let \\( Y_1, \\ldots, Y_T \\) be independent random variables on a probability space \\( (\\Omega, \\mathcal{F}, P) \\) with \\( Y_t &gt; 0 \\) \\( P \\)-almost surely and \\( E[Y_t]=1 \\) for every \\( t \\). Show that</p> \\[ X_0 = 1, \\quad X_t = \\prod_{s=1}^t Y_s, \\quad t = 1, \\ldots, T \\] <p>is a martingale with respect to the filtration \\( \\mathcal{F}_0 = \\{\\emptyset, \\Omega\\} \\) and \\( \\mathcal{F}_t = \\sigma(Y_1, \\ldots, Y_t) \\).</p> </li> <li> <p>Let \\( Y_1, \\ldots, Y_t \\) be independent random variables such that \\( Y_t \\sim \\mathcal{N}(0,1) \\) on some probability space \\( (\\Omega, \\mathcal{F}, P) \\). Consider the filtration \\( \\mathcal{F}_0 = \\{\\emptyset, \\Omega\\} \\) and \\( \\mathcal{F}_t = \\sigma(Y_1, \\ldots, Y_t) \\). We consider the price process</p> \\[ S_0 &gt; 0, \\quad S_t = S_0 \\exp\\left( \\sum_{s=1}^t \\left(\\sigma Y_s + \\mu\\right) \\right) \\] <p>where \\( \\sigma, \\mu \\) are constants such that \\( \\sigma &gt; 0 \\). Let further the bank account be</p> \\[ B_t = (1 + r)^t. \\] <p>For which values of \\( \\mu \\) is the discounted price process</p> \\[ X_t = \\frac{S_t}{B_t} \\] <p>a martingale?</p> <p>Hint: Note that if \\( Z \\sim \\mathcal{N}(0, \\sigma^2) \\), then it holds that \\( E[e^Z] = e^{\\sigma^2 / 2} \\).</p> </li> </ol> <p>Exercise: Measure Change</p> <p>Consider a probability space \\( (\\Omega, \\mathcal{F}, P) \\) and a filtration \\( \\{\\emptyset, \\Omega\\} = \\mathcal{F}_0 \\subseteq \\mathcal{F}_1 \\subseteq \\cdots \\subseteq \\mathcal{F}_T \\).</p> <p>Let \\( Q \\) be a probability measure equivalent to \\( P \\), and we denote by \\( Z = dQ / dP \\) its density, that is, the unique positive integrable random variable with expectation \\( 1 \\) such that for any \\( Q \\)-integrable random variable \\( H \\) it holds</p> \\[ E^Q\\left[ H \\right] = E\\left[ Z H \\right] \\] <p>We further denote by</p> \\[ Z_t = E\\left[ Z \\, | \\, \\mathcal{F}_t \\right], \\quad t = T, T-1, \\ldots, 0 \\] <p>the conditional density.</p> <p>Show that:</p> <ul> <li>\\( Z_t \\) is a positive random variable with expectation \\( 1 \\). It defines therefore a probability measure \\( Q_t \\sim P \\) on \\( \\mathcal{F}_t \\).</li> <li>Show that the stochastic process \\( Z = Z_0, Z_1, \\ldots \\) is a \\( P \\)-martingale.</li> <li> <p>Show that for any \\( Q \\)-integrable random variable \\( H \\) it holds</p> \\[ E^Q\\left[ H \\big| \\mathcal{F}_t \\right] = \\frac{1}{Z_t} E^P\\left[ Z H \\big| \\mathcal{F}_t \\right] \\] </li> <li> <p>Let \\( M = M_0, M_1, \\ldots \\) be an adapted and \\( Q \\)-integrable stochastic process. Show that \\( M \\) is a \\( Q \\)-martingale if and only if \\( Z M \\) is a \\( P \\)-martingale.</p> </li> </ul>"},{"location":"material/probability/","title":"Probability","text":""},{"location":"material/probability/#probability-space","title":"Probability Space","text":"<p>In probability, we consider:</p> <ul> <li>A state space \\( \\Omega \\) of states \\( \\omega \\in \\Omega \\):      Description of possible states of an outcome for which there is uncertainty.</li> <li>Events \\( A \\subseteq \\Omega \\) as a collection of states that can happen.     The family of all considered events \\( A \\subseteq \\Omega \\) is denoted by \\( \\mathcal{F} \\).</li> </ul> <p>Examples</p> <ol> <li> <p>Coin Flipping:</p> <ul> <li> <p>State space: \\( \\Omega = \\{H, T\\} \\), where \\( H \\) and \\( T \\) denote the states \"Head occurs\" and \"Tail occurs\" as the possible outcomes of throwing a coin.</p> </li> <li> <p>Events: \\( A = \\{H\\} \\) is the event that head will occur.</p> </li> </ul> </li> <li> <p>Temperature tomorrow:</p> <ul> <li> <p>State space: \\( \\Omega = \\mathbb{R} \\), where \\( x \\in \\Omega \\) represents the possible temperature at 8:00 am tomorrow.</p> </li> <li> <p>Events: \\( A = [13,19] \\) is the event that tomorrow at 8:00 am, the temperature will lie between \\( 13 \\) and \\( 19 \\) degrees.</p> </li> </ul> </li> <li> <p>Financial decision: </p> <ul> <li> <p>State space: \\( \\Omega = [-1,10]^2 \\), where for \\( (x, y) \\in \\Omega \\), \\( x \\) and \\( y \\) represent the interest rates that the central banks of the USA and EU, respectively, will fix next month.</p> </li> <li> <p>Events: \\( A = [0.25,0.75] \\times [0.9,1.8] \\cup \\{1\\} \\times [1.7,2.1] \\) is the event that next month the USA fixes an interest rate between \\( 0.25\\% \\) and \\( 0.75\\% \\) while the EU fixes one between \\( 0.9\\% \\) and \\( 1.8\\% \\), OR the USA fixes an interest rate of \\( 1\\% \\) while the EU fixes one between \\( 1.7\\% \\) and \\( 2.1\\% \\).</p> </li> </ul> </li> <li> <p>Texas Holdem:</p> <p>For Texas Holdem, we have 52 cards deck \\(D\\) with cards  ,  ,  , ...</p> <p>After pre-flop, flop, turn and river (if you're still there), you have to choose 5 best cards out of the best combinations you can get from the 5 on the table and the two in your hand.</p> <ul> <li> <p>State space: \\( \\Omega = \\{\\{c_1, c_2, c_3, c_4, c_5\\} \\colon c_i \\in D, \\text{ and } c_i \\neq c_j\\} \\).   Note here the notation in terms of set, since the order does not count. Furthermore, each card is different since distributing occurs without replacement.</p> </li> <li> <p>Event: The event \\(A\\) that I have a royal flush corresponds to \\(A\\) containing the elements \\(\\{\\) , , , , \\(\\}\\), \\(\\{\\) , , , , \\(\\}\\), \\(\\{\\) , , , , \\(\\}\\), \\(\\{\\) , , , , \\(\\}\\).</p> </li> </ul> </li> </ol> <p>Events are supposed to be measured afterwards, however we require some structure among events. We want to speak about the occurence of one or the other event, two events happening coincidentally, or an event not happening. Therefore, the definition of measurable space</p> <p>Measurable Space</p> <p>A measurable space is a tuple \\( (\\Omega, \\mathcal{F}) \\), where </p> <ul> <li>\\(\\Omega\\) is a set (state space)</li> <li>\\(\\mathcal{F}\\) is an algebra of subsets of \\(\\Omega\\) (Events)</li> </ul> <p>An algebra is a collection of sets satisfying the following properties</p> <ul> <li>\\( \\emptyset \\) (nothing happens) and \\( \\Omega \\) (anything can happen) are events.</li> <li>If \\(A\\) is an event, then so is \\(A^c\\);</li> <li>If \\(A\\) and \\(B\\) are events, then \\(A\\cup B\\) is an event (the event that \\(A\\) or \\(B\\) happen is itself an event)</li> </ul> <p>Warning</p> <p>Note that this is the intuitive definition of a measurable space, but for mathematical reason, we require the algebra of events \\(\\mathcal{F}\\) to be \\(\\sigma\\)-stable, that is instead of requiring union of two or finitely many events to be an events, we also require </p> <p>If the state space \\(\\Omega\\) is finite or countable, the classical assumption is to consider as algebra of events the power set \\(2^{\\Omega}\\) which is the collection of any subsets. If the state space is infinite, such as \\(\\mathbb{R}\\), the power set would be truly large and leading to mathematical issues. In the case of \\(\\mathbb{R}\\) for instance, the measurable sets are those generated by intervals.</p> <p>Proposition</p> <p>The third assumption for an algebra is equivalent to replace by </p> <ul> <li>If \\(A\\) and \\(B\\) are events, then \\(A\\cap B\\) is an event.</li> </ul> <p>Proof</p> <p>Let \\(A\\) and \\(B\\) be event, from the second assumption it follows follows that \\(A^c\\). Now the equivalence between the two assertion (intersection vs union) follows from Morgan's rule</p> \\[     A\\cap B = (A^c \\cup B^c)^c \\quad \\text{and}\\quad A\\cup B = (A^c \\cap B^c)^c \\] <p>Examples</p> <p>Here are some classical exmaples we will see throughout the lecture.</p> <ul> <li> <p>Coin toss:</p> <ul> <li>State Space: \\(\\Omega = \\{-1, 1\\}\\) two states for head and tail</li> <li>Events: \\(\\mathcal{F} = 2^\\Omega = \\{\\emptyset, \\Omega, \\{1\\}, \\{-1\\}\\}\\)</li> </ul> <p>There are here exactly \\(2^2 =4\\) events.</p> </li> <li> <p>Finite state space:</p> <ul> <li>State Space: \\(\\Omega = \\{\\omega_1, \\ldots, \\omega_N\\}\\)</li> <li>Events: \\(\\mathcal{F} = 2^\\Omega\\)</li> </ul> <p>There are here exactly \\(2^{\\#\\Omega} = 2^N\\) events (already with \\(N\\) beyond 100 this is more than a computer can take).</p> </li> <li> <p>Random Walk:</p> <p>The random walk consists to draw a coin several times in a row, recording every single result.</p> <ul> <li>State Sapce: \\(\\Omega = \\{\\omega = (\\omega_1, \\ldots, \\omega_T)\\colon \\omega_i = \\pm 1\\}\\) where each state is the sequence of results of the coin toss.</li> <li>Events: \\(\\mathcal{F}=2^\\Omega\\).</li> </ul> <p>As above, the cardinality of \\(\\mathcal{F}\\) is equal to \\(2^{\\# \\Omega}\\). However there are \\(2^N\\) possible sequences, and so the cardinality of events is equal to \\(2^{2^N}\\). You can imagine that for small \\(N\\) this size is already gigantic.</p> </li> </ul>"},{"location":"material/probability/#random-variables","title":"Random Variables","text":"<p>Aside from being able to measure events, we also want to know how to measure the events that a function of this state satisfies. For instance, in the case of the coin toss, suppose that you play a game where if head you win 100 and if tail you lose everything. As a function of the state it writes as \\(X \\colon \\Omega \\to \\mathbb{R}\\) where \\(X(\\omega) = 100\\) if \\(\\omega = 1\\) and \\(X(\\omega) = 0\\) otherwize. We want to be able to speak about the event \\(A\\) you strictly win something which clearly if \\(\\{1\\}\\). In the general case we define random variables as such functions where you can measure this function to reach some certain level.</p> <p>Definition</p> <p>Let \\( (\\Omega, \\mathcal{F}) \\) be a measurable space. A function</p> \\[   \\begin{equation*}     \\begin{split}       X\\colon \\Omega \\longrightarrow \\mathbb{R}\\\\           \\omega &amp; \\longmapsto X(\\omega)     \\end{split}   \\end{equation*} \\] <p>is called a random variable if for every level \\(x\\), the set</p> \\[     A = \\left\\{ \\omega \\in \\Omega \\colon X(\\omega)\\leq x \\right\\}:= \\{X\\leq x\\} \\] <p>is an event, that is \\(A \\in \\mathcal{F}\\).</p> <p>The fact that we require the event smaller than some value seems arbitrary, however, since we have a (\\(\\sigma\\))-algebra this is quite general</p> <p>Proposition</p> <p>It is equivalent for \\( X: \\Omega \\to \\mathbb{R} \\) to be a random variable to require:</p> <ol> <li>\\( \\{X &gt; x\\} \\in \\mathcal{F} \\) for any \\( x \\).</li> <li>\\( \\{X &lt; x\\} \\in \\mathcal{F} \\) for any \\( x \\).</li> <li>\\( \\{X \\geq x\\} \\in \\mathcal{F} \\) for any \\( x \\).</li> <li>\\( \\{x \\leq(&lt;) X \\leq(&lt;) y\\} \\in \\mathcal{F} \\) for any \\( x \\leq y \\).</li> </ol> Proof <ol> <li>Follows from \\( \\{X &gt; x\\} = \\{X \\leq x\\}^c \\), and \\( \\mathcal{F} \\) is closed under complementation.</li> <li>\\( \\{X &lt; x\\} = \\cap_{n} \\{X \\leq x +1/n\\} \\), and \\( \\mathcal{F} \\) is closed under countable union.</li> </ol> <p>The other assertions follows similar argumentations.</p> <p>This definition is compatible with many of the standard operations. In other terms the sum, product, composition with continuous function of random variables remain random variables.</p> <p>Proposition</p> <p>Let \\( X \\) be a random variable and \\( f:\\mathbb{R}\\to \\mathbb{R} \\) be a continuous function. Then</p> \\[   \\begin{equation*}     \\begin{split}       Y\\colon \\Omega &amp;\\longrightarrow \\mathbb{R}\\\\       \\omega &amp; \\longmapsto Y(\\omega) = f(X(\\omega))     \\end{split}   \\end{equation*} \\] <p>is a random variable denoted \\( Y = f(X) \\).</p> <p>Let \\( X, Y \\) be random variables as well as \\( (X_n) \\) be a converging sequence of random variables. The following are random variables:</p> <ul> <li>\\( aX + bY \\) for every \\( a, b \\in \\mathbb{R} \\);</li> <li>\\( XY \\);</li> <li>\\( \\max(X, Y) \\) and \\( \\min(X, Y) \\);</li> <li>\\( \\sup X_n \\) and \\( \\inf X_n \\);</li> <li>\\( \\lim X_n \\).</li> </ul> Proof <p>The first part of the proof is not trivial and has to do with topology as well as the definition of continuous functions. The argument goes as follows, for \\(y\\) in \\(\\mathbb{R}\\), the set \\(F = \\{x \\in \\mathbb{R}\\colon f(x) \\leq y\\}\\) is a close set since \\(f\\) is continuous (lower semi-continuous would be enought). Now it is possible to show following the previous proposition that if \\(X\\) is a random variable, then \\(\\{X \\in F\\}\\) is an event since \\(F\\) is closed. It follows that</p> \\[   \\begin{align*}     \\{Y \\leq y\\} &amp; = \\left\\{\\omega \\in \\Omega\\colon f(X(\\omega))\\leq y\\right\\}\\\\                   &amp; = \\left\\{ \\omega \\in \\Omega \\colon X(\\omega) \\in \\{x \\in \\mathbb{R}\\colon f(x)\\leq y\\} \\right\\}\\\\                   &amp; = \\left\\{ X \\in F \\right\\} &amp;&amp; \\text{which is an event.}   \\end{align*} \\] <p>For the other three points, it follows from the continuity of functions. For the \\(\\sup\\) and \\(\\inf\\), it follows from \\(\\{\\sup X_n \\leq x\\} = \\cap \\{X_n \\leq x\\}\\) and \\(\\{\\inf X_n &lt;x\\} = \\cup \\{X_n &lt;x\\}\\), with similar argumentation of the limit of converging sequence of random variables.</p> <p>If you are interested, you can ask for lecture notes on probability.</p> <p>Example: Indicator and Simple Random Variables</p> <p>We turn to the most simple yet one of the most important example of random variable in probability.</p> <ul> <li> <p>Indicator Function</p> <p>Definition</p> <p>Let \\( (\\Omega, \\mathcal{F}) \\) be a measurable space and let \\( A \\in \\mathcal{F} \\) be an event. The function</p> \\[ \\begin{equation*} \\begin{split}   1_A \\colon &amp; \\Omega \\longrightarrow \\mathbb{R}\\\\   \\omega &amp; \\longmapsto 1_A(\\omega) =       \\begin{cases}        1 &amp; \\text{if } \\omega \\in A, \\\\        0 &amp; \\text{if } \\omega \\notin A      \\end{cases} \\end{split} \\end{equation*} \\] <p>is called the indicator function of \\( A \\).</p> <p>Exercise</p> <p>The indicator function \\(1_A\\) of an event \\(A\\) is a random variable. Indeed, let \\(x\\) be in \\(\\mathbb{R}\\). It follows that</p> \\[   \\{1_A \\leq x\\} =    \\begin{cases}   \\emptyset &amp; \\text{if } x&lt;0\\\\   A^c &amp; \\text{if }0\\leq x &lt;1 \\\\   \\Omega &amp; \\text{if }x \\geq 1   \\end{cases} \\] </li> <li> <p>Plot</p> <p> </p> </li> </ul> <p>This definition is strongly related to a table of truth: \\( 1 \\) for true, \\( 0 \\) for false. Clearly \\( 1_{\\emptyset} = 0 \\) and \\( 1_{\\Omega} = 1 \\). Show that:</p> <ol> <li>If \\( A \\) and \\( B \\) are events such that \\( A \\cap B = \\emptyset \\), then \\( 1_{A \\cup B} = 1_A + 1_B \\).</li> <li>If \\( A \\) and \\( B \\) are events, then \\( 1_{A \\cap B} = 1_A 1_B \\).</li> <li>If \\( A \\subseteq B \\) are events, then \\( 1_A \\leq 1_B \\).</li> </ol> <ul> <li> <p>Simple Random Variable</p> <p>Definition: Simple Random Variables</p> <p>For a family \\( A_1, A_2, \\ldots, A_n \\) of disjoint events and numbers \\( \\alpha_1, \\ldots, \\alpha_n \\), we can define the simple random variable</p> \\[   X(\\omega) = \\sum_{k=1}^n \\alpha_k 1_{A_k}(\\omega) =      \\begin{cases}       \\alpha_k &amp; \\text{if } \\omega \\in A_k, \\\\       0 &amp; \\text{otherwize}     \\end{cases} \\] <p>According to the previous proposition, it follows that \\( X \\) is also a random variable.</p> <p>Note that intuitively, multiplication and addition of simple random variables remain simple random variables, however one has to be careful to show it on the events where both random variable coincide.</p> </li> <li> <p>Plot</p> <p> </p> </li> </ul> <p>Example: Random Variable on Finite State Space</p> <p>Let \\( \\Omega = \\{\\omega_1, \\omega_2, \\ldots, \\omega_N\\} \\) be a finite state space and \\( \\sigma \\)-algebra \\( \\mathcal{F} = 2^\\Omega \\). We consider a financial market with one stock \\( S \\) where \\( S_0 &gt; 0 \\) denotes the price today and \\( S_1 \\) represents the possible price of the stock tomorrow. The possible evolution for the stock is given as a function:</p> \\[   \\begin{equation*}     \\begin{split}       S_1\\colon \\Omega &amp; \\longrightarrow [0, \\infty)\\\\           \\omega_n &amp;\\longmapsto S_1(\\omega_n) = s_n     \\end{split}   \\end{equation*} \\] <p>We can also write the stock price function as a simple random variable (showing therefore that it is a random variable):</p> \\[ S_1 = \\sum_{n=1}^N s_n 1_{A_n} \\] <p>where \\( A_n = \\{\\omega_n\\} \\). In other terms, the stock price is entirely given by the vector \\( (s_1, \\ldots, s_N) \\). Without any loss of generality, since we have one stock, we may assume that \\( s_1 &lt; s_2 &lt; \\ldots &lt; s_N \\). Also, since the stock price is positive, we also have \\( 0 \\leq s_1 \\). The returns \\( R_1 = \\frac{S_1 - S_0}{S_0} \\) are also a random variable that can be described as a vector \\( (r_1, \\ldots, r_N) \\), where</p> \\[ r_n = \\frac{s_n - S_0}{S_0} \\]"},{"location":"material/probability/#probability-measure","title":"Probability Measure","text":"<p>Definition: Probability Measure</p> <p>A probability measure \\( P \\) on the measurable space \\( (\\Omega, \\mathcal{F}) \\) is a function \\( P: \\mathcal{F} \\to [0,1] \\) that associate to each event \\(A\\) the likelyhood of this event.</p> <p>It has the following basic properties:</p> <ul> <li> <p>\\( P[\\emptyset] = 0 \\) and \\( P[\\Omega] = 1 \\)(1)</p> <ol> <li>Clearly, the probability that nothing or anything can happen is \\(0\\) or \\(1\\).</li> </ol> </li> <li> <p>\\(P[A \\cup B] = P[A] + P[B]\\) if \\(A\\) and \\(B\\) are two disjoint events.(1)</p> <ol> <li>The countable property is however assumed, that is \\( P[\\cup A_n] = \\sum P[A_n] \\) for every sequence of pairwise disjoint(1) events \\( (A_n) \\subseteq \\mathcal{F} \\).</li> </ol> </li> </ul> <p>The triple \\( (\\Omega, \\mathcal{F}, P) \\) is called a probability space.</p> <p>The assumptions for a probability measure are few, however together with the definition of the algebra we can rapidly derive classical properties that are common knowledge.</p> <p>Lemma</p> <p>Let \\( P \\) be a probability measure. For any events \\( A \\), \\( B \\), or sequence \\( (A_n) \\) of events, the following hold:</p> <ul> <li>\\( P[B] = P[A] + P[B \\setminus A] \\geq P[A] \\) whenever \\( A \\subseteq B \\);</li> <li>\\( P[A^c] = 1 - P[A] \\);</li> <li>\\( P[A \\cup B] + P[A \\cap B] = P[A] + P[B] \\);</li> <li> <p>If \\( A_1 \\subseteq A_2 \\subseteq \\ldots \\subseteq A_n \\subseteq \\ldots \\), then:</p> \\[   P\\left[ \\cup A_n \\right] = \\lim P[A_n] \\] </li> <li> <p>If \\( A_1 \\supseteq A_2 \\supseteq \\ldots \\supseteq A_n \\supseteq \\ldots \\), then:</p> \\[   P\\left[ \\cap A_n \\right] = \\lim P[A_n] \\] <p>In particular, it equals \\( 0 \\) if \\( \\cap A_n = \\emptyset \\).</p> </li> </ul> Proof <p>We prove some of the points, leaving the others as an exercise.</p> <p>For the first point, let \\( A \\subseteq B \\). We have \\( B = A \\cup (B \\setminus A) \\), where this union is disjoint. By the second property of a probability measure and the positivity of probability:</p> \\[ P[B] = P[A \\cup (B \\setminus A)] = P[A] + P[B \\setminus A] \\geq P[A] \\] <p>Taking \\( B = \\Omega \\), and using \\( P[\\Omega] = 1 \\), the second point follows.</p> <p>Using similar arguments, prove the third point.</p> <p>For the fourth point, construct the sequence of disjoint sets:</p> \\[     B_1 = A_1, \\quad B_2 = A_2 \\setminus A_1, \\quad \\ldots, \\quad B_n = A_n \\setminus A_{n-1} \\] <p>By induction, it is easy to show:</p> \\[ A_n = \\cup_{k=1}^n A_k = \\cup_{k=1}^n B_k, \\quad \\text{and} \\quad \\cup_n A_n = \\cup_n B_n \\] <p>By additivity of the probability measure:</p> \\[   P[A_n] = P\\left[ \\cup_{k=1}^n A_k \\right] = P\\left[ \\cup_{k=1}^n B_k \\right] = \\sum_{k=1}^n P[B_k] \\nearrow \\sum_{k=1}^\\infty P[B_k] \\] <p>Thus:</p> \\[   \\lim P[A_n] = \\sum_{k=1}^\\infty P[B_k] \\] <p>By the second property of a probability measure:</p> \\[ P\\left[ \\cup_{k=1}^\\infty A_k \\right] = P\\left[ \\cup_{k=1}^\\infty B_k \\right] = \\sum_{k=1}^\\infty P[B_k] \\] <p>Combining these equations shows \\( \\lim P[A_n] = P[\\cup A_n] \\).</p> <p>Follow similar reasoning to prove the last point.</p> <p>Note: Shorthand Notations in Probability</p> <p>In probability theory, the following shorthand notations are commonly used:</p> \\[ P[X \\in B] := P[\\{\\omega \\in \\Omega : X(\\omega) \\in B\\}], \\quad  P[X = x] := P[\\{\\omega \\in \\Omega : X(\\omega) = x\\}] \\] \\[ P[X \\leq x] := P[\\{\\omega \\in \\Omega : X(\\omega) \\leq x\\}], \\quad \\ldots \\] <p>Examples</p> <ol> <li> <p>Probability on Finite Sets:     Suppose \\( \\Omega = \\{\\omega_1, \\ldots, \\omega_N\\} \\) is finite.     Each probability measure \\( P \\) on \\( \\mathcal{F} = 2^\\Omega \\) is entirely determined by the values \\( p_n = P[\\{\\omega_n\\}] \\) for \\( n = 1, \\ldots, N \\).     Indeed, for every event \\(A\\) is of the form \\(A=\\{\\omega_n\\colon n \\in I\\}\\) for some \\(I\\subseteq \\{1, \\ldots, N\\}\\).     It follows that</p> \\[   P[A] = \\sum_{\\omega \\in A} P[\\{\\omega\\}] = \\sum_{n \\in I} p_n \\] <p>This vector \\(\\boldsymbol{p}=(p_1, \\ldots, p_N) \\) has the property that \\(p_n = P[\\{\\omega_n\\}]\\geq 0\\) and \\(\\sum p_n =P[\\Omega] = 1\\).</p> <p>Reciprocally, if you give yourself a vector \\(\\boldsymbol{p}=(p_1, \\ldots, p_N)\\) with \\(p_n \\geq 0\\) and \\(\\sum p_n\\), it defines a probability \\(P\\) on \\(\\mathcal{F}\\) with the definition</p> \\[   P[A]:=\\sum_{n \\in I} p_n \\] <p>where \\(A = \\{\\omega_n \\colon n \\in I\\}\\). As an exercise, verify that this defines a probability measure.</p> <p>The set of such vectors is denoted by</p> \\[   \\Delta := \\left\\{ \\boldsymbol{p} \\in \\mathbb{R}^N \\colon : p_n \\geq 0, \\, \\sum p_n = 1 \\right\\} \\] <p>An important case is when \\( p_n = 1/N \\) for all \\( n \\). This is called the uniform probability distribution.</p> </li> <li> <p>Probability on the Coin Toss Space:     Let \\( \\Omega = \\{\\omega = (\\omega_1, \\ldots, \\omega_T) : \\omega_t = \\pm 1\\} \\), a finite state space.     Assuming the probability of heads is \\( p \\) and coin tosses are independent, the probability is:</p> \\[   P[\\{\\omega = (\\omega_1, \\ldots, \\omega_T)\\}] = p^l q^{T-l} \\] <p>where \\( l \\) is the number of times \\( \\omega_t = 1 \\) for \\( t = 1, \\ldots, T \\).</p> </li> <li> <p>Normal Distribution:     For \\( \\Omega = \\mathbb{R} \\) and \\( \\mathcal{F} \\) the \\( \\sigma \\)-algebra of \\( \\mathbb{R} \\) generated by intervals, define for any event \\(A\\) the probability</p> \\[   P[A] = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\int_A e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\lambda(dx) \\] <p>where \\( \\lambda \\) is the Lebesgue measure on \\( \\mathbb{R} \\), the one measuring intervals. This is the normal distribution. For example, temperatures in Shanghai at this time of year may follow a normal distribution around 24\u00b0C with variance 1.</p> </li> </ol>"},{"location":"material/probability/#integration","title":"Integration","text":"<p>The historical idea behind integration was to measure areas below a function. The expectation in probability brings exactly the same intuition to this more abstract level.</p> <p>Consider the simple example of the indicator function \\(1_A\\), it represents a rectangle of height \\(1\\) and width represented by the measure of \\(A\\), that is \\(P[A]\\). Hence, the area of the rectangle, or expectation of the indicator function, is given by \\(E[1_A]=1 \\times P[A]\\).</p> <p>Extending this concept is straightforward for any positive simple random variable.</p> <ul> <li> <p>Integration of Simple Random Variable</p> <p>Definition: Expectation 1.0</p> <p>Let \\((\\Omega,\\mathcal{F},P)\\) be a probability space. Given a simple random variable  </p> \\[   X = \\sum_{k\\leq n} \\alpha_k 1_{A_k} \\] <p>we define the expectation of \\(X\\) with respect to \\(P\\) as  </p> \\[   E[X]:=\\sum_{k\\leq n} \\alpha_k P[A_k] \\] </li> <li> <p>Plot</p> <p> </p> </li> </ul> <p>Warning</p> <p>One needs to be careful that this definition is independent of the representation of the simple random variable. Indeed, we have \\(X= 1_A + 1_B = 1_{A\\cup B}\\) if \\(A\\) and \\(B\\) are disjoint for instance. Luckily, by the properties of the probability measure, this random variable has the same expectation for the two representations.</p> <p>Proposition</p> <p>The two following important properties of the expectation on simple random variables can be rapidely checked.</p> <ul> <li>Monotony: \\(E[X]\\leq E[Y]\\) whenever \\(X\\leq Y\\).  </li> <li>Linearity: \\(E[aX+bY]=aE[X]+bE[Y]\\).  </li> </ul> <p>The proof of which is easy and left to you.</p> <p>Exercise</p> <p>Given a simple random variable \\(X\\) show that</p> <ol> <li>If \\(X\\) is positive, then \\(E[X]&gt;0\\) if and only if \\(P[X&gt;0] &gt;0\\).  </li> <li>If \\(X\\) is positive, then \\(E[X] = 0\\) if and only if \\(P[X = 0]=1\\).  </li> </ol> <p>We can now define the expectation of an arbitrary positive random random variable. The idea is to approximate from below this random variable by simple ones and take the limit.</p> <ul> <li> <p>First Approximation</p> <p> </p> </li> <li> <p>Second Approximation</p> <p> </p> </li> </ul> Note <p>Though the definition of the expectation does not implies the explicit construction of a sequence approximating, it is however possible to formalize the idea in the picture.</p> <p>Given a random variable \\(X\\), the strategy is as follows: For every natural number \\(n\\), divide the ever growing vertical interval \\([0, n)\\) into \\(2^n\\) sub intervals \\(\\left[k \\frac{n}{2^Nn}, (k+1)\\frac{n}{2^n}\\right)\\) for \\(k=0, \\ldots, 2^n-1\\). Define now</p> \\[   \\alpha_k^n = k \\frac{N}{2^n} \\quad \\text{and}\\quad A_k^n = \\left\\{ k\\frac{N}{2^n} \\leq X &lt; (k+1)\\frac{n}{2^N} \\right\\} \\] <p>It follows that the sequence \\((X_n)\\) of simple random variables defined as</p> \\[     X_n = \\sum_{k=0}^{2^N-1} \\alpha_k^n 1_{A_k^n} \\] <p>is increasing and converges to \\(X\\).</p> <p>Definition: Expectation 1.5</p> <p>Given a positive random variable, the expectation of which is defined as</p> \\[   E[X] := \\sup \\left\\{ E[Y] \\colon Y\\text{ simple random variable and } Y\\leq X \\right\\} \\] <p>This is well defined but eventually equal to \\(\\infty\\). For this is also holds that for two positive random variable \\(X\\) and \\(Y\\) with positive numbers \\(a\\) and \\(b\\) then \\(E[aX + bY] = aE[X] + b E[Y]\\) as well as \\(E[X]\\leq E[Y]\\) if \\(X\\leq Y\\).</p> <p>To consider general random variable, we need to assume integrability.</p> <p>Definition: Expectation 2.0</p> <p>A random variable is called integrable if \\(E[X^+]&lt;\\infty\\) and \\(E[X^-]&lt;\\infty\\). The expectation of an integrable random variable is then defined as</p> \\[E[X] = E[X^+]-E[X^-]\\] <p>On the set of integrable random variables, which is a vector space, the expectation is also linear and monotone.</p> <p>The following fundamental theorem is due to Lebesgue. It tells under which conditions it is possible to swap limit and expectation.</p> <p>Theorem</p> <p>Let \\((X_n)\\) be a sequence of random variables.  The following holds true</p> <ol> <li> <p>Monotone Convergence: If \\((X_n)\\) are positive and increasing, that is, X_1\\leq X_2 \\leq \\cdots$  it holds that</p> \\[     \\sup E[X_n] = \\lim E[X_n] = E[\\sup X_n] = E[\\lim X_n] \\] </li> <li> <p>Fatou's Lemma: If \\((X_n)\\) are positive then it holds</p> \\[     E\\left[ \\liminf X_n \\right]:=E\\left[ \\sup_n \\inf_{k\\geq n} X_k\\right] \\leq \\liminf E[X_n] \\] </li> <li> <p>Lebesgue's Dominated Convergence: If \\(X_n(\\omega) \\to X(\\omega)\\) for all (at least in probability) and \\(|X_n|\\leq Y\\) for some integrable random variable \\(Y\\), then it holds</p> \\[   \\lim E[X_n] = E[\\lim X_n] = E[X] \\] </li> </ol> Proof <p>We start by the monotone convergence.</p> <p>By monotonicity, we clearly have \\(E[X_n]\\leq E[X]\\) for every \\(n\\), therefore \\(\\sup E[X_n]\\leq E[X]\\).</p> <p>Reciprocally, suppose that \\(E[X]&lt;\\infty\\) and pick \\(\\varepsilon&gt;0\\) and a positive simple random variable \\(Y \\) such that \\(Y\\leq X\\) and \\(E[X]-\\varepsilon\\leq E[Y]\\). For \\(0&lt;c&lt;1\\), define the sets \\(A_n=\\{X_n\\geq cY\\}\\). Since \\(X^n\\) is increasing to \\(X\\), it follows that \\(A_n\\) is an increasing sequence of events. Furthermore, since \\(cY\\leq Y\\leq X\\) and \\(cY&lt;X\\) on \\(\\{X&gt;0\\}\\), it follows that \\(\\cup A_n=\\Omega\\). By non-negativity of \\(X_n\\) and monotonicity, it follows that</p> \\[     cE[1_{A_n}Y]\\leq E[1_{A_n}X_n]\\leq E[X_n] \\] <p>and so</p> \\[     c\\sup E[1_{A_n}Y]\\leq \\sup E[X_n] \\] <p>Since \\(Y=\\sum_{l\\leq k} \\alpha_l 1_{B_l}\\) for positive numbers \\(\\alpha_1,\\ldots,\\alpha_k\\) and events \\(B_1,\\ldots, B_k\\), it follows that  </p> \\[     E\\left[ 1_{A_n}Y \\right]=\\sum_{l\\leq k}\\alpha_l P[A_n\\cap B_l]. \\] <p>However, since \\(P\\) is a probability measure, and \\(A_n\\) is increasing to \\(\\Omega\\), it follows from the lower semi-continuity of probability measures, that \\(P[A_n\\cap B_l]\\nearrow P[\\Omega\\cap B_l]=P[B_l]\\), and so  </p> \\[     \\sup E[1_{A_n}Y]=\\sum_{l\\leq k}\\alpha_l \\sup P[A_n\\cap B_l]=\\sum \\alpha_l P[B_l]=E[Y]. \\] <p>Consequently</p> \\[     E[X]\\geq \\lim E[X_n]=\\sup E[X_n]\\geq cE[Y] \\geq cE[X]-c\\varepsilon \\] <p>which, by letting \\(c\\) converge to \\(1\\) and \\(\\varepsilon\\) to \\(0\\), yields the result.  </p> <p>The case where \\(E[X]=\\infty\\) is similar and left to the reader.</p> <p>As for Fatou's lemma, define \\(Y_n =\\inf_{k\\geq n} X_k\\) which defines by assumption an increasing sequence of positive random variables. It follows from monotone convergence that</p> \\[     \\sup_n E\\left[ Y_n  \\right] = E[\\sup_n Y_n] = E[\\sup_n \\sup_{k\\geq n}X_n] = E[\\liminf X_n] \\] <p>On the other hand, it clearly holds that \\(X_k \\geq Y_n\\) for every \\(k\\geq n\\) and therefore \\(\\inf_{k\\geq n} E[X_k] \\geq E[Y_n]\\). Combined with the previous inequality we get</p> \\[   E[\\liminf X_n] = \\sup_n E[Y_n]\\leq \\sup_n \\inf_{k\\geq n}E[X_k] = \\liminf E[X_n] \\] <p>As for the dominated convergence of Lebesgue, we have by assumption that \\(X_n+Y\\) is a sequence of positive random variables, which by Fatou's lemma yields</p> \\[   E[X+Y] =E[\\liminf X_n +Y ] \\leq \\liminf E[X_n] +E[Y] \\] <p>Reciprovally \\(Y = X_n\\) is a sequence of positive random variable for which also holds</p> \\[   E[Y-X] = E[\\liminf Y - X_n] \\leq E[Y] +\\liminf -E[X_n] = E[Y] - \\limsup E[X_n] \\] <p>Combining both inequality yields</p> \\[   \\limsup E[X_n] \\leq E[X] \\leq \\liminf E[X_n] \\] <p>Since $\\liminf \\leq \\limsup $ if and only if there exists a limit we deduce that \\(E[X] = \\lim E[X_n]\\).</p> <p>Example</p> <ul> <li> <p>Integration for the simple coin toss:     Let \\(\\Omega =\\{\\omega_1,\\omega_2\\}\\) and \\(p=P[\\{\\omega_1\\}]\\) and \\(q=(1-p)\\).     Every random variable \\(X:\\Omega \\to \\mathbb{R}\\) is entirely determined by the values \\(X(\\omega_1) = x_1\\) and \\(X(\\omega_2)=x_2\\).     It follows that  </p> \\[     E[X]=pX(\\omega_1)+qX(\\omega_2) = p x_1 + (1-p)x_2 \\] </li> <li> <p>Integration in the finite state case:     Let \\(\\Omega=\\{\\omega_1,\\ldots,\\omega_N\\}\\) be a finite state space.     The probability measure is entirely given by the vector \\(\\boldsymbol{p}=(p_1,\\ldots,p_N)\\in \\mathbb{R}^N\\), where \\(p_n=P[\\{\\omega_n\\}]\\geq 0\\) and \\(\\sum p_n=1\\).     Every random variable \\(X:\\Omega \\to \\mathbb{R}\\) can be seen as a vector \\(\\boldsymbol{x} \\in \\mathbb{R}^N\\), where \\(x_n=X(\\omega_n)\\).     It follows that the expectation of \\(X\\) under \\(P\\) is given by</p> \\[     E[X]=\\sum p_n X(\\omega_n)=\\sum p_n x_n=\\boldsymbol{p}\\cdot \\boldsymbol{x} \\] <p>In other terms, the expectation of \\(X\\) boils down to the scalar product of the probability vector \\(\\boldsymbol{p}\\) with the vector of values \\(\\boldsymbol{x}\\) of the random variable.</p> </li> </ul>"},{"location":"material/probability/#measure-change","title":"Measure Change","text":"<p>The concept of the expectation of a random variable \\( E[X] \\) depends, by definition, on the probability measure \\( P \\). We should therefore write \\( E^P[X] \\) to signify this dependence. If, on the same measurable space \\( (\\Omega, \\mathcal{F}) \\), we are given another probability \\( Q \\), the question arises: how is \\( E^P[X] \\) related to \\( E^Q[X] \\)?</p> <p>Remark</p> <p>Before diving into this question, let us first see how, starting from a probability \\( P \\), we can define a new probability \\( Q \\). Suppose we are given a random variable \\( Z \\) such that:</p> <ol> <li>\\( Z \\) is positive.</li> <li>\\( E^P[Z] = 1 \\).</li> </ol> <p>We can define the function:</p> \\[ \\begin{aligned}     Q \\colon \\mathcal{F} &amp; \\longrightarrow [0,1] \\\\       A &amp; \\longmapsto Q[A] = E^P[Z \\cdot 1_A] \\end{aligned} \\] <p>This function, for any event \\( A \\), returns the expectation of \\( Z \\) over \\( A \\). It turns out that this function, under the assumptions on \\( Z \\), defines a new probability measure. Specifically:</p> <ul> <li>\\( Q[\\emptyset] = E^P[Z \\cdot 1_\\emptyset] = E^P[0] = 0 \\),</li> <li>\\( Q[\\Omega] = E^P[Z \\cdot 1_\\Omega] = E^P[Z] = 1 \\).</li> </ul> <p>Additivity also holds: for any two disjoint events \\( A \\) and \\( B \\), \\( 1_{A \\cup B} = 1_A + 1_B \\). Hence:</p> \\[ Q[A \\cup B] = E^P[Z \\cdot 1_{A \\cup B}] = E^P[Z \\cdot 1_A] + E^P[Z \\cdot 1_B] = Q[A] + Q[B]. \\] Warning <p>To fully define \\( Q \\) as a probability measure, you must also check \\(\\sigma\\)-additivity. That is, for every sequence \\( (A_n) \\) of pairwise disjoint events, it must hold:</p> \\[ Q\\left[\\bigcup A_n\\right] = \\sum Q[A_n]. \\] <p>Define the random variables \\( X_n = Z \\cdot 1_{\\cup_{k \\leq n} A_k} = Z \\cdot \\left( \\sum_{k \\leq n} 1_{A_k} \\right) \\) and let \\( X = Z \\cdot 1_{\\cup A_n} \\). Since \\( |X_n| \\leq Z \\), where \\( Z \\) is integrable, dominated convergence implies:</p> \\[ \\lim E^P[X_n] = E^P[X]. \\] <p>Meanwhile:</p> \\[ E^P[X_n] = \\sum_{k \\leq n} E^P[Z \\cdot 1_{A_k}] = \\sum_{k \\leq n} Q[A_k], \\] <p>and \\( E^P[X] = Q\\left[\\bigcup A_n\\right] \\).</p> <p>Hence, any positive random variable \\( Z \\) with expectation 1 under \\( P \\) defines a new probability measure \\( Q \\).</p> <p>Furthermore, for any bounded random variable \\( X \\), it holds that: [ E^Q[X] = E^P[Z \\cdot X]. ]</p> <p>To see this, consider a simple random variable \\( X = \\sum \\alpha_k \\cdot 1_{A_k} \\):</p> \\[ \\begin{aligned}     E^Q[X] &amp;= \\sum \\alpha_k Q[A_k] \\\\            &amp;= \\sum \\alpha_k E^P[Z \\cdot 1_{A_k}] \\\\            &amp;= E^P[Z \\cdot X]. \\end{aligned} \\] <p>The general case follows by approximating \\( X \\) with simple random variables.</p> <p>Additionally, \\( Q \\) is dominated by \\( P \\) in the sense that \\( P[A] = 0 \\) implies \\( Q[A] = E^P[Z \\cdot 1_A] = 0 \\).</p> <p>From this, we see that a positive random variable \\( Z \\) with expectation 1 allows us to define a new probability \\( Q \\), dominated by \\( P \\), and connects expectations under \\( Q \\) to those under \\( P \\). The challenging and powerful task is to establish the reciprocal relationship. The key lies in the concepts of absolute continuity or equivalence between probability measures, and the Radon-Nikodym Theorem.</p> <p>Definition</p> <p>Given two probability measures \\( P \\) and \\( Q \\), we define:</p> <ol> <li> <p>\\( Q \\) is absolutely continuous with respect to \\( P \\) (\\( Q \\ll P \\)) if:</p> \\[ P[A] = 0 \\quad \\text{implies} \\quad Q[A] = 0. \\] </li> <li> <p>\\( Q \\) is equivalent to \\( P \\) (\\( Q \\sim P \\)) if both \\( Q \\ll P \\) and \\( P \\ll Q \\), i.e.:</p> \\[ P[A] = 0 \\quad \\text{if and only if} \\quad Q[A] = 0. \\] </li> </ol> <p>By definition:</p> \\[ Q \\ll P \\quad \\text{if and only if} \\quad P[A] = 1 \\text{ implies }Q[A] = 1, \\] <p>or equivalently:</p> \\[ Q \\ll P \\quad \\text{if and only if} \\quad Q[A] &gt; 0 \\text{ implies }P[A] &gt; 0. \\] <p>In the equivalent case:</p> \\[ Q \\sim P \\quad \\text{if and only if} \\quad P[A] = 1 \\text{if and only if } Q[A] = 1, \\] <p>or equivalently:</p> \\[ Q \\sim P \\quad \\text{if and only if} \\quad P[A] &gt; 0 \\text{ if and only if } Q[A] &gt; 0. \\] <p>Absolute continuity implies that events unlikely under \\( P \\) are also unlikely under \\( Q \\). Equivalence means that \\( P \\) and \\( Q \\) agree on which sets are unlikely.</p> <p>Radon-Nikodym Theorem</p> <p>On a measurable space \\( (\\Omega, \\mathcal{F}) \\), if a probability measure \\( Q \\) is absolutely continuous with respect to another probability measure \\( P \\), there exists a (\\( P \\)-almost surely) unique random variable \\( Z \\) such that:</p> \\[ \\begin{aligned}     Z &amp;\\geq 0, \\\\     E^P[Z] &amp;= 1, \\\\     E^Q[X] &amp;= E^P[Z \\cdot X] \\quad \\text{ for any bounded random variable } X. \\end{aligned} \\] <p>This unique random variable is called the density of \\( Q \\) with respect to \\( P \\) and is denoted \\( \\frac{dQ}{dP} \\).</p> <p>The notation \\( \\frac{dQ}{dP} \\) is cosmetic; it does not represent a literal ratio. It simplifies expressions such as:</p> \\[ E^P\\left[ \\frac{dQ}{dP} \\cdot X \\right] = \\int X \\frac{dQ}{dP} \\, dP = \\int X \\, dQ = E^Q[X]. \\] <p>This theorem underpins many results in stochastic processes and finance, such as the Black-Scholes-Merton formula. However, proving it requires knowledge of functional analysis, which is beyond this lecture's scope. The proof is simpler in a finite state space.</p> <p>Exercise</p> <p>Let \\( \\Omega = \\{\\omega_1, \\ldots, \\omega_n\\} \\) be a finite state space with \\( \\sigma \\)-algebra \\( \\mathcal{F} = 2^\\Omega \\). Suppose \\( P \\) is a probability measure given by \\( \\boldsymbol{p} = (p_1, \\ldots, p_n) \\), where \\( P[\\{\\omega_i\\}] = p_i &gt; 0 \\) and \\( \\sum p_i = 1 \\). Let \\( Q \\) be another probability measure on \\( (\\Omega, \\mathcal{F}) \\) given by \\( \\boldsymbol{q} = (q_1, \\ldots, q_n) \\), where \\( Q[\\{\\omega_i\\}] = q_i \\geq 0 \\) and \\( \\sum q_i = 1 \\).</p> <p>Since \\( P[A] = 0 \\) implies \\( A = \\emptyset \\), it follows that \\( Q[A] = Q[\\emptyset] = 0 \\). Hence, \\( Q \\ll P \\).</p> <p>Find a random variable \\( \\frac{dQ}{dP} \\colon \\Omega \\to \\mathbb{R} \\) such that \\( \\frac{dQ}{dP} \\geq 0 \\), \\( E^P\\left[\\frac{dQ}{dP}\\right] = 1 \\), and:</p> \\[ E^Q[X] = E^P\\left[\\frac{dQ}{dP} \\cdot X\\right] \\] <p>for every random variable \\( X \\colon \\Omega \\to \\mathbb{R} \\). Show that \\( \\frac{dQ}{dP} \\) is unique.</p> <p>In this finite setting, \\( \\frac{dQ}{dP} \\) can be represented by a vector \\( \\boldsymbol{z} = (z_1, \\ldots, z_n) \\) with \\( z_i = \\frac{dQ}{dP}(\\omega_i) \\). The conditions reduce to finding \\( \\boldsymbol{z} \\) such that \\( z_i \\geq 0 \\), \\( \\sum z_i p_i = 1 \\), and for every vector \\( \\boldsymbol{x} = (x_1, \\ldots, x_n) \\):</p> \\[ \\sum x_i q_i = E^Q[X] = E^P\\left[\\frac{dQ}{dP} \\cdot X\\right] = \\sum x_i z_i p_i. \\]"},{"location":"material/probability/#independence","title":"Independence","text":"<p>A fundamental concept in probability, distinct from general measure theory, is independence. Intuitively, two events \\( A \\) and \\( B \\) are independent if their probability of joint occurrence equals the product of their respective probabilities.</p> <p>This concept can be extended to random variables and families of events, with significant implications for results in probability theory.</p> <p>Definition</p> <p>Given a probability space \\( (\\Omega, \\mathcal{F}, P) \\):</p> <ol> <li> <p>Two events \\( A \\) and \\( B \\) are called independent if:</p> \\[ P[A \\cap B] = P[A] P[B]. \\] </li> <li> <p>Two families of events \\( \\mathcal{C} \\) and \\( \\mathcal{D} \\) are independent if any event \\( A \\) in \\(\\mathcal{C}\\) is independent of any event \\( B \\) in \\(\\mathcal{D}\\).</p> </li> <li> <p>Two random variables \\( X \\) and \\( Y \\) are independent if the \\(\\sigma\\)-algebras generated by their information,</p> \\[ \\sigma(X) = \\sigma(\\{X \\leq x\\} : x \\in \\mathbb{R}) \\quad \\text{and} \\quad \\sigma(Y) = \\sigma(\\{Y \\leq x\\} : x \\in \\mathbb{R}), \\] <p>are independent.</p> </li> <li> <p>A collection of families of events \\( \\mathcal{C}^i \\) (with \\( i \\) indexing the families) is independent if for every finite selection of events \\( A^{i_1}, \\ldots, A^{i_n} \\), where \\( A^{i_k}\\) is in \\(\\mathcal{C}^{i_k} \\), it holds that:</p> \\[ P\\left[ A^{i_1} \\cap \\cdots \\cap A^{i_n} \\right] = \\prod_{k=1}^n P[A^{i_k}]. \\] </li> <li> <p>A family (or sequence) of random variables \\( (X_i) \\) is independent if the family of \\(\\sigma\\)-algebras \\( \\sigma(X_i) \\) is independent.</p> </li> </ol> <p>Warning</p> <p>The first three points focus on pairwise independence for events, families, or random variables. However, for collections with more than two elements, pairwise independence is insufficient.  For example, a sequence of random variables requires a stronger notion of independence that accounts for all finite subsets.  </p> <p>Exercise</p> <p>Consider a four-element probability space \\( \\Omega = \\{\\omega_1, \\omega_2, \\omega_3, \\omega_4\\} \\) with uniform probability \\( P[\\{\\omega_i\\}] = \\frac{1}{4} \\). Construct three events \\( A_1 \\), \\( A_2 \\), and \\( A_3 \\) such that: - \\( A_1 \\) is independent of \\( A_2 \\), - \\( A_1 \\) is independent of \\( A_3 \\), - \\( A_2 \\) is independent of \\( A_3 \\), - but \\( A_1 \\), \\( A_2 \\), and \\( A_3 \\) together are not independent.</p> <p>Formally:</p> \\[ \\begin{aligned}     P[A_1 \\cap A_2] &amp;= P[A_1] P[A_2], \\\\     P[A_1 \\cap A_3] &amp;= P[A_1] P[A_3], \\\\     P[A_2 \\cap A_3] &amp;= P[A_2] P[A_3], \\\\     P[A_1 \\cap A_2 \\cap A_3] &amp;\\neq P[A_1] P[A_2] P[A_3]. \\end{aligned} \\] <p>If you struggle, ask ChatGPT\u2014it can handle this.</p> <p>Independence is a strong assumption, but it depends on the probability measure. Even if two events are independent under a specific \\( P \\), independence might fail under a different measure. This concept is crucial in foundational results such as the law of large numbers and the central limit theorem, which are cornerstones of Monte Carlo methods.</p> <p>Let us now present a proposition related to independent random variables, which will be further explored in the context of stochastic processes and conditional expectations.</p> <p>Proposition</p> <p>Let \\( X \\) and \\( Y \\) be two independent bounded random variables. Then:</p> \\[ E[X Y] = E[X] E[Y]. \\] <p>Proof sketch</p> <p>Consider the case where \\( X = 1_A \\) and \\( Y = 1_B \\) are indicator functions. Independence of \\( X \\) and \\( Y \\) implies that \\( A \\) and \\( B \\) are independent. Hence: [ E[XY] = E[1_A 1_B] = E[1_{A \\cap B}] = P[A \\cap B] = P[A] P[B] = E[X] E[Y]. ]</p> <p>This reasoning extends easily to simple random variables, as the \\(\\sigma\\)-algebras generated by \\( X \\) and \\( Y \\) correspond to the events on which they are defined.</p> <p>For the general case, approximate \\( X \\) and \\( Y \\) by sequences of simple random variables \\( (X_n) \\) and \\( (Y_n) \\), and use the properties of independence and limits of expectations.</p>"},{"location":"material/probability/#conditional-expectation","title":"Conditional Expectation","text":"<p>The conditional expectation is the first step towards stochastic processes. It is basically the best approximation in terms of expectation given some information. In other terms, let \\(\\mathcal{G}\\subseteq \\mathcal{F}\\) be a sub-\\(\\sigma\\)-algebra of events, what is the best approximation of the expectation of \\(X\\) knowing the events in \\(\\mathcal{G}\\).</p> <p>Conditional Expectation</p> <p>Let \\((\\Omega, \\mathcal{F}, P)\\) be a probability space, \\(X\\) a random variable and \\(\\mathcal{G}\\subseteq \\mathcal{F}\\) a \\(\\sigma\\)-algebra.</p> <p>Then, there exists a unique(1) random variable \\(Y\\) with the properties</p> <ol> <li>\\(Y\\) is \\(\\mathcal{G}\\)-measurable;</li> <li>\\(E[Y1_A] = E[X1_A]\\) for any event \\(A\\) in \\(\\mathcal{G}\\).</li> </ol> Proof <p>The proof of the theorem is a consequence of Radon-Nykodym derivative. Indeed, define the measurs \\(Q^+\\) and \\(Q^-\\)</p> \\[     \\begin{equation*}         \\begin{split}             Q^\\pm \\colon \\mathcal{G} &amp;\\longrightarrow [0, \\infty)\\\\                         A &amp; \\longmapsto Q^\\pm[A] = E[X^\\pm 1_A]         \\end{split}     \\end{equation*} \\] <p>which are measures defined on the smallest \\(\\sigma\\)-algebra of events \\(\\mathcal{G}\\). These measures are absolutely continuous with respect to \\(P\\), and therefore there exists unique \\(dQ^\\pm/dP\\) their densities that are \\(\\mathcal{G}\\)-measurable.</p> <p>Defining </p> \\[     Y = \\frac{dQ^+}{dP} - \\frac{dQ^-}{dP} \\] <p>give a unique \\(\\mathcal{G}\\)-measurable random variable satistying by definition the expectation property.</p> <p>Since the random variable satisfying the two conditions is unique(1) we can therefore use it as definition.</p> <p>Conditional Expectation</p> <p>The conditional expectation of a random variable \\(X\\) with respect to \\(\\mathcal{G}\\) is denoted by \\(E[X|\\mathcal{G}]\\) and is defined as the unique random variable which is \\(\\mathcal{G}\\)-measurable and such that \\( E[ E[X |\\mathcal{G}]1_A] = E[X 1_A]\\) for all events \\(A\\) in \\(\\mathcal{G}\\).</p> <p>The conditional expectation shares most of the properties of the traditional expectation</p> <p>Proposition</p> <p>Let \\(X\\) be a random variable, \\(\\mathcal{G} \\subseteq \\mathcal{F}\\). It holds that</p> <ul> <li>Expectation: \\(E[E[X|\\mathcal{G}]] = E[X]\\)</li> <li>Conditional Linearity: \\(E[Y X + Z |\\mathcal{G}] = Y E[X |\\mathcal{G}] + Z\\) for any random variables \\(Y\\) and \\(Z\\) which are \\(\\mathcal{G}\\)-measurable.</li> <li>Tower Property: \\(E[E[X | \\mathcal{G_2}] | \\mathcal{G}_1] = E[X|\\mathcal{G}_1]\\) if \\(\\mathcal{G}_1\\subseteq \\mathcal{G}_2\\).</li> <li>Trivial: \\(E[X |\\mathcal{F}_0] = E[X]\\) if \\(\\mathcal{F}_0 = \\{\\emptyset, \\Omega\\}\\),</li> <li>Independence: \\(E[X | \\mathcal{G}] = E[X]\\) if \\(X\\) is independent of \\(\\mathcal{G}\\).</li> </ul>"},{"location":"material/stochastic/","title":"Stochastic Processes","text":"<p>Stochastic processes mean that you want to carry over time the idea that outcomes will be revealed as time goes by. Considering for instance your view of the evolution of a financial asset. At the very begining your decisions towards it relies only on what you can forsee in the future about its evolution however as times goes buy you learn more and more infromation about the nature of this financial asset.</p> <p>Example</p> <p>We consider a probability space \\((\\Omega, \\mathcal{F}, P)\\) and a sequence \\((Y_t)\\) of independent identically distributed (iid) random variables such that</p> \\[   P[Y_t = 1] = P[Y_1 = 1] = \\frac{1}{2} = P[Y_1 = -1] = P[Y_t = -1] \\] <p>In other terms, \\(Y_n\\) represent the result of tossing a fair coin at time \\(t\\) with \\(1\\) if tail and \\(-1\\) if head.</p> <p>We define the (symetric) random walk as</p> \\[   S_0 = 100 \\quad \\text{and} \\quad S_t = S_{t-1} + Y_t = 100 + \\sum_{s=1}^t Y_s \\] <p>this produces the following possible paths</p> <p> </p> <p>Now for a price of \\(100\\) RMB you have the choice between the different games.</p> <ol> <li>All In: Receive the value of the random walk after 100 coin tosses, that is \\(S_{100}\\).</li> <li>Stop Gain: If the random walk reaches \\(120\\), you stop the game and cash \\(120\\), otherwize you get \\(S_{100}\\)</li> <li>Stop Loss: If the random walk falls to \\(90\\), you stop the game and cash \\(90\\), otherwize you get \\(S_{100}\\).</li> <li>Stop Gain/Loss: If the random walk reaches \\(120\\) or \\(90\\) you stop the gain an cash \\(120\\) or \\(90\\) respectively, otherwize you get \\(S_{100}\\).</li> <li>Not a gambler: I don't play and keep my \\(100\\) RMB</li> </ol> <p>Now which game would you venture in and why? Which game would bring you in expectation the best outcome?</p>"},{"location":"material/stochastic/#information-conditional-expectation","title":"Information, Conditional Expectation","text":"<p>Information is considered as a \\(\\sigma\\)-algebra of events. Now information means that we have an increasing sequence of events that we are aware of, called a filtration.</p> <p>Definition: Filtration</p> <p>A filtration \\(\\mathbb{F} = (\\mathcal{F}_t)_{t=0, \\ldots, T}\\) is a collection of \\(\\sigma\\)-algebra of events such that</p> \\[ \\mathcal{F}_0 \\subseteq \\mathcal{F}_1 \\subseteq \\cdots \\subseteq \\mathcal{F}_T\\] <p>In other terms, the collection of events known at time \\(s\\) is included in the set of events known at time \\(t\\geq s\\).</p> <p>Warning</p> <p>It is not necessary but throughout we make the assumption that </p> \\[\\mathcal{F}_0 = \\{\\emptyset, \\Omega\\} \\quad \\text{and}\\quad \\mathcal{F}_T = \\mathcal{F}\\] <p>In other terms, we assume that at time zero we know nothing and at time \\(T\\) we know everything.</p> <p>Lemma</p> <p>If a random variable \\(\\xi\\) is \\(\\mathcal{F}_0=\\{\\emptyset, \\Omega\\}\\)-measurable, then it must be constant.</p> Proof <p>It is a basic exercise to check. Suppose that a random variable \\(\\xi\\) is \\(\\mathcal{F}_0=\\{\\emptyset, \\Omega\\}\\)-measurable then it constant. Indeed, if it where not, let \\(\\omega_1\\) and \\(\\omega_2\\) be two states on which \\(\\xi(\\omega_1) &lt; \\xi(\\omega_2)\\), let \\(x\\) be such that \\(\\xi(\\omega_1)&lt;x&lt;\\xi(\\omega_2)\\). It follows that \\(\\omega_1 \\in A=\\{\\xi\\leq x\\}\\) while \\(\\omega_2 \\not \\in A\\). This is however not possible since the event \\(A\\) is either \\(\\Omega\\) or \\(\\emptyset\\).</p> <p>In this case, it follows that any random variable \\(\\xi\\) which is \\(\\mathcal{F}_0\\) measurable must be constant.</p> <p>Definition</p> <p>A family \\(X = (X_t)\\) of random variable indexed by time is called a stochastic process.</p> <p>A stochastic process \\(X\\) is called </p> <ul> <li>Adapted: if \\(X_t\\) is \\(\\mathcal{F}_t\\)-measurable for every \\(t\\);</li> <li>Predictable: if \\(X_t\\) is \\(\\mathcal{F}_{t-1}\\)-measurable for every \\(t\\)</li> </ul> Remark <p>For predictability, one needs to specify a convention for \\(X_0\\). either we say that predictable processes starts at time \\(1\\) or we say that \\(\\mathcal{F}_{-1} = \\mathcal{F}_0 = \\{\\emptyset, \\Omega\\}\\).</p>"},{"location":"material/stochastic/#martingales","title":"Martingales","text":"<p>Martingales are the most important object to study the properties of stochastic processes.</p> <p>Definition</p> <p>A stochastic process \\(X = (X_t)_{t=0, \\ldots, T}\\) is called a martingale if</p> <ol> <li>\\(X\\) is adapted</li> <li>\\(X\\) is integrable</li> <li> <p>\\(X\\) satisfies the martingale property</p> \\[     E[X_{t+1}|\\mathcal{F}_t] = X_t \\] </li> </ol> <p>It is a super-martingale if </p> <ol> <li>\\(X\\) is adapted</li> <li>\\(X\\) is integrable</li> <li> <p>\\(X\\) satisfies the super martingale property</p> \\[     E[X_{t+1}|\\mathcal{F}_t] \\leq X_t \\] </li> </ol> <p>It is a sub-martingale if </p> <ol> <li>\\(X\\) is adapted</li> <li>\\(X\\) is integrable</li> <li> <p>\\(X\\) satisfies the super martingale property</p> \\[     E[X_{t+1}|\\mathcal{F}_t] \\geq X_t \\] </li> </ol> <p>Clearly, \\(X\\) is a sub-martingale if and only if \\(-X\\) is a super-martingale and \\(X\\) is a martingale if and only if it is a super and sub martingale at the same time.</p> <p>Warning</p> <p>Note that the notion of martingale (sup or super) depends on the measure considered. A martingale under a probability measure \\(P\\) might no longer be a martingale under another probability measure.</p> <p>Doob-Meyer Decomposition</p> <p>Let \\(X\\) be an integrable and adapted process. It can be uniquely decomposed into:</p> \\[ X=M+A, \\] <p>where \\(A\\) is a predictable process with \\(A_0=0\\) and \\(M\\) is a martingale.  </p> <p>The process \\(X\\) is a super-martingale if and only if \\(A\\) is decreasing and a sub-martingale if and only if \\(A\\) is increasing.</p> Proof <p>Existence:  Suppose that we have a decomposition \\(X = M+A\\) with \\(M\\) martingale and \\(A\\) predictable, then it must hold that</p> \\[     \\begin{align*}         0 &amp; = E[M_{t+1} - M_t|\\mathcal{F}_t]\\\\             &amp; = E[X_{t+1} - X_t - A_{t+1} + A_t | \\mathcal{F}_t]\\\\             &amp; = E[X_{t+1} - X_t  | \\mathcal{F}_t] - A_{t+1} + A_t &amp;&amp;A \\text{ is predictable}     \\end{align*} \\] <p>showing that \\(A_{t+1} =  E[X_{t+1} - X_t |\\mathcal{F}_t] - A_t\\).</p> <p>We therefore define recursively</p> \\[     \\begin{equation*}         \\begin{cases}             A_0 &amp; = 0\\\\             A_{t+1} &amp; = E[X_{t+1} - X_{t}|\\mathcal{F}_t] - A_t         \\end{cases}     \\end{equation*} \\] <p>which by induction can be shown to be predictable and starting at \\(0\\). Then \\(M = X+A\\) by the previous computations is a martingale defining the decomposition.</p> <p>Uniqueness: Suppose that \\(X = M+A = \\tilde{M} + \\tilde{A}\\) be two decompositions. It follows that \\(M - \\tilde{M}\\) is a martingale and predictable. It follows that for every \\(t\\) it must hold</p> \\[     \\begin{align*}         0 &amp;= E\\left[ (M_{t+1} - M_t) - (\\tilde{M}_{t+1} - \\tilde{M}_t) |\\mathcal{F}_t \\right] &amp;&amp; \\text{Martingale property}\\\\         &amp; = (M_{t+1} - M_t) - (\\tilde{M}_{t+1} - \\tilde{M}_t) &amp;&amp; M-\\tilde{M}\\text{ is predictable}     \\end{align*} \\] <p>We therefore get that</p> \\[     M_{t+1} - \\tilde{M}_{t+1} = M_{t} - \\tilde{M}_{t} = \\cdots = M_0 - \\tilde{M}_0 = \\tilde{A}_0 - A_0 = 0-0 = 0  \\] <p>showing that \\(M=\\tilde{M}\\) and therefore \\(A=\\tilde{A}\\).</p> <p>Proposition</p> <p>Let \\(M\\) be an adapted and integrable process. The two following assertions are equivalent:</p> <ol> <li>\\(M\\) is a martingale</li> <li> <p>for any bounded predictable process \\(\\eta = (\\eta_t)\\), the process \\(V= (V_t)\\)</p> \\[     V_t = V_0 + \\sum_{s=1}^t \\eta_s (M_s - M_{s-1}) \\] <p>is a martingale.</p> </li> </ol> Proof <p>Suppose that \\(M\\) is a martingale and let \\(\\eta\\) be a bounded predictable process. By definition \\(V\\) is adapted. Furthermore, denoting by \\(c\\) the constant bounding \\(\\eta\\), it holds that</p> \\[     |V_t| \\leq |V_0| + c \\sum_{s=1}^t (|M_s| + |M_{s-1}|) \\] <p>which is a a sum of integrable random variables hence integrable. As for the martingale property</p> \\[     \\begin{align*}         E[V_{t+1} - V_t|\\mathcal{F}_t] &amp; = E[\\eta_{t+1} (M_{t+1} - M_t) | \\mathcal{F}_t]\\\\         &amp; = \\eta_{t+1} E[M_{t+1}- M_t |\\mathcal{F}_t] &amp;&amp; \\eta\\text{ is predictable}\\\\         &amp; = 0 &amp;&amp; M\\text{ is a martingale}\\\\     \\end{align*} \\] <p>Reciprocally, for \\(V_0 = M_0\\) and \\(\\eta_t = 1\\) for every \\(t\\) by hypotheses, \\(V\\) is a martingale. However \\(V_t = M_t\\) for every \\(t\\).</p>"},{"location":"material/stochastic/#stopping-times","title":"Stopping times","text":"<p>Definition</p> <p>A stopping time is a random variable \\(\\tau:\\Omega \\to \\{0,1,\\ldots, T\\}\\cup\\{\\infty\\}\\) such that \\(\\{\\tau =t\\} = \\{\\omega \\in \\Omega\\colon \\tau(\\omega)=t\\}\\) is in \\(\\mathcal{F}_t\\) for every \\(0\\leq t\\leq T\\).</p> <p>In other terms, \\(\\{ \\tau=t\\}\\) represents the event on which the buyer will decide to exercise its American contingent claim. The event \\(\\{\\tau=\\infty\\}\\) represents the event where the triggering conditions have not been met before the time horizon, and therefore the buyer will exercise it at time \\(T\\).</p> <p>Remark</p> <p>Also (and this is specific to discrete time), the condition \\(\\{\\tau = t\\}\\) in \\(\\mathcal{F}_t\\) for every \\(t\\) is equivalent to the condition \\(\\{\\tau \\leq t\\}\\) in \\(\\mathcal{F}_t\\) for every \\(t\\). This comes from the fact that \\(\\{\\tau \\leq t\\} = \\cup_{s=0}^t \\{\\tau = s\\}\\) and \\(\\{\\tau = t\\} = \\{\\tau\\leq t\\} \\setminus \\{\\tau \\leq t-1\\}\\).</p> <p>Lemma</p> <p>The following statments hold true</p> <ol> <li>Deterministic times are stopping times, i.e. \\(\\tau \\equiv t\\) for some \\(t\\) is a stopping time.</li> <li>If \\(\\sigma\\) and \\(\\tau\\) are stopping times then so is \\(\\sigma\\vee \\tau\\), \\(\\sigma \\wedge \\tau\\) and \\(\\sigma + \\tau\\).</li> <li> <p>If \\(X\\) is an adapted stochastic process and \\(I\\) is an interval then the entry time</p> \\[     \\tau(\\omega) = \\inf \\left\\{ t\\colon X_t(\\omega) \\in I \\right\\} \\] <p>is a stopping time.</p> </li> </ol> Proof <p>As for 1., defining \\(\\tau \\equiv t\\) as a stopping time follows from \\(\\{\\tau = s\\}\\) is equal to \\(\\emptyset\\) if \\(s\\neq t\\) and \\(\\Omega\\) is \\(s=t\\) both of which are in \\(\\mathcal{F}_s\\).</p> <p>As for 2., we just show \\(\\sigma \\wedge \\tau\\) is a stopping time. It follows from \\(\\{\\sigma\\wedge \\tau = t\\} = (\\{\\sigma = t\\}\\cap \\{\\tau \\leq t\\}^c) \\cup \\{\\tau = t\\}\\cap \\{\\sigma \\leq t\\}^c\\). The other cases follow from similar arguments and are left as an exercise.</p> <p>As for 3., it holds</p> \\[   \\{\\tau \\leq t\\} = \\{X_s \\in I \\text{ for some }s\\leq t\\} = \\cup_{s=0}^t \\{X_s \\in I\\}   \\] <p>however, \\(\\{X_s \\in I\\}\\) is in \\(\\mathcal{F}_s\\subseteq \\mathcal{F}_t\\) since \\(X\\) is adapted.</p> <p>Remark</p> <p>As a particular case, given a stopping time \\(\\tau\\), then \\(\\tau \\wedge t\\) is a stopping time smaller than \\(t\\). It follows that \\(t \\mapsto t\\wedge \\tau\\) is a stopped clock where it runs time until it hits \\(\\tau\\) and then stay at \\(\\tau\\). This motivates the following concept of a stopped process.</p> <p>Definition: Stopped Process</p> <p>Let \\(X\\) be a stochastic process and \\(\\tau\\) a stopping time. We denote by \\(X^\\tau = (X_t^\\tau) = (X_{t \\wedge \\tau})\\) the stopped process</p> \\[     \\begin{equation*}       X_t^\\tau(\\omega)=X_{t\\wedge \\tau(\\omega)}(\\omega) =       \\begin{cases}         X_t(\\omega) &amp;\\text{if }t&lt; \\tau(\\omega)\\\\         X_{\\tau(\\omega)}(\\omega) &amp; \\text{if }\\tau(\\omega)\\leq t       \\end{cases}     \\end{equation*} \\] <p> </p> <p>Stopping an adapted process, then it remains an adapted process.</p> <p>Lemma</p> <p>If \\(X\\) is adapted then so is \\(X^\\tau\\).</p> Proof <p>Let \\(x\\) be in \\(\\mathbb{R}\\), given a time \\(t\\), we have</p> \\[     \\begin{align*}         \\{X_t^\\tau \\leq x\\} &amp; = \\{X_{t\\wedge \\tau} \\leq x\\}\\\\                             &amp; = (\\{X_t \\leq x\\} \\cap \\{\\tau &gt; t\\}) \\cup_{s=0}^t (\\{X_s \\leq x\\}\\cap \\{\\tau = s\\})\\\\                             &amp; = \\underbrace{(\\underbrace{\\{X_t \\leq x\\}}_{\\in \\mathcal{F}_t} \\cap \\underbrace{\\{\\tau \\leq t\\}^c}_{\\in \\mathcal{F}_t}) \\cup_{s=0}^t (\\underbrace{\\{X_s \\leq x\\}}_{\\in \\mathcal{F}_s \\subseteq \\mathcal{F}_t}\\cap \\underbrace{\\{\\tau = s\\}}_{\\in \\mathcal{F}_s \\subseteq \\mathcal{F}_t})}_{\\in \\mathcal{F}_t}     \\end{align*} \\]"},{"location":"material/stochastic/#doobs-optional-sampling-theorem","title":"Doob's Optional Sampling Theorem","text":"<p>Theorem: Doob's Optional Sampling</p> <p>Let \\(X\\) be an adapted and integrable process. The following assertions are equivalent:</p> <ol> <li>\\(X\\) is a martingale;</li> <li>For every stopping time \\(\\tau\\), the stopped process \\(X^\\tau\\) is a martingale;</li> </ol> Proof <p>As for 1. implies 2, we already know that if \\(X\\) is a martingale and \\(\\eta = (\\eta_t)_{t=1, \\ldots, T}\\) is a predictable process, then the portfolio</p> \\[   V_t = V_0 + \\sum_{s=1}^t \\eta_s (X_s - X_{s-1}) \\] <p>is a martingale. Taking the portfolio \\(V_0 = X_0\\) and the strategy</p> \\[   \\begin{equation*}    \\eta_t = 1_{\\{t\\leq \\tau\\}} = \\begin{cases}     1 &amp; \\text{if }t\\leq \\tau\\\\     0 &amp; \\text{if }t&gt;\\tau   \\end{cases}   \\end{equation*} \\] <p>where you buy and hold \\(X\\) until time \\(\\tau\\), it follows that \\(\\eta\\) is predictable since \\(\\{t\\leq \\tau\\}=\\{\\tau \\leq t-1\\}^c\\) is in \\(\\mathcal{F}_{t-1}\\). Furthermore, by definition </p> \\[   \\begin{equation*}     V_t = \\begin{cases}       X_t &amp; \\text{if }t&lt; \\tau\\\\       X_\\tau &amp; \\text{if }\\tau \\leq t     \\end{cases}     = M_{t\\wedge \\tau} = M^\\tau_t   \\end{equation*} \\] <p>showing that \\(M^\\tau\\) is a martingale.</p> <p>As for 2. implies 1., it is immediate by considering the stopping time \\(\\tau = T\\).</p> <p>Remark</p> <p>Note that if \\(\\eta\\) is a positive predictable process and \\(X\\) is a super-/sub-martingale, then it is easy to show that the portfolio \\(V_t = V_0 + \\sum_{s=1}^t\\eta_s (X_s - X_{s-1})\\) is a super-/sub-martingale. It follows from the proof that Doob's optional sampling theorem also holds for super-/sub-martingales.</p> <p>Original version of Doob Meyer's Optional Sampling Theorem</p> <p>Let \\(X\\) be a martingale and \\(\\tau\\) be a stopping time with \\(\\tau \\leq T\\), then it follows that</p> \\[     X_0 = E[X_{\\tau}] \\] Proof <p>Let \\(\\tau\\) be a stopping time bounded by \\(T\\) for which clearly holds $0\\wedge \\tau = 0# and \\(T\\wedge \\tau =T\\), since \\(X\\) is a martingale, it follows that \\(X^\\tau\\) is a martingale. By the tower property we get</p> \\[     X_0 = X_{0\\wedge \\tau} = E[X_0^\\tau] = E[X_T^\\tau] = E[\\underbrace{X_{T\\wedge \\tau}}_{=X_\\tau}] \\] <p>Remark</p> <p>The same argumentation with inequality holds if \\(X\\) is a super-/sub-martingale.</p>"}]}