{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Scientific Programming: An Introduction","text":"<p>These lecture notes, still a work in progress, are for a course taught at SJTU to Math undergraduate students.</p>"},{"location":"#what-this-lecture-is-not-about","title":"What This Lecture Is Not About","text":"<ul> <li>Computer theory</li> <li>The inner workings of programming</li> <li>Specific programming methodologies (procedural, functional, OO, etc.)</li> <li>Algorithm complexity theory</li> </ul>"},{"location":"#intended-audience","title":"Intended Audience","text":"<ul> <li>Individuals with little to no programming experience</li> <li>Basic calculus and algebra are sufficient; being a math wizard is not necessary</li> <li>A mathematical perspective is emphasized but not mandatory</li> <li>Those interested in understanding programming fundamentals over technicalities</li> <li>Aspiring scientific computation implementers</li> <li>Individuals seeking to manage data systematically</li> </ul>"},{"location":"#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this course, you should be able to:</p> <ul> <li>Perform scientific computations (optimization, integration, dynamical systems, Monte Carlo simulations, some ML, etc.)</li> <li>Efficiently organize, clean, and query data</li> <li>Debug and optimize your code</li> <li>Plan and execute complex projects with appropriate tools and architecture</li> </ul>"},{"location":"#python-as-the-primary-language","title":"Python as the Primary Language","text":"<p>We use Python for its:</p>"},{"location":"#natural-syntax","title":"Natural Syntax","text":"<p>Comparing C++:</p> <pre><code>#include &lt;iostream&gt;\nint main()\n{\n    std::cout &lt;&lt; \"Hello world\" &lt;&lt; std::endl;\n}\n</code></pre> <p>With Python:</p> <pre><code>print(\"Hello world\")\n</code></pre> <p>As mathematicians it is also a language that is close to the colloquial mathematical formalism: write less, express more conceptually.</p>"},{"location":"#portability","title":"Portability","text":"<ul> <li>Python is an interpreted language, allowing for immediate execution and rapid prototyping, although with potential trade-offs if misused.</li> <li>Code is cross-platform compatible (Linux, macOS, Windows, Android, iOS, etc.) without needing recompilation.</li> </ul>"},{"location":"#versatility","title":"Versatility","text":"<ul> <li>Supports various programming paradigms (object-oriented, functional)</li> <li>Suitable for data analysis, automation, machine learning, web development, API deployment, etc.</li> <li>Facilitates interaction with other programming languages</li> <li>Strikes a balance between low-level and high-level programming</li> </ul>"},{"location":"#extensive-community-support","title":"Extensive Community Support","text":"<ul> <li>Among the top 3 most utilized programming languages</li> <li>Preferred for data analysis, machine learning, AI</li> <li>Rich ecosystem of well-maintained, open-source libraries, including but not limited to:<ul> <li><code>numpy</code>, <code>pandas</code> for data manipulation</li> <li><code>scipy</code>, <code>statsmodels</code> for scientific computing</li> <li><code>matplotlib</code>, <code>seaborn</code> for visualization</li> <li><code>pytorch</code>, <code>tensorflow</code> for machine learning and AI</li> </ul> </li> <li>Highly active on GitHub, unlike Matlab</li> </ul>"},{"location":"#job-market-demand","title":"Job Market Demand","text":"<p>Familiarity with Python and related libraries is increasingly a prerequisite in various fields, replacing basic requirements like Excel proficiency from years past.</p>"},{"location":"#open-source","title":"OPEN SOURCE","text":"<p>Python was created by Guido van Rossum who released it and maintained it for over 30 years as open-source project. This fostered a global community of contributors ensuring transparency, reliability through constant auditing process, and huge diversity of the ecosystem.</p> <p></p>"},{"location":"javascripts/node_modules/mathjax/","title":"MathJax","text":""},{"location":"javascripts/node_modules/mathjax/#beautiful-math-in-all-browsers","title":"Beautiful math in all browsers","text":"<p>MathJax is an open-source JavaScript display engine for LaTeX, MathML, and AsciiMath notation that works in all modern browsers.  It was designed with the goal of consolidating the recent advances in web technologies into a single, definitive, math-on-the-web platform supporting the major browsers and operating systems.  It requires no setup on the part of the user (no plugins to download or software to install), so the page author can write web documents that include mathematics and be confident that users will be able to view it naturally and easily.  Simply include MathJax and some mathematics in a web page, and MathJax does the rest.</p> <p>Some of the main features of MathJax include:</p> <ul> <li> <p>High-quality display of LaTeX, MathML, and AsciiMath notation in HTML pages</p> </li> <li> <p>Supported in most browsers with no plug-ins, extra fonts, or special   setup for the reader</p> </li> <li> <p>Easy for authors, flexible for publishers, extensible for developers</p> </li> <li> <p>Supports math accessibility, cut-and-paste interoperability, and other   advanced functionality</p> </li> <li> <p>Powerful API for integration with other web applications</p> </li> </ul> <p>See http://www.mathjax.org/ for additional details about MathJax, and https://docs.mathjax.org for the MathJax documentation.</p>"},{"location":"javascripts/node_modules/mathjax/#mathjax-components","title":"MathJax Components","text":"<p>MathJax version 3 uses files called components that contain the various MathJax modules that you can include in your web pages or access on a server through NodeJS.  Some components combine all the pieces you need to run MathJax with one or more input formats and a particular output format, while other components are pieces that can be loaded on demand when needed, or by a configuration that specifies the pieces you want to combine in a custom way.  For usage instructions, see the MathJax documentation.</p> <p>Components provide a convenient packaging of MathJax's modules, but it is possible for you to form your own custom components, or to use MathJax's modules directly in a node application on a server.  There are web examples showing how to use MathJax in web pages and how to build your own components, and node examples illustrating how to use components in node applications or call MathJax modules directly.</p>"},{"location":"javascripts/node_modules/mathjax/#whats-in-this-repository","title":"What's in this Repository","text":"<p>This repository contains only the component files for MathJax, not the source code for MathJax (which are available in a separate MathJax source repository).  These component files are the ones served by the CDNs that offer MathJax to the web.  In version 2, the files used on the web were also the source files for MathJax, but in version 3, the source files are no longer on the CDN, as they are not what are run in the browser.</p> <p>The components are stored in the <code>es5</code> directory, and are in ES5 format for the widest possible compatibility.  In the future, we may make an <code>es6</code> directory containing ES6 versions of the components.</p>"},{"location":"javascripts/node_modules/mathjax/#installation-and-use","title":"Installation and Use","text":""},{"location":"javascripts/node_modules/mathjax/#using-mathjax-components-from-a-cdn-on-the-web","title":"Using MathJax components from a CDN on the web","text":"<p>If you are loading MathJax from a CDN into a web page, there is no need to install anything.  Simply use a <code>script</code> tag that loads MathJax from the CDN.  E.g.,</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>See the MathJax documentation, the MathJax Web Demos, and the MathJax Component Repository for more information.</p>"},{"location":"javascripts/node_modules/mathjax/#hosting-your-own-copy-of-the-mathjax-components","title":"Hosting your own copy of the MathJax Components","text":"<p>If you want to host MathJax from your own server, you can do so by installing the <code>mathjax</code> package using <code>npm</code> and moving the <code>es5</code> directory to an appropriate location on your server:</p> <pre><code>npm install mathjax@3\nmv node_modules/mathjax/es5 &lt;path-to-server-location&gt;/mathjax\n</code></pre> <p>Note that we are still making updates to version 2, so include <code>@3</code> when you install, since the latest chronological version may not be version 3.</p> <p>Alternatively, you can get the files via GitHub:</p> <pre><code>git clone https://github.com/mathjax/MathJax.git mj-tmp\nmv mj-tmp/es5 &lt;path-to-server-location&gt;/mathjax\nrm -rf mj-tmp\n</code></pre> <p>Then (in either case) you can use a script tag like the following:</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"&lt;url-to-your-site&gt;/mathjax/tex-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>where <code>&lt;url-to-your-site&gt;</code> is replaced by the URL to the location where you moved the MathJax files above.</p> <p>See the documentation for details.</p>"},{"location":"javascripts/node_modules/mathjax/#using-mathjax-components-in-a-node-application","title":"Using MathJax components in a node application","text":"<p>To use MathJax components in a node application, install the <code>mathjax</code> package:</p> <pre><code>npm install mathjax@3\n</code></pre> <p>(we are still making updates to version 2, so you should include <code>@3</code> since the latest chronological version may not be version 3).</p> <p>Then require <code>mathjax</code> within your application:</p> <pre><code>require('mathjax').init({ ... }).then((MathJax) =&gt; { ... });\n</code></pre> <p>where the first <code>{ ... }</code> is a MathJax configuration, and the second <code>{ ... }</code> is the code to run after MathJax has been loaded.  E.g.</p> <pre><code>require('mathjax').init({\n  loader: {load: ['input/tex', 'output/svg']}\n}).then((MathJax) =&gt; {\n  const svg = MathJax.tex2svg('\\\\frac{1}{x^2-1}', {display: true});\n  console.log(MathJax.startup.adaptor.outerHTML(svg));\n}).catch((err) =&gt; console.log(err.message));\n</code></pre> <p>Note: this technique is for node-based application only, not for browser applications.  This method sets up an alternative DOM implementation, which you don't need in the browser, and tells MathJax to use node's <code>require()</code> command to load external modules.  This setup will not work properly in the browser, even if you webpack it or bundle it in other ways.</p> <p>See the documentation and the MathJax Node Repository for more details.</p>"},{"location":"javascripts/node_modules/mathjax/#reducing-the-size-of-the-components-directory","title":"Reducing the Size of the Components Directory","text":"<p>Since the <code>es5</code> directory contains all the component files, so if you are only planning one use one configuration, you can reduce the size of the MathJax directory by removing unused components. For example, if you are using the <code>tex-chtml.js</code> component, then you can remove the <code>tex-mml-chtml.js</code>, <code>tex-svg.js</code>, <code>tex-mml-svg.js</code>, <code>tex-chtml-full.js</code>, and <code>tex-svg-full.js</code> configurations, which will save considerable space.  Indeed, you should be able to remove everything other than <code>tex-chtml.js</code>, and the <code>input/tex/extensions</code>, <code>output/chtml/fonts/woff-v2</code>, <code>adaptors</code>, <code>a11y</code>, and <code>sre</code> directories.  If you are using the results only on the web, you can remove <code>adaptors</code> as well.</p> <p>If you are not using A11Y support (e.g., speech generation, or semantic enrichment), then you can remove <code>a11y</code> and <code>sre</code> as well (though in this case you may need to disable the assistive tools in the MathJax contextual menu in order to avoid MathJax trying to load them when they aren't there).</p> <p>If you are using SVG rather than CommonHTML output (e.g., <code>tex-svg.js</code> rather than <code>tex-chtml.js</code>), you can remove the <code>output/chtml/fonts/woff-v2</code> directory.  If you are using MathML input rather than TeX (e.g., <code>mml-chtml.js</code> rather than <code>tex-chtml.js</code>), then you can remove <code>input/tex/extensions</code> as well.</p>"},{"location":"javascripts/node_modules/mathjax/#the-component-files-and-pull-requests","title":"The Component Files and Pull Requests","text":"<p>The <code>es5</code> directory is generated automatically from the contents of the MathJax source repository.  You can rebuild the components using the command</p> <pre><code>npm run make-es5 --silent\n</code></pre> <p>Note that since the contents of this repository are generated automatically, you should not submit pull requests that modify the contents of the <code>es5</code> directory.  If you wish to submit a modification to MathJax, you should make a pull request in the MathJax source repository.</p>"},{"location":"javascripts/node_modules/mathjax/#mathjax-community","title":"MathJax Community","text":"<p>The main MathJax website is http://www.mathjax.org, and it includes announcements and other important information.  A MathJax user forum for asking questions and getting assistance is hosted at Google, and the MathJax bug tracker is hosted at GitHub.</p> <p>Before reporting a bug, please check that it has not already been reported.  Also, please use the bug tracker (rather than the help forum) for reporting bugs, and use the user's forum (rather than the bug tracker) for questions about how to use MathJax.</p>"},{"location":"javascripts/node_modules/mathjax/#mathjax-resources","title":"MathJax Resources","text":"<ul> <li>MathJax Documentation</li> <li>MathJax Components</li> <li>MathJax Source Code</li> <li>MathJax Web Examples</li> <li>MathJax Node Examples</li> <li>MathJax Bug Tracker</li> <li>MathJax Users' Group</li> </ul>"},{"location":"lecture/00-Introduction/000-index/","title":"Introduction","text":""},{"location":"lecture/00-Introduction/000-index/#about-computing-something","title":"About Computing Something","text":"<p>Our goal, from a scientific simplistic viewpoint, is to perform the following </p> \\[ \\begin{equation} x \\longmapsto f(x) \\end{equation} \\] <p>In other terms, given an input \\(x\\) we want to transform it into an output \\(f(x)\\).</p> <p>Examples</p> <ul> <li> <p>A constant function \\(f\\) printing <code>Hello World</code>: </p> \\[x \\longmapsto \\text{``Hello World!''}\\] </li> <li> <p>A function computing the exponential of \\(x\\):</p> \\[x \\longmapsto e^x\\] </li> <li> <p>A function \\(f\\) computing the expectation of a random variable \\(X\\) with Monte-Carlo</p> \\[X \\longmapsto \\frac{1}{N} \\sum_{k=1}^N x_k, \\quad x_k \\text{ independently drawn from }\\mathcal{L}aw(X)\\] </li> <li> <p>etc.</p> </li> </ul> <p>Defining, using, composing functions is natural for mathematicians. Yet, the question is whether it is possible to design a machine that will accomplish this task and return the output. The idea of such machines dates back centuries ago, primarily performing basic arithmetic such as finger counting. They were of mechanical nature such as abacus, suanpan (\u7b97\u76d8), Pascaline or various compass and astronomical devices.</p> <p>Many evolutions happened during the 19th and beginning of 20th century, but the first real modern version of a computer how we know now was done by Konrad Suze around 1939-40. He designed a computer using electric signals (with vacuum tubes) rapidly followed by the works of Alan Turing or John von Neumann.</p> <p>Note that this period coincides with an intense work in the mathematical community about the foundations of mathematics. Without entering into the history of computers and their inner functioning, let us notice the following. Mathematics starts with logic, axiomatic set theory, then proceeds to the building of natural numbers \\(\\mathbb{N}\\), rational numbers \\(\\mathbb{Q}\\), real numbers \\(\\mathbb{R}\\), vector spaces, manifolds, etc. and functions on each of these spaces.</p> <p>If one wants to perform computations in a mathematical sense, one would need those elements. Starting with a simple two elements logical Boolean Algebra <code>{True, False}</code>, it holds </p> <ul> <li>Or: <code>True or True = True</code>, <code>True or False = True</code>, <code>False or False = False</code></li> <li>And: <code>True and True = True</code>, <code>True and False = False</code>, <code>False and False = False</code></li> <li>Negation: <code>~True = False</code>, <code>~False = True</code></li> </ul> Boolean algebra or set theoretical viewpoint <p>This can be written in terms of</p> <ul> <li>\\(X = \\{0,1\\}\\) boolean algebra as \\(1 + 1 = 1\\) where \\(1\\) stands for <code>True</code> and \\(+\\) for <code>or</code>. Same for \\(0\\) standing for <code>False</code>, \\(*\\) for <code>and</code>.</li> <li>\\(X = 2^{\\emptyset}=\\{\\emptyset, \\{\\emptyset\\}\\}\\) as \\(\\{\\emptyset\\}\\cup \\{\\emptyset\\} = \\{\\emptyset\\}\\) where \\(\\{\\emptyset\\}\\) stands for <code>1</code> and \\(\\cup\\) for <code>or</code>. Similarly \\(\\emptyset\\) stands for \\(0\\), \\(\\cap\\) stands for <code>and</code> and the complement \\({}^c\\) stands for <code>~</code>.</li> </ul> <p>Starting with these premises, following John von Neumann's method, one can within ZF theory construct natural numbers, and from there integers, rational numbers etc. At least build a finite arithmetic.</p> <p>Hence for a computer to work, one needs in the first place</p> <ol> <li>A \\(0\\) and \\(1\\) (or <code>False/True</code> states)</li> <li>The operations <code>or</code>, <code>and</code> and <code>~</code></li> <li> <p>A way to read the input and output of those operations (1)</p> <ol> <li> This part is for instance one of the challenges in quantum computing.</li> </ol> </li> </ol> <p>As for the \\(0\\), \\(1\\), the invention of electricity allows to produce and measure presence of current (or absence thereof). As for the operations, they are derived from the invention of so called transistors that can generate gates performing those operations.</p> <p>The first attempts tried to get it working in a decimal world. However from this setting it is more natural, efficient and less error prone for a computer to work in base 2 as it is closer to what the circuitry allows, and since mathematically it does not matter what base is used, binary it be.</p>"},{"location":"lecture/00-Introduction/000-index/#talking-to-a-machine","title":"Talking to a Machine","text":"<p>The machine can now deal with finite arithmetic, in other terms, we can do simple \\(x \\mapsto f(x)\\). However, it remains complicated to explain to this machine</p> <ul> <li>what \\(x\\) (the input) is. (Convert into binary sequences)</li> <li>how it shall transform \\(x\\) into \\(f(x)\\) with such basic operations</li> <li>how to read out the output (for instance from binary to decimal and print it on a screen or record it somewhere)</li> </ul> <p>This is where programming languages come into place. Like any language, they are characterized by a syntax (form) and semantics (meaning). One receiving end is the computer with a very rudimentary form (made of 0/1 and operations on it), while the other is a human with sophisticated one. Hence programming languages are often classified from low level (close to the machine language) to high level (close to human language). Here is a personal ranking of programing languages along this dimension</p> <ul> <li> <p>(Extremely) Low level:     Those languages are the closest to the machine code instructions.     They are extremely efficient as there is no overhead between the instructions and the computer.     However, beyond simple but critical operations, it is virtually impossible to express more complex framework in reasonable amount of time.     Classical example of which is the assembly language (before it was even punching cards).     Current applications are rare but very specific (flight instruments, rockets, cryptography, special algorithm).     Example of assembly language<pre><code>Example:  M  ADD  R1, ='3'\nwhere, M - Label; ADD - symbolic opcode; \nR1 - symbolic register operand; (='3') - Literal\n\nAssembly Program:\nLabel  Op-code   operand   LC value(Location counter)\nJOHN   START     200\n       MOVER     R1, ='3'    200\n       MOVEM     R1, X       201\nL1     MOVER     R2, ='2'    202\n       LTORG                 203\nX      DS        1           204\n       END                   205\n</code></pre></p> </li> <li> <p>Low level:      Those programming languages are also of procedural nature but with a more natural syntax and semantic with advanced multipurpose functionalities (control flows, recursion, functions, or advanced data structure).     They remain close enough to the machine to be very efficient.      They do not allow for higher level concepts (templating, objects, etc) and require care how to handle memory.     Typical examples are <code>FORTRAN</code>, <code>C</code> or more recently <code>Rust</code>, <code>CUDA</code>.     They are still very widely used as they are the backbones of many infrastructures and operating system (Linux for instance) as well as scientific libraries.</p> FortranCRust <pre><code>program fibonacci\n    implicit none\n    integer :: n, i\n    integer, allocatable :: fib(:)\n\n    print *, 'Enter the number of terms:'\n    read *, n\n\n    allocate(fib(n))\n\n    fib(1) = 0\n    if (n &gt; 1) fib(2) = 1\n\n    do i = 3, n\n        fib(i) = fib(i-1) + fib(i-2)\n    end do\n\n    print *, 'Fibonacci sequence:'\n    do i = 1, n\n        print *, fib(i)\n    end do\n\n    deallocate(fib)\nend program fibonacci\n</code></pre> <pre><code>#include &lt;stdio.h&gt;\n\nvoid fibonacci(int n) {\n    int t1 = 0, t2 = 1, nextTerm;\n\n    for (int i = 1; i &lt;= n; ++i) {\n        printf(\"%d, \", t1);\n        nextTerm = t1 + t2;\n        t1 = t2;\n        t2 = nextTerm;\n    }\n}\n\nint main() {\n    int n;\n    printf(\"Enter the number of terms: \");\n    scanf(\"%d\", &amp;n);\n    printf(\"Fibonacci Sequence: \");\n    fibonacci(n);\n    return 0;\n}\n</code></pre> <pre><code>fn fibonacci(n: u32) -&gt; u32 {\n    let (mut a, mut b) = (0, 1);\n    for _ in 0..n {\n        let temp = a;\n        a = b;\n        b = temp + b;\n    }\n    a\n}\n\nfn main() {\n    let n = 10; // Example: first 10 Fibonacci numbers\n    for i in 0..n {\n        println!(\"{}\", fibonacci(i));\n    }\n}\n</code></pre> </li> <li> <p>Medium level:      With more complex needs and larger projects, languages have been extended in terms of functionalities such as memory management, object oriented or functional programming.     On the one hand, they often remove many difficulties related to lower level languages such as addressing and managing the memory, implement asynchronous or parallel programming, and make use of objects or more abstract structures.     They are tons of such languages with each its own philosophy.     Most widely known are <code>C++</code>, <code>Java</code>, <code>Haskell</code> (functional programming language), <code>JavaScript</code> (web oriented), etc.     They are still very efficient and used in infrastructure or many web related applications.</p> C++JavaHaskell <pre><code>#include &lt;iostream&gt;\nusing namespace std;\n\nclass Circle {\npublic:\n    Circle(double radius) : radius(radius) {}  // Constructor\n\n    double area() const {\n        return radius * radius * 3.14159;\n    }\n\nprivate:\n    double radius;\n};\n\nint main() {\n    Circle circle(5.0);  // Create a Circle object\n    cout &lt;&lt; \"Area of the circle: \" &lt;&lt; circle.area() &lt;&lt; endl;\n    return 0;\n}\n</code></pre> <pre><code>public class Circle {\n    private double radius;\n\n    public Circle(double radius) {\n        this.radius = radius;\n    }\n\n    public double area() {\n        return radius * radius * Math.PI;\n    }\n\n    public static void main(String[] args) {\n        Circle circle = new Circle(5.0);\n        System.out.println(\"Area of the circle: \" + circle.area());\n    }\n}\n</code></pre> <pre><code>data Circle = Circle Double  -- Circle type with a single constructor\n\narea :: Circle -&gt; Double\narea (Circle radius) = pi * radius ^ 2\n\nmain :: IO ()\nmain = print $ area $ Circle 5.0\n</code></pre> </li> <li> <p>Higher level:     Those programming language takes the previous level type but focus on simplifying the syntax and semantics, removing lot of the compilation/debugging work as well as static typing requirements (specifying the nature of all variable before use).     They are also by definition very dynamic (objects can be declared at running time) and interpreted (scripting language).     Paramount example of which is <code>Python</code> but also <code>Lua</code>, <code>Ruby</code>, etc.     They are also programming languages with specific application at hand such as <code>R</code>, <code>Matlab</code>, <code>Mathematica</code>.</p> PythonRMatlab <pre><code>import numpy as np\n\n# Define an array of numbers\ndata = np.array([1, 2, 3, 4, 5])\n\n# Calculate the mean and standard deviation\nmean = np.mean(data)\nstd_dev = np.std(data)\n\nprint(\"Mean:\", mean)\nprint(\"Standard Deviation:\", std_dev)\n</code></pre> <pre><code># Define a vector of numbers\ndata &lt;- c(1, 2, 3, 4, 5)\n\n# Calculate the mean and standard deviation\nmean &lt;- mean(data)\nstd_dev &lt;- sd(data)\n\nprint(paste(\"Mean:\", mean))\nprint(paste(\"Standard Deviation:\", std_dev))\n</code></pre> <pre><code>% Define an array of numbers\ndata = [1, 2, 3, 4, 5];\n\n% Calculate the mean and standard deviation\nmean_val = mean(data);\nstd_dev = std(data);\n\ndisp(['Mean: ', num2str(mean_val)])\ndisp(['Standard Deviation: ', num2str(std_dev)])\n</code></pre> </li> <li> <p>(Extremely) high level:     <code>ChatGPT</code>.     This is not really a programming language but as a large language model it can perform the task of converting natural language into code.     More advanced models such as AlphaDev from google deepmind code are trained to design algorithm that are more efficient than those written by human beings.     </p> </li> </ul> <p>As you can see, <code>ChatGPT</code>, or any other LLM, is a game changer on how we approach programming languages. </p> <p>So why shall we learn a programming language?</p> <p>Well the answer is two fold. Learning a specific programming language indeed does not make much sense anymore. However learning the foundations of a programming language is important to understand how a machine is generating output, how to design programs and then ask any AI to help you along the way.</p>"},{"location":"lecture/01-Basics-Programing/011-first-steps/","title":"Basic Operations, Data Types","text":"<p>Beforehand, let us just write our first program <pre><code>print(\"Hello world!\")\n</code></pre> Fairly easy, it is a constant function that prints a message.</p> <p>Note that you can put comments in the code that is then ignored by python. These lines to be ignored are indicated by <code>#</code> character</p> <pre><code># We want to print Hello world\nprint(\"Hello World\")\n# We next print a date\nprint(\"Today is 2024-02-14\")\n</code></pre> <p>As your code starts to get longer, it is well advised to comment the most important steps.</p> <ol> <li>When you come back to this code several months later, you can rapidly understand what you did.</li> <li>When others use your code (you just don't code for yourself) they can also rapidly follow what you are coding.</li> </ol>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#variables","title":"Variables","text":"<p>In math, declaring variables is bottom line and done like</p> \\[ \\text{Let }a =\\sqrt{\\pi} \\] <p>In python this is the same way <pre><code>a = 1\nprint(\"The value of a is now:\", a)\n</code></pre></p> <p>Variables are mutable, means that another assignation will override it</p> <pre><code>a = 3\nprint(\"Value of a is\", a)\na = 5\nprint(\"The value of a is now\", a)\n</code></pre> <p>Variable can be assigned to other variables. Those assignations are copies and therefore independent objects</p> <pre><code>a = 5\nb = a\nprint(\"The value of a is\", a, \"and the value of b is\", b)\n# If I reassign a it will change a but not b --&gt;\na = 6\nprint(\"The value of a is now\", a, \"and the value of b remains\", b)\n</code></pre> <p>Equal is not Equal</p> <p>In math the symbol \\(=\\) has a very special meaning, it is a binary relation which is in particular symmetric. But it is also colloquially used as definition such as let \\(x = \\sqrt{\\pi}\\) whereby some use the notation \\(x:=\\sqrt{\\pi}\\). In python <code>=</code> means definition or assignment. It is not a symmetric relation meaning that <code>1 =a</code> (if <code>a</code> was not defined before) will not work. Also <code>a =1</code> and <code>b=2</code>, then <code>a=b</code> is not true mathematically, while in python it just reassigning <code>b</code> to <code>a</code>.</p>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#data-types","title":"Data types","text":"<p>Every programming language distinguishes between the nature of inputs. The basic datatype in python are</p> <ul> <li>boolean: either <code>True</code> or <code>False</code></li> <li>integers: elements of \\(\\mathbb{Z}\\)</li> <li>floats: elements of \\(\\mathbb{R}\\)</li> <li>strings: sequence of characters <code>\"Hello this is Samuel\"</code></li> </ul> \\(\\mathbb{N}\\) still remains out of reach. <p>It is clear from the description of a computer that the range of available numbers is bounded (by memory). In particular integers will only be a bounded subset of \\(\\mathbb{Z}\\). For floats it is even more complicated. Remember that \\(\\mathbb{Q}\\) is constructed from \\(\\mathbb{Z}\\) and \\(\\mathbb{R}\\) is constructed as the quotient of all Cauchy sequences in \\(\\mathbb{Q}\\). Since we only have access to a bounded subset of \\(\\mathbb{Z}\\) we can not even construct full \\(\\mathbb{Q}\\). Furthermore quotients of Cauchy sequences is an operation involving infinity (or the cardinality of \\(\\mathbb{N}\\)) since those are limit objects. Hence, they are not within the realm of a computer. Hence floats is just a subset of \\(\\mathbb{Q}\\) which is even discrete, meaning that the minimal distance between two floats is strictly positive which is not the case of any arbitrary subsets of \\(\\mathbb{Q}\\).</p> <p>Those limitations do also have an impact on operations. Indeed, addition, multiplication are not stable within bounded subsets. Hence, there is a lot of theory (fundamental and applied as well) how to handle this correctly, but from our small viewpoint we assume that we have very large numbers and that everything we do remains within an \\(\\epsilon\\) error. For instance computers do not have \\(0\\) usually...</p> <p>At any time, given a variable <code>x</code> you can assess what type python is considering for it by using the function <code>type(x)</code>.</p> <pre><code>my_bollean = False\nmy_integer = 25\nmy_float = 3.1\nmy_string = \"My string\"\n# and you can print them as well as the type\n\nprint(\"my_boolean = \", my_bollean)\nprint(\"my_integer = \", my_integer)\nprint(\"my_float = \", my_float)\nprint(\"my_string =\", my_string)\nprint(\"With types:\", type(my_bollean), type(my_integer), type(my_float), type(my_string))\n</code></pre> <p>Note that some operations propagate accross types. In math, since \\(\\mathbb{Z}\\subseteq \\mathbb{R}\\) as a group, the addition between an integer \\(n\\)  in \\(\\mathbb{Z}\\) and a real number \\(x\\) in \\(\\mathbb{R}\\) is well defined as an element of \\(n+x\\) in \\(\\mathbb{R}\\). Python does the same automatically by casting the type in the correct space whenever it is unambiguous. <pre><code>a = 1\nb = 3.2\nprint(a+b)\nprint(type(a), type(b), type(a+b))\n</code></pre></p> Further use <pre><code>print(\"Integer positive negative\")\nprint(1, 2, -3)\nprint(\"of type\")\nprint(type(-3))\nprint(\"Floats with different declarations\")\nprint(1.7, 10e-2, -3.0)\nprint(\"of type\")\nprint(type(1.7))\n</code></pre>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#the-special-case-of-strings","title":"The special case of strings","text":"<p>Strings in programming languages is a complex datatype. Indeed, their length is not exactly defined (from a couple of characters for a name, to several millions for a book). In python, each datatype is a class (we will see that later), hence they have many additional functions that can be called on themselves. A particular one in the case of strings is that they can be partially declared and formatted afterwards.</p>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#declaration","title":"Declaration","text":"<p>Strings are declared in several ways</p> <ul> <li>Between single quotes: <code>'my string'</code></li> <li>Between triple single quotes: <code>'''my string'''</code></li> <li>Between double quotes: <code>\"my string\"</code></li> <li>Between triple double quotes: <code>\"\"\"my string\"\"\"</code></li> </ul> <pre><code>string1 = 'Hello'\nstring2 = \"Samuel.\"\nstring3 = \"\"\"Your email address is 'sdrapeau@saif.sjtu.edu.cn'. How are you doing on this \"2024-02-19\"?\"\"\"\nprint(string1, string2, string3)\n</code></pre> Single, double, triple, triple double quotes? What the heck! <p>There are kilometers of documentation and webpage about it, you can have a look. However the basic issue is that if a string contains itself a character like <code>'</code> or <code>\"</code> the declaration becomes ambiguous: <code>'I'm Samuel'</code>, <code>\"Let us meet at the \"Da Pino\" restaurant\"</code>. In this case, those characters conflicting with the declaration of the string should be escaped (with another character, which again raises the question how to escape this escape character...). For the sake of simplicity, the triple (single/double) quotes allow you to define inside the string quotes as long as they are strictly less than three: <code>'''I'm Samuel'''</code>, <code>\"\"\"Let us meet at the \"Da Pino\" restaurant\"\"\"</code>. Rule of thumb:</p> <ul> <li>Use double quotes for simple strings or messages where you know that there won't be double quotes inside</li> <li>If you want to use single quotes, no pb, just follow the same argumentation than for the first point</li> <li>Use double quote for simple <code>f-strings</code> (see after)</li> <li>For large string or complex <code>f-strings</code> or formated strings use triple double quote</li> </ul>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#concatenate-strings","title":"Concatenate strings","text":"<p>While <code>1+3.5</code> works as expected as mentioned above, <code>1 + \"Samuel\"</code> will result into an error that addition is not well defined between and integer and a string. However, the addition between strings is well defined in the sense that it results in concatenation.</p> <pre><code>my_name = \"Samuel\"\nfirst_string = \"Hello\"\nlast_string = \"How are you?\"\nconcatenated = first_string + \" \" + my_name + \"! \" + last_string\nprint(concatenated) # will print `Hello Samuel! How are you?`\n</code></pre>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#formatted-strings","title":"Formatted strings","text":"<p>Strings in combination with the print statement are extremely useful to track what your program is doing: a primitive yet powerful debugging method. To handle strings dynamically, python allows to predefine strings that can be later formatted. Mathematically these are function of the kind</p> \\[ f(name, date) = \\text{Hello }\\{name\\}\\text{! Today we are }\\{date\\} \\] <p>which produces</p> \\[ f(\\text{Samuel}, \\text{2024-02-14}) = \\text{Hello Samuel! Today we are 2024-02-14} \\] <p>You can declare strings to be formatted with double quotes, triple single quotes or triple double quotes. It is usually better to use triple double quotes. Place holders for the variables are indicated with curly brakets <code>{&lt;...&gt;}</code> Since strings are class object, you can access to the function <code>format</code> to format it</p> <pre><code># Declare the string to be formatted.\n# The place holders and names for each variables are between curly brackets\nto_format_string = \"\"\"\nHello {name}!\nYour birthdate is {birthdate}\nYou are {age} years old.\nYou weight {weight}.\n\"\"\"\n\n# you can pass strings, integers, floats as well as simple operations in the variables.\nfirst_string = to_format_string.format(\n    name = 'Samuel',\n    birthdate = '1977-05-23',\n    age = 46,\n    weight = 60+25\n)\nsecond_string = to_format_string.format(\n    name = 'Irina',\n    birthdate = '1992-09-15',\n    age = 31,\n    weight = 70-15\n)\nprint(first_string)\nprint(second_string)\n</code></pre>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#shortcut-f-strings","title":"Shortcut: <code>f-strings</code>","text":"<p>Instead of using a formatted string as above, another way is to use the shortcut <code>f-strings</code>. The main difference is that the variable inside an <code>f-string</code> must be declared before. The declaration is usually done with a single double quote of a triple double quote if the string might be complex.</p> <pre><code># pre-declare the variables\nname = \"Samuel\"\nage = 36\n# get the formated string without the format function\nmy_string  = f\"Hello {name}, you are {age} years old.\"\nprint(my_string)\n</code></pre>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#advanced-formatting","title":"Advanced formatting","text":"<p>Beyond just putting variables at different places, the format functionality allows to provide additional formating, in particular for numbers. This is usually done by adding <code>{&lt;...&gt;:&lt;some format code&gt;}</code>.</p> <pre><code>num1 = 1\nnum2 = 73/7\nnum3 = 0.45622\n\nmy_string = \"\"\"\nWe have:\nAn integer: {number1}\ntwo floats: {number2} and {number3}\n\nI can appy formating with :.xf or :.x% where\n - x represent the number of significant digits\n - f/% represent if float formating or percent formating (will multiply by 100)\n\nFormated Floats (works for int): {number1:.2f} {number2: .4f}\nFormated percent: {number3:.2%} and {number3:.0%}\n\"\"\"\n\nresult = my_string.format(number1 = num1, number2 = num2, number3 = num3)\nprint(result)\n</code></pre>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#operators","title":"Operators","text":"<p>Now that we have datatype and can define variable, we address the basic operators necessary for manipulating these variables.</p>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#arithmetic-operators","title":"Arithmetic operators","text":"<p>The simple Arithmetic operators are just what it means.</p> Symbol Task Performed + Addition - Subtraction / Division * Multiplication % Modulus // Floor division ** To the power of Euclidean division <p>As for the Euclidean division, remember that for two integers \\(a\\) and \\(b\\neq 0\\), there exists unique integers \\(m\\) and \\(n\\) such that</p> <ul> <li>\\(a = m  b + n\\)</li> <li>\\(0\\leq n &lt; |b|\\)</li> </ul> <p>In python this Euclidean division is performed using the floor division and the modulus, that is <code>a = a//b * b + a%b</code></p> <pre><code>a = 5\nb = 3\n\n# in the following f-string the symbol \\t stands for a tab space (usually 4 spaces)\nprint(f\"\"\"\naddition:\\t a+b = {a+b}\nsubstraction:\\t a-b = {a-b}\ndivision:\\t a/b = {a/b}\nmultiplication:\\t a*b = {a * b}\npower:\\t a^b = {a**b}\nmodulus:\\t a mod b = {a%b}\nfloor division:\\t a '//' b = {a//b}\nHence: \\t a//b * b+ a%b = {a//b * b + a%b}\n\"\"\")\n</code></pre>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#relational-operators","title":"Relational Operators","text":"<p>The relational operators are binary relations. In math, a binary relation \\(R\\) on some product set \\(X\\times Y\\) returns for each tuple \\((x,y)\\) either \\(1\\) (true) or \\(0\\) (false) written as \\(xRy\\) if \\(1\\). The classical but fundamental operators are as follows</p> Symbol Task Performed == True, if left and right sides are equal != True, if left and right are not equal &lt; Strictly less than &gt; Strictly greater than &lt;= Less than &gt;= Greater than <pre><code># define z with equal\nz = 1\n\n# test is z is equal to 1 WITH two consecutive equal ==\nprint(f\"\"\"\nz is equal to 1: {z == 1}\nz is equal to 1: {1 == z} # note that it is as expected symetric\nz is different of 2: {z != 2}\nz is strictly greater than 4: {z &gt; 4}\n\"\"\")\n</code></pre>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#bitwise-operators","title":"Bitwise Operators","text":"<p>Bitwize operation are also binary relations related to logic. They are called bitwize since they are defined on the logic Boolean algebra <code>{True, False}</code></p> Symbol Task Performed &amp; Logical and l Logical or ^ xor ~ Negate Note <p>Note that <code>xor</code> also called exclusive <code>or</code> is True only when the left and right are strictly different. In math these operators are usually denoted by \\(\\wedge\\) for <code>and</code>, \\(\\vee\\) for <code>or</code> and \\(\\neg\\) for <code>~</code> and \\(\\not \\leftrightarrow\\) for <code>xor</code>. <code>xor</code> is redundant as it can be expressed in terms of the three other operators</p> \\[ \\begin{equation}     A \\not \\leftrightarrow B = (A \\vee B)\\wedge \\neg(A \\wedge B) = (A\\wedge \\neg B)\\vee(\\neg A \\wedge B) \\end{equation} \\] <p><pre><code>print(f\"\"\"\nTrue or False is: {True | False}\nTrue and False is: {True &amp; False}\nThe negation of True is: {~True}\nTrue xor False (exclusive or): {True ^ False}\nTrue xor True: {True ^ True}\n\"\"\"\n)\n</code></pre> The main use of those bitwize operators is within control flows we will see later in combination with the relational operators <pre><code>a = 1\nb = 2\n\nprint(f\"a is equal to 1 and b is different of 3 is {(a == 1) &amp; (b != 3)}\")\nprint(f\"a is negative and b is different of 3 is {(a &lt;=0) &amp; (b != 3)}\")\n</code></pre></p>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#data-structures","title":"Data Structures","text":"<p>Aside basic data types such as int, floats or strings, python allows for more advanced data structures, which can be considered as containers of variables. They come in four forms: tuple, lists, sets, dictionaries. They distinguish themselves by the properties</p> <ul> <li>Mutable: can be modified (append, delete, change value)</li> <li>Ordered: if an inherent order is defined for the objects in the container</li> <li>Duplicates: if elements in the container can have duplicated values.</li> <li>Heterogeneous: if elements can have different types</li> </ul> Tuple Lists Dictionaries Sets Mutable Ordered Duplicate Heterogeneous <p>With these properties in mind, the most widely used object remains lists and dictionaries. Tuple are also often used but less in the sense of a data structure, but rather as input/output of functions.. Sets are seldom used, only if you want to constructs collections without duplicates and do not care about ordering.</p> <p>We therefore present tuple, lists and dictionary thereafter and shortly tuple and sets in the end.</p>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#lists","title":"Lists","text":"<p>Lists can be viewed as arrays of variables that can be of mixed type. Those arrays can be expanded, shrunken, modified. They are ordered as the elements are all associated with an index starting from <code>0</code> and ending with <code>N-1</code> where <code>N</code> is the length of the list.</p> <ul> <li>Declare: between squared parenthesis <code>x = [x0, x1, ..., x_{N-1}]</code></li> <li>Properties: function length <code>len(x)</code> returns the length of the list <code>N</code></li> <li>Access: <ul> <li>element <code>x[k]</code> represent the value of the array at the <code>k</code>th. position</li> <li>backward: <code>x[-1]</code> is for <code>x[N-1]</code>, <code>x[-2]</code> is for <code>x[N-2]</code> etc.</li> </ul> </li> <li>Slice: extract sub array:<ul> <li><code>x[start:end]</code> is equal to the array <code>[x[start], x[start+1],..., x[end-1]]</code></li> <li><code>x[:end]</code> is equal to the array <code>[x[0], ..., x[end-1]]</code></li> <li><code>x[start:]</code> is equal to the array <code>[x[start], ..., x[N-1]]</code></li> </ul> </li> </ul> <p>Warning</p> <p>Remember that arrays in python are enumerated starting from <code>0</code> and not <code>1</code> like is usual in math. Hence, the last element of an array is <code>x[N-1]</code> for an array of size <code>N</code></p> <pre><code># declare an empty array\nx = []\nprint(f\"Empty array {x} of length {len(x)} and type {type(x)}\")\n\n# declare a heterogeneous array\nx = ['apple', 'orange', 25, 5.4]\nprint(f\"\"\"\nHetegrogeneous array: {x}\nof length: {len(x)}\nand type {type(x)}\n\"\"\")\n\n# Access values\nprint(f\"\"\"\nFirst element:\\t {x[0]}\nThird element:\\t {x[2]}\nLast element:\\t {x[-1]}\n\"\"\"\n)\n\n# Slice\nprint(f\"\"\"\nSlice from 1 to 3 (exclusive): {x[1:3]}\nSlice from begining to 3 (exclusive): {x[:3]}\nSlice from 2 (inclusive) to end: {x[2:]}\n\"\"\")\n</code></pre> <p>Since lists are mutable, you can </p> <ul> <li>Change values:      <code>x[k] = some_val</code> change the value of the element at the <code>k</code>th position to <code>some_val</code></li> <li>Append values:      <code>x.append(some_val)</code> append <code>some_val</code> at the end of the array.     The new length is modified to <code>N+1</code></li> <li>Insert values:      <code>x.insert(k, some_val)</code> insert <code>some_val</code> at position <code>k</code>.     Each previous value between <code>k</code> and <code>N-1</code> is shifted to positions <code>k+1</code> and <code>N</code> with a new length of <code>N+1</code></li> <li> <p>Remove specific item:      <code>x.remove(some_val)</code> removes <code>some_val</code> from the array if it is present.     If there are multiple instance of which, it will remove the first occurrence.(1)</p> <ol> <li> Be careful with this function as it is not bijective due to the fact that duplicate values might be in the array</li> </ol> </li> <li> <p>Remove specific indexed value:     <code>x.pop(k)</code> remove the <code>k</code>th element resulting in <code>N-1</code> length array.(1)     Furthermore, while the function <code>x.pop(k)</code> remove the item it also returns the value.     In other term <code>val = x.pop(k)</code> result in <code>x</code> without the <code>k</code>th value and <code>val</code> containing the former value <code>x[k]</code>.</p> <ol> <li> This is the preferred way as it is explicit.</li> </ol> </li> </ul> <pre><code>x = ['apple', 'orange', 25, 5.4]\n\n# append the string \"new value\" at the 4th position of the array\nx.append(\"new value\")\nprint(f\"\"\"\nx after appending \"new value\" at the last position:\n{x}\n\"\"\")\n\n# insert an integer 25 at position 1\nx.insert(1, 25)\nprint(f\"\"\"\nx after inserting 25 at position 1:\n{x}\n\"\"\")\n\n# remove the value 25 from the array (be carefull there are two so it removes the first)\nx.remove(25)\nprint(f\"\"\"\nx after removing (the first occurence of) 25:\n{x}\n\"\"\")\n\n# remove the element at position 1\nval = x.pop(1)\nprint(f\"\"\"\nx after removing the value {val} at position 1:\n{x}\n\"\"\")\n</code></pre> <p>Since list elements can be made of any type, you can also construct list of lists as well as concatenate them as strings.</p> <pre><code>x = [1, 2]\ny = ['carrot','potato']\nz = [x,y]\nconcat = x+y\nprint(f\"\"\"\nFirst list x:\\t{x}\nSecond list y:\\t{y}\nList of lists [x, y] =\\t{z}\nAccessing the second element of the first list in [x, y]:\\t{z[0][1]}\nConcatenation of lists x + y:\\t{concat}\n\"\"\"\n)\n</code></pre>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#dictionaries","title":"Dictionaries","text":"<p>Dictionaries are collections of <code>key/value</code> pairs. Like in a classical dictionary where a specific word is associated to a definition. As words in a classical dictionary, keys are unique, however values can be non unique. It is similar to a list where the integer indices of the list are replaced by unique however arbitrary keys.</p> <ul> <li>Declare: between curly brakets <code>x = {'keys': val1, 'key2' = val2}</code></li> <li>Properties:<ul> <li>function length <code>len(x)</code> returns the length of the dictionary (is however of limited use)</li> <li>function listing keys <code>x.keys()</code> returns the list of the keys.</li> </ul> </li> <li>Access: <ul> <li>element <code>x['key']</code> represent the value of the dictionary for the key <code>key</code></li> </ul> </li> </ul> <pre><code># declare an empty dictionary\nx = {}\nprint(f\"Empty dictionary {x} of length {len(x)} and type {type(x)}\")\n\n# declare a dictionary\nx = {'fruit': 'apple', 'vegetable': 'cucumber'}\nprint(f\"\"\"\nDictionary: {x}\nof length: {len(x)}\nand type {type(x)}\n\"\"\")\n\n# Access values\nprint(f\"\"\"\nValue of key fruit:\\t {x['fruit']}\nValue of key vegetable:\\t {x['vegetable']}\nList of the keys:\\t {x.keys()}\n\"\"\"\n)\n</code></pre> <p>As lists, dictionary are mutable, you can </p> <ul> <li>Change values:      <code>x['key'] = some_val</code> change the value for the key <code>key</code> to <code>some_val</code></li> <li>Add values:      <code>x['new_key'] = val</code> adds a new key (if it does not exists otherwize it overwrites the existing value) with value <code>new_val</code>.</li> <li>Remove specific key/value:     <code>x.pop('key')</code> remove the key value for key <code>key</code>.</li> </ul> Info <p>There are other methods for dictionary such as <code>x.popitem()</code> that removes the last inserted key. Even if in Python a dictionary is considered as not ordered, it keeps internally an order. Therefore in terms of philosophy this method should not be used.</p> <pre><code>x = {'fruit': 'apple', 'vegetable': 'cucumber'}\n\n# change the value 'apple' of key `fruit` to `orange`\nx['fruit'] = orange\nprint(f\"\"\"\nDictionary with new value orange for key fruit: {x}\n\"\"\")\n\n# Add a new item `meat` with value `steak`\nx['meat'] = 'steak'\nprint(f\"\"\"\nx after adding a new key/value pair:\n{x}\n\"\"\")\n\n# remove the key `vegetable` from the dictionary\nval = x.pop['vegetable']\nprint(f\"\"\"\nx after removing the key `vegetable` with value {val}:\n{x}\n\"\"\")\n</code></pre> <p>As lists, the value of every key/value pair in a dictionary can be of every nature. In other terms you can build for instance dictionaries of dictionaries that are quite often used for web development (a form of no-sql structure for data).</p> <pre><code>x = {'fruit': 'apple', 'vegetable': 'cucumber'}\n\n# Adding a subdictionary\nx[\"condiments\"] = {\"salty\": \"salt\", \"sugary\": \"caramel\"}\nprint(f\"\"\"\nWe now have a dictionary with a sub dictionary\n{x}\nThe subdictionarry can be accessed\n{x['condiments']}\nand its elements too\n{x['condiments']['salty']}\n\"\"\"\n)\n</code></pre>"},{"location":"lecture/01-Basics-Programing/011-first-steps/#tuples","title":"Tuples","text":"<p>Tuples are similar to list up to the fact that once declared they are immutable. Values can not be changed, tuples can not be extended or shrunk.</p> <ul> <li>Declare: between parenthesis <code>x = (x0, x1, ..., x_{N-1})</code> or just as sequence separated by commas <code>x = x0, x1, ..., x_{N-1}</code>.</li> <li>Properties: function length <code>len(x)</code> returns the length of the list <code>N</code></li> <li>Access: <ul> <li>element <code>x[k]</code> represent the value of the tuple at the <code>k</code>th. position</li> <li>backward: <code>x[-1]</code> is for <code>x[N-1]</code>, <code>x[-2]</code> is for <code>x[N-2]</code> etc.</li> </ul> </li> </ul> <pre><code># declare an empty tuple\nx = ()\nprint(f\"Empty tuple {x} of length {len(x)} and type {type(x)}\")\n\n# declare a heterogeneous tuple\nx = ('apple', 'orange', 25, 5.4)\n# alternatively\nx = 'apple', 'orange', 25, 5.4\n\nprint(f\"\"\"\nHetegrogeneous tuple: {x}\nof length: {len(x)}\nand type {type(x)}\n\"\"\")\n\n# Access values\nprint(f\"\"\"\nFirst element:\\t {x[0]}\nThird element:\\t {x[2]}\nLast element:\\t {x[-1]}\n\"\"\"\n)\n</code></pre> <p>Tuple can be decomposed <pre><code>x = 3.5, 7.6, 0\na, b, c = x\n\nprint(x)\nprint(a, b, c)\n</code></pre></p>"},{"location":"lecture/01-Basics-Programing/012-control-flows/","title":"Control Flow Statements","text":"<p>So far we have datatype for variables and basic operations to deal with. However, in order to perform computations beyond simple arithmetic we need to deal with things such as</p> \\[ \\begin{align} u_0 &amp; = x\\\\ u_{n+1} &amp;=  \\begin{cases} \\alpha u_n + \\beta &amp; \\text{ if } u_n&gt;0\\\\ -\\alpha u_n -\\beta &amp; \\text{otherwize} \\end{cases} \\end{align} \\] <p>which are the backbones for functions and computation overall. Such a simple construction requires the computer to master two things</p> <ol> <li>Conditional evaluation (the if section)</li> <li>Induction (go from n to n+1)</li> </ol> <p>From a programming language viewpoint these two fundamental flows are implemented in different form but are all present. The two basic ones are</p> <ul> <li><code>if then else</code> Conditional evaluation</li> <li><code>for loops</code> inductions</li> </ul> <p>For the moment we will concentrate on these two as they are the prominent ones.</p> Other control flows <p>Control flows are not bounded to these two ones. You also have in different programming languages as well as python some of those</p> <ul> <li><code>while</code> do something while a condition is true</li> <li><code>case</code> choice between alternatives (no in python in this form but in combination with match)</li> <li><code>match</code> match a condition to a branch of code</li> <li><code>go to</code> (fortran style, not in python) allows to jump in the code</li> <li><code>while</code> do something while a condition is True</li> </ul> <p>One can also mention <code>try</code> in combination with <code>except</code> to catch errors.</p> <p>Furthermore those control flows are paired with <code>break</code>, <code>continue</code>, <code>pass</code> etc. which we will see later in the lecture.</p>"},{"location":"lecture/01-Basics-Programing/012-control-flows/#if-then-else","title":"If, then, else","text":"<p>This conditional flow allows to evaluate if something is true or false and acts upon it</p> <pre><code>x = 6\nif x &gt; 5:\n    print(f\"{x} is indeed greater than 5, fantastic\")\n\nx = 3\nif x &gt; 5:\n    print('fantastic')  # is not printed since now x&lt;5\nprint(\"this line is executed since it is out of the scope of the if statement\")\n</code></pre> <p>Warning</p> <p>The evaluation is given by <code>if ....:</code> and then the code to implement if <code>True</code> is processed below but with indentation (at least 2 space, usually one tab which is 4 spaces). Normally your code editor register it. It is elegant however prone to errors since the beginning and the end of the condition is given by this indentation. Indeed, whether true or false everything after which is not indented will be processed. </p> <p>We can add an alternative execution if the result of the evaluation is <code>False</code> using the keyword <code>else</code>. We can also compose the control flow with corresponding indentation. <pre><code>x = 6\nif x != 6:\n    print(f'cool {x} is indeed different of 6')\nelse:\n    print(f'Uncool {x} is equal to 6')\n\n# Combining conditions\nx = -2\nif x &lt; 0:\n    if x &lt; -1:\n        print(\"super cool\")\n    else:\n        print('Also ok')\nelse x &lt; 1:\n    print('cool')\n</code></pre> You can also combine several conditions using the <code>elif</code> statement that will evaluate an alternative statement before going forward <pre><code>x= 5\nif x == 6:\n    print(f\"cool {x} is equal to 6\")\nelif x&lt;6:\n    print(f\"that ok, {x} is still smaller than 6\")\nelse:\n    print(f\"that's not cool {x} is greater than 6\")\nprint(\"The program continues\")\n</code></pre></p> <p>Warning</p> <p>Be aware that this control flow will be executed sequentially. In other terms it evaluates one condition after the other. The first branch evaluated as <code>True</code> will be executed and the program then jumps out of the control flow to continue at the first lower level of indentation.</p> <pre><code># if x is smaller than 1 you want to print cool\n# if x is smaller than 0 you want to print super cool\n# otherwize this is uncool\n\n# wrong implementation for x = 0.5 where you should print super cool\nx = -1\nif x &lt; 1:\n    print(\"Cool\")\nelif x&lt;0:\n    print(\"Super cool\")\nelse:\n    print(\"not ok it is greater or equal than 1 \")\nprint(\"End of wrong condition flow\")\n# in this conditional sequence the first evaluation is true and executed\n# and then the statement is exited\n\n# The right way to do it is to evaluate from larger to narrower condition\n\nx = -1\nif x &lt; 0:\n    print(\"Super cool\")\nelif x&lt;1:\n    print(\"Cool\")\nelse:\n    print(\"not ok it is greater or equal than 1 \")\nprint(\"End of correct condition flow\")\n</code></pre>"},{"location":"lecture/01-Basics-Programing/012-control-flows/#for-loop","title":"For loop","text":"<p>For loops allows to realize induction (at least bounded). The idea is to iterate through numbers and do something. In order to do so, we need to define the object over which we iterate (is called an iterator). In any programming languages you can iterate through a range of integers. In python such an iterator of integers is called a <code>range</code>. With such an iterator at hand we can loop through them one element after the other using <code>for n in range(5)</code></p> <pre><code>my_range = range(5) # a range of integers from 0 to 4\nprint(my_range)\n\n# we can now loop through it\nfor n in range(5):\n    print(f\"We are at the stage {n} of the loop\")\n    print(f\"We can conduct operations for instance squaring n resulting in {n**2}\")\n\nmyrange = range(1,5) # a range of integers from 1 to 4\nprint(my_range)\n</code></pre> <p>In any programming languages, integer iterators are defined and the backbone of computations. However as python is a higher level programming language, iterators can also be of higher levels. In particular, lists or dictionaries are iterators.(1)</p> <ol> <li> Be aware that dictionaries are assumed to be unordered.     However they have an internal counter about the sequence at which key/values have been inserted and the iterator will take this order.</li> </ol> <pre><code>list_of_lists = [[1, 2, 3],\n                 [4, 5, 6],\n                 [7, 8, 9]]\n\nfor x in list_of_lists:\n    print(f\"Sublist {x}\")\n\n# alternative\nlen(list_of_lists)\nfor i in range(len(list_of_lists)):\n    print(f\"Sublist {list_of_lists[i]}\")\n</code></pre> <p>Remember that for lists and dictionary both are the same with the exception that the index in lists is a key in dictionary. When you enumerate in both of them you can access to the (index or key)/value using a tuple.</p> <ul> <li>Lists: <code>enumerate(x)</code> returns an interator <code>(n, x[n])</code> for  <code>n</code> in <code>range(len(x))</code></li> <li>Dictionary: <code>x.items()</code> returns an iterator <code>(key, val)</code> for each <code>key</code> in <code>x.keys()</code></li> </ul> <pre><code>my_list = ['first val', 3, 5.6]\n\nfor idx, val in enumerate(my_list):\n    print(f\"value {val} of the list at index {idx}\")\n\nmy_dict = {'fruit': \"apple\", 'vegetable': 'cucumber'}\n\nfor key, val in my_dict.items():\n    print(f\"Value {val} of the dictionary for the key {key}\")\n</code></pre>"},{"location":"lecture/01-Basics-Programing/012-control-flows/#example","title":"Example","text":"<p>The Fibonacci sequence, is a recursive sequence \\(u_0,u_1,\\ldots, u_n,\\ldots\\) given by</p> \\[ \\begin{equation}   u_{n+2}=u_{n+1}+u_n  \\end{equation} \\] <p>with start values \\(u_0=a\\) and \\(u_1=b\\).</p> <p>We compute and print the ten first Fibonacci numbers</p> <pre><code>a = 1\nb = 2\n\nfor n in range(1,10):\n    # temporary store the value of u_{n+1} into c\n    c = b\n    # assign u_n + u{n+1} into b\n    b = a + b\n    # assign the value u_{n+1} into b\n    a = c\n    # after these operations, given a = u_n and b = u_{n+1}\n    # you shift a = u_{n+1} and b = u_n + u_{n+1} = u_{n+2}\n    print(f\"The {n}th Fibonacci number if {a}\")\n\n# more elegant implementation using the fact that you can allocate tuples\na = 1\nb = 2\nfor n in range(1,10):\n    a, b = b, a+b\n    print(f\"The {n}th Fibonacci number if {a}\")\n</code></pre>"},{"location":"lecture/01-Basics-Programing/012-control-flows/#assignment-by-loops","title":"Assignment by loops","text":"<p>Python has a very handy way to build up objects with <code>in line</code> loops. This is practical but can not be extended to complex constructions.</p> <p>Suppose that we want to plot the function \\(x \\mapsto f(x) = x^2\\). In order to do so, you need the graph, that is a list a values of \\(x\\) in a given range as well as a list of corresponding values of \\(x^2\\).</p> <pre><code># construct a list of equidistant 11 values of x between 0 and 1 iterating over some range\nx_axis = [n /10 for n in range(11)]\n# get the square of them by iterating over x_axis\ny_axis = [x**2 for x in x_axis]\n\nprint(f\"\"\"\nThe x axis values are:\\t{x}\nThe y axis values are:\\t{y} \n\"\"\")\n</code></pre>"},{"location":"lecture/01-Basics-Programing/013-functions-classes/","title":"Functions and Classes","text":"<p>Remember that from the very beginning, from a mathematical viewpoint, we want to implement the following</p> \\[ \\begin{equation}  x \\longmapsto f(x) \\end{equation} \\] <p>In other terms we want to compute the value of a function. Given an input \\(x\\), transform it to get an output \\(f(x)\\). Such a shorthand writing is colloquial in mathematics, however, it is not correct. To define a function you write the following </p> \\[ \\begin{equation} \\begin{split} f \\colon X  &amp; \\longrightarrow Y\\\\ x &amp; \\longmapsto f(x) \\end{split} \\end{equation} \\] For the mathematicians among you <p>Mathematically, a function is defined as a graph \\(\\mathrm{Graph}(f) \\subseteq X\\times Y\\) with the property that for any \\(x\\) in \\(X\\), there exists a unique \\(y\\) in \\(Y\\) such that \\((x, y)\\) is in \\(\\mathrm{Graph}(f)\\). Aside from this definition, it means that a function is well defined given a domain \\(X\\) and an image \\(Y\\).</p> <p>Functions are fundamental objects in mathematics in particular in terms of composition. Indeed given functions</p> \\[ \\begin{equation} X \\xrightarrow{f} Y \\xrightarrow{g} Z \\end{equation} \\] <p>We can consider the function \\(f\\circ g\\) given by</p> \\[ \\begin{equation*}     \\begin{split}         g\\circ f \\colon X &amp;  \\longrightarrow Z\\\\         x &amp; \\longmapsto g\\circ f (x) = g\\left(f\\left(x\\right)\\right)      \\end{split} \\end{equation*} \\] <p>Starting from this we can consider more complex structures such as</p> \\[ \\begin{align*}     f \\colon X \\times Y &amp; \\longrightarrow Z &amp; g \\colon I &amp; \\longrightarrow Y\\\\                 (x, y) &amp; \\longmapsto f(x, y) &amp; i &amp; \\longmapsto g(i) \\end{align*} \\] <p>then you can construct the following composition</p> \\[ \\begin{equation*}     \\begin{split}         h\\colon X \\times I &amp; \\longrightarrow Z\\\\                 (x, i) &amp; \\longmapsto h(x, i) = f(x, g(i))     \\end{split} \\end{equation*} \\] <p>It turns out that in programming definition and composition of functions is the fundamental backbone to get results.</p> <p>In math we still use shorthand writting whenever it is clear from the context what is meant. In a paper about number theory, writing \"Let \\(f(n) := 2n +1\\)\" is clear that you consider as domain \\(\\mathbb{N}\\) and image \\(\\mathbb{N}\\). It is elegant and for the reader there is no ambiguity. However a computer is a dumb machine for which you should exactly tell what is what.</p> <p>Hence, domain \\(X\\), image \\(Y\\) and the action itself \\(f\\) are requirements for a well defined function. For ease of notations, it turns out that python adopts from the beginning the shorthand writing exposed above. In other terms you don't need to specify the domain and image of a function explicitly. The compiler will figure out by itself according to the nature of the input and the function itself:</p> <ol> <li>are the inputs valid?</li> <li>can the inputs be processed through the function?</li> <li>the type of the output?</li> </ol> <p>This is a daunting task to ask for a programming language if you think about it. This is one of the main reason why Python is considered as a high level programming language.  It has smart way to check for any step above, yet, it comes at efficiency costs and sometimes issues in terms of predictability has it takes control of the procedure that you may not have thought about.</p> <p>Example</p> <p>If you define <code>x=1</code> and <code>y=3.5</code>, then <code>x</code> is of integer type while <code>y</code> is of float type. Nevertheless you can define <code>x+y</code> which is equal to <code>4.5</code> which is natural, however combines a priori in an operation elements from different domains, and the result is a float. Yet, if you define <code>y = \"hello\"</code> to be a string, <code>x+y</code> is going to be an error as there is no way to infer for the compiler how to sum an integer with a string.</p>"},{"location":"lecture/01-Basics-Programing/013-functions-classes/#functions","title":"Functions","text":"<p>A function in a programming language is exactly the same as in math, it takes and input and return an output. A function is define using <code>def f(x):</code> where <code>def</code> is a keyword indicating that we define a function, <code>f</code> is the name of the function and <code>x</code> are the variables or input to be considered for this function. The content of the function is then specified with an indentation to delimit the scope of this function. After definition, this function can always be called with <code>f(x)</code> for a specific value of <code>x</code> just as in math.</p> <p>The most basic function (I won't speak about the empty function) is the constant function. Constant function takes no input or whatever input and return the same output. Since python do not need to specify domain and image it is just a universal function so to say.</p> <p><pre><code># A constant function that just print a message\ndef hello():\n    print(\"Hello World!\")\n\n# we now call the function\nhello()\n</code></pre> Constant functions are called <code>void</code> function in programming language however it might be confusing sometimes as in python they can act on variables declared outside the scope of the function. <pre><code>x = 1\n# function with no input\ndef hello():\n    print(\"Hello World!\")\n    # add one to x\n    x = x + 1\nprint(f\"Value of x before calling the function {x}\")\n# call the function\nhello()\nprint(f\"Value of x after calling the function {x}\")\n</code></pre> Most low level programming languages do not allow you to use variables outside the scope and the input of the function. This is however not the case for python, hence care here! As a rule of thumb, never use a variable declared outside the scope of the function and not an input.</p> <p>If the function takes an input it is declared in the function definition and it can be used in the scope of the function.</p> <pre><code>def hello_user(username):\n    print(f\"Hey {username}!\")\n    print(f\"{username}, How do you do?\")\n\nhello_user(\"Samuel\")\nhello_user(\"Linda\")\n</code></pre> <p>However the most interesting functions are those that return an output rather than print something. In order to return a value, the function must be ended with the keyword <code>return</code>. In this case, you can also use composition</p> <pre><code>def multiply(x, y):\n    c = x*y\n    return c\n\nresult = multiply(7,3)\nprint(result)\n# now we can compose to compute 3 * (2 * 3)\nprint(multiply(3,multiply(2,3)))\n</code></pre> <p>Fibonacci</p> <p>Provide a function that returns the <code>n</code>th Fibonacci number <pre><code>def fibbo(a0, b0, n):\n    a = a0\n    b = b0\n    if n != 0:\n        for i in range(1,n):\n            a, b = b, a+b\n    result = a        \n    return result\n\n# return the 10th fibonacci number\nfibbo(10,-5, 10)\n</code></pre></p> <p>It is possible to pass not only variables to a function but also specify default values. Default values are automatically filled if not provided. Note that default values in python must be provided after the variables without default value.</p> <pre><code># we fix 10 as default for n\ndef fibbo(a0, b0, n = 10):\n    a = a0\n    b = b0\n\n    if n != 0:\n        for i in range(1,n):\n            a, b = b, a+b\n    return a\n\n# return the 10th fibonacci number we do no longer need to specify 10 for n\nprint(f\"Default fibonnaci number (10): {fibbo(10,-5)}\")\n\n# if we want another number we can specify it\nprint(f\"20th fibonnaci number: {fibbo(10,-5, n=20)}\")\n# it would be here equivalent to call fibbo(10, -5, 20)\n</code></pre>"},{"location":"lecture/01-Basics-Programing/013-functions-classes/#classes","title":"Classes","text":"<p>Classes are also function but seen from a higher level. Mathematically speaking they are closer to category theories where objects and morphism describe the relation between the objects. A class defines a structure for objects including their data and functions also called methods that operates on these data.</p> <p>A class is defined almost like a function, but using the <code>class</code> keyword, and the class definition usually contains a number of class method definitions (a function in a class).</p> <ul> <li> <p>Each class method have an argument <code>self</code>. This object is a self-reference to itself.</p> </li> <li> <p>Some class method names have special meaning, for example:</p> <ul> <li><code>__init__</code>: The name of the method that is invoked when the object is first created.</li> <li><code>__str__</code> : A method that is invoked when a simple string representation of the class is needed, as for example when printed.</li> </ul> </li> </ul> <pre><code>class Point:\n    #Simple class for representing a point in a Cartesian coordinate system.\n\n    # inside the class we define the initialization method (will be called any time a new instance is created)\n    def __init__(self, x, y):\n        # Create a new Point at x, y.\n        self.x = x\n        self.y = y\n\n    # A method (function) to translate a point\n    def translate(self, dx, dy):\n        # Translate the point by dx and dy in the x and y direction.\n        self.x += dx\n        self.y += dy\n\n    # A method to compute the norm of the vector\n    def norm(self):\n        return (self.x ** 2 + self.y ** 2) ** 0.5\n\n    # a method to show the coordinates\n    def show_myself(self):\n        print(f\"I am a point with coordinates: ({self.x}, {self.y})\")\n    def __str__(self):\n        return(\"Point at [%f, %f] with length %f\" % (self.x, self.y, self.normxy))\n\n\n# declaring instances of this class\np1 = Point(1,1)\np2 = Point(1,1)\n\n# calling a class method\n\np1.translate(-0.5, 0.5)\n\nprint(p1)\nprint(p2)\n</code></pre> <p>Note</p> <p>Unless your project really requires to have higher conceptual objects where data structures and tightly connected methods are needed, you do not need classes in the first place. We will see examples throughout this lecture where classes comes naturally to proceed to some computations. Some libraries too are designed from an oject oriented perspective (<code>pytorch</code> for instance) or make use of class for their primary objects <code>pandas dataframes</code> for instance.</p>"},{"location":"lecture/02-Basics-Scientifics/021-numpy/","title":"Arrays - Numpy","text":"<p>With the basic of programming so far we can do scientific computing, basic scientific computing. Theoretically we can compute approximation of any function such as \\(x \\mapsto e^x\\). This would however be very difficult to implement all the classical mathematics such as basic functions <code>exp</code>, <code>ln</code>, <code>sin</code>, <code>arccos</code> etc. The same goes for linear algebra like matrix multiplication or inversion, as well as for integration, differentiation, optimization, generation of random numbers.</p> <p>Python is modular in the sense that many so called libraries can be used to perform more advanced task. Those libraries are most of the time open source and provide pre built functions and structures that can be called directly in the program after importing the library.</p> <p>In order to use such a library you need to</p> <ul> <li>install it (either through <code>anaconda</code> or <code>pip</code>)</li> <li>import it at the begining of the script using <code>import xxx</code></li> </ul> <p>NumPy is a cornerstones of scientific computing in the python community. The <code>numpy</code> library is used in almost all numerical computation using Python. It is a library that provide high-performance vector, matrix and higher-dimensional data manipulation for Python (tensors). It is implemented in C and Fortran so when calculations are vectorized (formulated with vectors and matrices), performance is very good.</p> <p>At the core, <code>numpy</code> puts python lists on steroids for computation. The objects are arrays and handled as lists with many additional functionalities.</p> <p>Note</p> <p>As a convention over time the <code>numpy</code> library is imported with a nickname <code>np</code>.</p>"},{"location":"lecture/02-Basics-Scientifics/021-numpy/#basics","title":"Basics","text":"<pre><code># import the library numpy with nickname np\nimport numpy as np\n\n# Create a list\nmy_list = [0, 1, 3, 2]\n\n# create a numpy array from this list\nmy_array = np.array(my_list)\n\nprint(f\"\"\"\nPython list: {my_list} \\t with type: {type(my_list)}\nNumpy array: {my_array}\\t with type: {type(my_array)}\n\"\"\")\n\n# Like lists you access elements the same way by indexing and slicing.\nx = np.array([1, 2, 7, 18, 4])\nprint(f\"\"\"\nmy array: {x}\nFirst element: {x[0]}\nArray from 1 to second exclusive, {x[1:2]}\nArray from begining to third exclusive: {x[:3]}\nArray from second to end: {x[2:]}\nType of x[0]: {type(x[0])}\nType of x[1:2]: {type(x[1:2])}\n\"\"\"\n)\n</code></pre> <p>Unlike the concatenation properties, you can now perform arithmetic operations which will be applied point wise. <pre><code>x = np.array([1, 2, 7, 18, 4])\ny = np.array([-1, 0, 1, 0, 2])\n\nprint(f\"\"\"\nArray: x:{x} and y:{y}\nAddition x+y: {x+y}\nMultiplication x*y: {x*y}\nDivision x/y: {x/y}\nAddition scalar plus array 3+x: {3+x}\nMultiplication by scalar 3 * x: {3 * x}\n\"\"\"\n)\n</code></pre></p> <p>You can declare matrices as 2 dimensional arrays and access the dimension and the shape <pre><code>my_matrix = np.array(\n    [ \n        [3.4, 8.7, 9.9], \n        [1.1, -7.8, -0.7],\n        [4.1, 12.3, 4.8]\n    ]\n)\nprint(f\"\"\"\nMatrix: {my_matrix}\nNumber of dimensions: {my_matrix.ndim}\nDimensions: {my_matrix.shape}\n\"\"\"\n)\n\n# Note that you can go to arbitrary higher number of dimensions.\n</code></pre></p> <p>Warning</p> <p>Be aware these are not lists. <code>numpy</code> arrays have uniform type and can not be mixed. The type of an array once declared can not be changed (unless explicitly done) afterwards for computational efficiency. You can not allocate values which are not of the predeclared type to an array.</p> <pre><code># declare a list of integers\nmy_list = [0, 1, -2]\n# declare an array of integers\nx = np.array([0, 1, -2])\n\nprint(f\"\"\"\nList {my_list} of type {type(my_list)}\nFirst element of the list {my_list[0]} of type {type(my_list[0])}\nArray {x} of type {type(x)}\nFirst element of the array {x[0]} of type {type(x[0])}\n\"\"\")\n\n# allocate the value 3.5 at the first entry\nmy_list[0] = 3.5\nx[0] = 3.5\n\nprint(f\"\"\"\nList {my_list} of type {type(my_list)}\nFirst element of the list {my_list[0]} of type {type(my_list[0])}\nArray {x} of type {type(x)}\nFirst element of the array {x[0]} of type {type(x[0])}\n\"\"\"\n)\n\n# Allocate value \"Samuel\" at the first entry\n# the code will run into an error for numpy therefore we catch it\nmy_list[0] = \"Samuel\"\nprint(f\"\"\"\nList {my_list} of type {type(my_list)}\nFirst element of the list {my_list[0]} of type {type(my_list[0])}\n\"\"\"\n)\n\ntry:\n    x[0] = \"Samuel\"\nexcept Exception as error:\n    print(f\"We got an error\\n{error}\")\n</code></pre> <p>Note</p> <p>When a <code>numpy</code> array is declared, it will basically try to cast the smallest type under which all variables fits. <pre><code># array with mixed floats and integers\nx = np.array([1, 2.5, 3])\nprint(f\"array {x} with type {x.dtype}\")\n# array with some strings\nx = np.array([1, 2.5, \"Samuel\"])\nprint(f\"array {x} with type {x.dtype}\")\n# Enforce type with keyword dtype\nx = np.array([1, 2, 3], dtype = float)\nprint(f\"array {x} with type {x.dtype}\")\n</code></pre></p>"},{"location":"lecture/02-Basics-Scientifics/021-numpy/#generating-arrays","title":"Generating arrays","text":"<p>Quite often you need to generate array from specific shape and structure such as an identity matrix. Numpy provides handy ways to construct them, the list of which can be found here</p> <p>It includes among others</p> <ul> <li><code>arange</code>: array of equally spaced values of size <code>mesh</code> (default is 1) between <code>start</code> (default is 0), <code>end</code>.</li> <li><code>linspace</code>: array of <code>num</code> equally spaced values between <code>start</code> and <code>end</code> (excluded unless otherwize specified)</li> <li><code>zeros</code>: array of zeros of given dimension</li> <li><code>ones</code>: array of ones of given dimension</li> <li><code>diag</code>: (for <code>2d arrays</code>) construct a diagonal matrix or extract the diagonal</li> <li><code>eye</code>: returns an identity matrix</li> <li><code>random.xxx</code>: produce random numbers (different distributions)</li> </ul> <pre><code>x = np.arange(10)\nprint(x)\n\nx = np.arange(-1, 1, 0.5)\nprint(x)\n\nx = np.linspace(-1, 1, 5)\nprint(x)\n\nx = np.zeros( (3, 3) )\nprint(x)\n\nx = np.ones((3,3))\nprint(x)\n\n# construct diagonal matrix from vector\nvector = np.array([0, 1, 5, -1])\nx = np.diag(vector)\nprint(vector)\nprint(x)\n\n# extract diagonal from matrix\ndiagonal = np.diag(x)\nprint(diagonal)\n\n# Generate random numbers\nx = np.random.rand(10)\nprint(x)\n</code></pre>"},{"location":"lecture/02-Basics-Scientifics/021-numpy/#slicing-on-conditions","title":"Slicing on Conditions","text":"<p>We already saw several examples of slicing for list that are basically done with respect to the domain of indices of the list itself. Numpy allows to generate efficiently slicing based on the values of the array, basically retrieving indices of the kind</p> \\[ \\begin{equation} \\left\\{i \\colon x_i \\in B\\right\\} \\end{equation} \\] <p>which is the reciprocal image of \\(i \\mapsto f(i)=x_i\\).</p> <pre><code># %%\nx = np.random.rand(10)\n\nprint(x)\n# make an boolean array of the values above 0.5\nmark = x&gt;0.5\nprint(f\"resulting true/flase array for the condition :{mark}\")\n# extract an array of the values above 0.6\nprint(f\"\"\"\nValues of\n{x}\nabove 0.5 is:\n{x[mark]}\n\"\"\")\n\n# get the corresponding indices where mark is true\nindices = np.where(mark)\nprint(f\"Indices where the condition is true: {indices}\")\nprint(f\"Values of x for the indices where the condition is true: {x[indices]}\")\n\n# Do it directly for instance for the values between 0.2 and 0.6\nprint(x[(x&gt;0.2) &amp; (x&lt;=0.6)])\n</code></pre>"},{"location":"lecture/02-Basics-Scientifics/021-numpy/#broadcasting","title":"Broadcasting","text":"<p>Where <code>numpy</code> shines from a scientific computational viewpoint, is about so called broadcasting. In math you would define a function </p> \\[ \\begin{equation*} \\begin{split} f\\colon X &amp; \\longrightarrow Y\\\\ x &amp; \\longmapsto f(x) \\end{split} \\end{equation*} \\] <p>and for any imaginable \\(x\\) you can evaluate the value of which. However from a computational perspective, for several reasons such as plotting but also parallel computing among others, you would like to have immediately several evaluations of which at the same time.</p> <p>In other terms, given a vector \\(\\mathbf{x} := (x_0, \\ldots, x_{N-1})\\) you would like to have in return the vector \\(\\mathbf{f}(\\mathbf{x}):= (f(x_0), \\ldots, f(x_{N-1}))\\). In other terms, we want a function, assume \\(N\\) given for simplicity</p> \\[ \\begin{equation*} \\begin{split} \\mathbf{f}\\colon X^N &amp; \\longrightarrow Y^N\\\\ \\mathbf{x} &amp; \\longmapsto \\mathbf{f}(\\mathbf{x}) = (f(x_0), \\ldots, f(x_{N-1})) \\end{split} \\end{equation*} \\] <p>This simple operation is called broadcasting and <code>numpy</code> implements it effortlessly for most of the standard functions.</p> <pre><code>x = 2\nprint(f\"exponential of {x}={np.exp(x)}\")\n# now with an array\nx = np.linspace(-1, 1, 20)\nprint(f\"\"\"\nExponential of:\n{x}\nis\n{np.exp(x)}\n\"\"\")\n\n# for a random vector\nx = np.random.rand(10)\nprint(x)\nprint(\"Exponential: \\n\", np.exp(1+x))\nprint(\"Log: \\n\", np.log(1 + x))\nprint(\"absolute value: \\n\", np.absolute(x))\nprint(\"maximum:\\n\", np.maximum(x, 0)) \n</code></pre> <p>Note</p> <p>If you want to program it from a straight forward perspective it would run as follow <pre><code>def scalar_fun(x):\n    ...\n    return x\n\ndef fun(x):\n    N = len(x) # get the length of x\n    result = np.zeros(N) # create an array of size N\n    for i in range(N):\n        result[i] = scalar_fun(x[i])\n    return result\n</code></pre> This is fully legitimate however almost all the time very slow due to the python loop. As long as there is a <code>numpy</code> function available, use it in the context of broadcasting.</p> <p>Every linear combination, multiplication and composition of <code>numpy</code> functions will broadcast automatically. Beyond this scope, if you need to write a function, think twice before you input a <code>numpy</code> array before writing the function. Always think that the input are <code>numpy</code> arrays.</p> <pre><code># define a function computing (x - k)^+=max(x-k, 0)\n\ndef maximum00(x, k):\n    result = 0\n    if x&gt;=k:\n        result = x-k\n    return result\n\nprint(f\"\"\"\nfor x=10 and k =9: {maximum00(10,9)}\n\"\"\")\n\ntry:\n    x = np.arange(10)\n    print(f\"\"\"\n    Result for x of (x-9)^+: {maximum00(x, 9)} \n          \"\"\")\nexcept Exception as error:\n    print(f\"We got an error\\n{error}\")\n\n\n# In this case you can modify the function as follows\ndef maximum01(x, k):\n    # ensure that x is an numpy array\n    x = np.array(x)\n    result = np.zeros_like(x)\n    # now allocate the value correctly\n    mask = x&gt;=k\n    result[mask] = x[mask] - k\n    result[~mask] = 0 # ~mask set true to false and reciprocal\n    return result\n\nx = np.arange(10)\nprint(f\"\"\"\nUsing maximum01\nResult for x of (x-9)^+: {maximum01(x, 9)} \n\"\"\")\n\n# Naturally this is naive, you have a numpy function maximum that provides this result direclty\nprintf(f\"Using numpy maximum: {np.maximum(x - k, 0)}\")\n</code></pre>"},{"location":"lecture/02-Basics-Scientifics/021-numpy/#linear-algebra","title":"Linear algebra","text":"<ul> <li>elementwise multiplication and power</li> <li>transpose</li> <li>norm</li> <li>matrix multiplication </li> <li>eigenvalue</li> <li>inverse ...</li> </ul> <pre><code>x = np.array([1, 2])\nA = np.array([[1, 0], [0, 1]])\nB = np.array([[4, 1], [2, 2]])\n\n# element wise multiplication\nprint(\"x+x \\n\", x + x)\nprint(\"A + B  element wise \\n\", A + B)\n\n# transpose\nprint(B)\nprint(\"transpose \\n\", B.T)\n\n# matrix multipplication\nprint(\"Matrix multiplication AB \\n\", A.dot(B)) # perform the matrix multiplication A B\nprint(\"Matrix vector multiplication Bx \\n\", B.dot(x))\nprint(\"Matrix vector multiplication xB \\n\", x.dot(B))\nprint(\"Inner prod\", x.dot(x))\n\n# eigenvalues and vectors\nprint(\"Eigenvalues of B returns eigenvalues and eigenvectors \\n\", np.linalg.eig(B))\n\nresult = np.linalg.eig(B)\nprint(\"eigenvalues\", result[0])\nprint(\"eigenvectors\", result[1])\n\n# inverse\nprint(B)\nprint(\"multiplicative inverse of B \\n\", np.linalg.inv(B) )\nprint(\"B B^(-1) \\n\", B.dot(np.linalg.inv(B)) )\n</code></pre>"},{"location":"lecture/02-Basics-Scientifics/021-numpy/#randomness","title":"Randomness","text":"<p>We already saw that we can generate random numbers. With this at hand, you have easy access to basis statistical information such as the mean, variance, standard deviation (you can have a look at quantiles, etc.)</p> <pre><code>x = np.random.rand(40)\ny = np.random.rand(40)\nprint(\"mean: \", x.mean())\nprint(\"std: \", x.std())\nprint(\"max:\", x.max())\nprint(\"min:\", x.min())\nprint(\"sum:\", x.sum())\nprint(\"cumulative sum: \", x.cumsum())\nprint(\"correlation matrix: \", np.corrcoef(x, y))\n</code></pre>"},{"location":"lecture/02-Basics-Scientifics/021-numpy/#fake-vs-true-copy","title":"Fake vs True Copy","text":"<p>Remember that with basic python, equality means allocation and therefore declaration of a new variable</p> <pre><code>a = 1\nb = a\nprint(f\"Value of a={a} and b={b}\")\n#change the value of a which will not change the value of b\na = 2\nprint(f\"Value of a={a} and b={b}\")\n</code></pre> <p>Note</p> <p>Without entering into the concept of pointers, from a basic perspective, for a computer, a variable is an address in memory containing the value of this variable. With this at hand you have two ways to consider the variable:</p> <ul> <li>either by its address in memory</li> <li>either by its value at this given address</li> </ul> <p>Python considers that a variable is the value in the pointed memory. If you declare <code>a =1</code> (assigning the value <code>1</code> at some place in memory with address called <code>a</code>), if you declare <code>b=1</code>, it will create a new address <code>b</code> pointing to a value in memory equal to the memory value address of <code>a</code> which is <code>1</code>. Now if you change the value of a by <code>a = 2</code> you are saying the at the memory address of <code>a</code> change the value from <code>1</code> to <code>2</code>. The value of address <code>b</code> still points to a memory allocation which value is <code>1</code>.</p> <p>From a mathematical perspective this makes sense, but even if as a mathematician you can conceive in mind any large dimensional object of arbitrary cardinality a computer is not.</p> <p>Suppose for instance that you have a very large vector \\(\\mathbf{x} = (x_0, \\ldots, x_{N-1})\\) stored in memory for computations, each single value being a large float taking lot of memory space. For whatever reasons you want to perfrom computations on the values of which being greater than \\(10\\), that is \\(\\mathbf{y} = \\mathbf{x}[\\mathbf{x}\\geq 10]\\). In a pythonic way you would define \\(\\mathbf{y} = \\mathbf{x}[\\mathbf{x}\\geq 10]\\) allocating a new segment of the memory with this new vector. With multiple copies like this you would rapidly exhaust the memory while doing nothing meaningful. If instead you were just defining \\(\\mathbf{y}\\) as the adresses where \\(\\mathbf{x}\\) is greater than \\(10\\), you would only store the vector of addresses which is way smaller in size.</p> <p>Since <code>numpy</code> relies on <code>C</code> and <code>Fortran</code> with in mind this typical kind of situations, it considers arrays as address in memory. For most computations <code>a =b</code> is just assigning to <code>b</code> the address of <code>a</code> rather than defining a value (with many exceptions that are documented but sill difficult to catch). If you want a tru new value of an array, you need to tell <code>numpy</code> that you want to do so.</p> <pre><code># with straightforward fake pythonic copy\na = np.array([0,1,2])\nb = a \na[0] = 2\nprint(f\"Value of a:{a}\")\nprint(f\"Value of b: {b}\")\n\n# with explicit copy\na = np.array([0,1,2])\nb = a.copy()\na[0] = 2\nprint(f\"Value of a:{a}\")\nprint(f\"Value of b: {b}\")\n</code></pre>"},{"location":"lecture/02-Basics-Scientifics/022-plotly/","title":"Producing Some Nice Graphs (we truly like it)","text":"<p>It is underestimated how graphs representation can help to tackle some complex problems. If the graph is ugly, you just don't want to see it, while if the graphs is appealing, you might want to explore more the problematic. Colors, and clear representations allows to express complex relations therefore a good graph library is fundamental to data analysis and scientific computing.</p> <p>Historically, <code>matplotlib</code> is the library that is dedicated at producing every possible plots based on data. It is however quite complicated (with very difficult way of modifying making a graph nicer). Think of <code>matplotlib</code> as a powerful but low level programming language.</p> <p>Based on this library, or independently developed, several plot libraries have emerged:</p> <ul> <li><code>seaborn</code>: plotting library encapsulating <code>matplotlib</code> but with a nice touch.</li> <li><code>bokeh</code>:  plotting library more advanced but to my mid a bit difficult to master however with beautiful results (in particular for dynamic graphs)</li> <li><code>plotly</code>: Plotting library company providing solution for data representation and analysis. Their graph library is open source for python.</li> </ul> <p>There are several others (let me know if you find a nice one), but for this lecture I will mainly use <code>plotly</code> as it is easily usable with python and for out purposes.</p> <p>The basic of <code>plotly</code> (we will present rapidly plotly express which is a one stop way to represent data from a pandas dataframe) is to define a <code>graph</code> as an object and add components interactively one after the other (like a dictionary).</p> <pre><code># Import the libraryt\nimport numpy as np\nimport plotly.graph_objs as go\n\n# we create a graph with x\\mapsto sin(x)\nx = np.linspace(-5, 5, 100)\ny = np.sin(x)\n\n# Declare the graph\nfig = go.Figure()\n# add a scatter plot (x, y)\nfig.add_scatter(x = x, y = y)\n# show the graph\nfig.show()\n</code></pre> <p>Let us make use of the library with a simple example of a stock price modeled as a random walk.</p> <p>We first define our Random walk which is mathematically given as</p> \\[ \\begin{equation} S_0=s,\\quad S_t = X_t+S_{t-1}=S_0+\\sum_{s=1}^t X_s \\end{equation} \\] <p>Where </p> \\[\\begin{equation} X_t= \\begin{cases} 1 &amp;\\text{If I get head for my coin toss}\\\\ -1 &amp;\\text{If I get tail} \\end{cases} \\end{equation}\\] <p>The basic element you need is a sequence of independent coin tosses delivering \\(\\pm 1\\). We know how to generate uniformly random numbers between \\(0\\) and \\(1\\). Given a sequence \\(\\mathbf{x} = (x_0, \\ldots, x_{N-1})\\) of such random numbers, I can transform them into an independent sequence of \\(\\pm 1\\) where each probability is \\(1/2\\) as follows</p> <pre><code>shock = np.random.rand(40)\nprint(shock)\nprint(2 * np.round(shock) - 1)\n</code></pre> <p>We now define the function that returns the stock price for a given amount of times. <pre><code>def random_walk(startprice, days):\n    price = np.zeros(days)\n    shock = np.round(np.random.rand(days))\n    price[0] = startprice\n    for i in range(1, days):\n        price[i] = price[i-1] + 2*shock[i]-1\n    return price\n\n# This gives us the generation of sample random walks that we can print\n\nprint(random_walk(10, 20))\n</code></pre></p> <p>We can now plot a sample path</p> <pre><code>days = 10\n# Define the X and Y axis\nX = np.arange(days)\nY = random_walk(10, days)\n\n# Define a figure object\nfig = go.Figure()\nfig.add_scatter(x = X, y = Y)\nfig.layout.title = \"My random walk\"\nfig.show()\n</code></pre> <p>We can plot several random path since every time we run the function a new random sample is drawn (this function is per se not a function by the way).</p> <pre><code>days = 1000\n\nX= np.arange(days)\n#define an object\nfig = go.Figure()\n# add scatters to the figure\nfor i in np.arange(5):\n    Y = random_walk(10, days)\n    # append the new scatter with a name for each trace\n    fig.add_scatter(x = X, y = Y, name = 'RW number %i'%i, line = {'width':1})\n\n# add a title\nfig.layout.title = \"Sample paths of the random walks\"\n# display the random walks\nfig.show()\n</code></pre> <p>We can generate a huge amount of different graphs with plotly. We will see the different types, but as illustration we can plot the some histogram.</p> <pre><code># generate samples from a normal distribution\nX = np.random.randn(100000)\n\n# plot them\nfig = go.Figure()\nfig.add_histogram(x = X, histnorm = 'probability')\nfig.show()\n</code></pre>"},{"location":"lecture/02-Basics-Scientifics/023-scipy/","title":"First Scientific Computations","text":"<p>Aside from linear algebra, we need classical implementation of scientific computations.</p> <ul> <li>Integration</li> <li>Optimization</li> <li> <p>Statistics (1)</p> <ol> <li> Some other major scientific computations encompass ODE/PDE, Fourier analysis for instance but we will see them later on in this lecture.</li> </ol> </li> </ul> <p>The basic first go library for it is Scipy which can be seen as a meta library for each of this major scientific purposes. (1)</p> <ol> <li> <p> It encompasses the following directions. </p> <ul> <li>Special functions (scipy.special)</li> <li>Integration (scipy.integrate)</li> <li>Optimization (scipy.optimize)</li> <li>Interpolation (scipy.interpolate)</li> <li>Fourier Transforms (scipy.fftpack)</li> <li>Signal Processing (scipy.signal)</li> <li>Linear Algebra (scipy.linalg)</li> <li>Sparse Eigenvalue Problems (scipy.sparse)</li> <li>Statistics (scipy.stats)</li> <li>Multi-dimensional image processing (scipy.ndimage)</li> </ul> </li> </ol> <p>Each of these submodules provides a number of functions and classes that can be used to solve problems in their respective topics. To import a particular function we use <code>from scipy.&lt;module&gt; import &lt;function&gt;</code>, for instance </p> <pre><code># import quad for integration from the module integrate\nfrom scipy.integrate import quad\n# import minimize for minimization from the module optimize\nfrom scipy.optimize import minimize\n# import the normal distribution from the module stats\nfrom scipy.stats import norm\n</code></pre>"},{"location":"lecture/02-Basics-Scientifics/023-scipy/#integration","title":"Integration","text":"<p>We want to procede to numerical evaluation of integrals of the type</p> <p>\\(\\displaystyle \\int_a^b f(x) dx\\)</p> <p>The naive method is to approximate the area below using some rectangles or trapezoides</p> <p> </p> About Integral Approximation <p>Further methods were developped based on the fact that</p> <ol> <li> <p>polynomials \\(P[x] = \\sum_{k=0}^n a_k x^k\\) have exact integrals</p> \\[ \\begin{equation*} \\int_0^1 P[x] dx = \\sum_{k=0}^n \\frac{a_k}{k+1} \\end{equation*} \\] </li> <li> <p>Any resonably regular function \\(f:[0,1] \\to \\mathbb{R}\\) can be approximated by a polynomial \\(P^\\varepsilon\\):</p> \\[ \\begin{equation}     \\sup_{0\\leq x \\leq 1} \\left| f(x) - P^\\varepsilon[x] \\right|\\leq \\varepsilon \\end{equation} \\] </li> </ol> <p>Which implies that</p> \\[ \\begin{equation*}     \\left| \\int_0^1 f(x)dx - \\int_0^1 P^\\varepsilon[x] dx \\right| \\leq \\varepsilon \\end{equation*} \\] <p>This equation yields the following key aspects from a computational perspective:</p> <ol> <li>With \\(\\varepsilon\\) you control the accuracy of your approximation</li> <li>Given an accuracy level \\(\\varepsilon\\), how to find the polynomial \\(P^\\varepsilon\\)?</li> </ol> <p>The answer to the second question is topic of a large theory about efficient choice of interpolation polynomial basis wuch as Chebychev, Legendre, etc. </p> <p>Without entering in the overall theory of integral approximation, the usual methods are of quadrature type, available in the sublibrary <code>integrate</code>, and called <code>quad</code>, <code>dblquad</code> and <code>tplquad</code> for single, double and triple integrals, respectively (there are many others too).</p>"},{"location":"lecture/02-Basics-Scientifics/023-scipy/#one-dimensional-integration","title":"One Dimensional Integration","text":"<p>We compute the integral between \\(0\\) and \\(1\\) of \\(x^\\alpha\\).</p> <pre><code># We import the different methods to call them easily\nimport numpy as np\nfrom scipy.integrate import quad\n\n# define the function to be integrated\ndef f(x):\n    return x\n\n# Define the boundaries of the integral\n\nx_lower = 0 # the lower limit of x\nx_upper = 1 # the upper limit of x\n\n# Compute the integral\nval, abserr = quad(f, x_lower, x_upper)\n\nprint(f\"Integral value ={val} with absolute error = {abserr}\")\n</code></pre> <p>In the case of a generic integration of functions depending on parameters</p> \\[ \\begin{equation*} \\alpha \\longmapsto \\int_0^1 f(x, \\alpha) dx    \\end{equation*} \\] <p>where \\(\\alpha\\) is a parameter, we can specify the arguments with respect to which the integral shall be computed.</p> <pre><code># define a function returning x^\\alpha\n\ndef integrand(x, alpha):\n    result = x ** alpha\n    return result\n\nx_lower = 0  # the lower limit of x\nx_upper = 1 # the upper limit of x\n\n# we compute the integral of x^2 by passing the argument args = (2,) (single tuple for alpha =2)\nval1, abserr1 = quad(integrand, x_lower, x_upper, args=(2,))\n# we compute the integral of x^0.5 by passing the argument args = (0.5,) (single tuple for alpha =0.5)\nval2, abserr2 = quad(integrand, x_lower, x_upper, args=(0.5,))\n\nprint(f\"The integral of x to the power 2 is equal to {val1} with absolute error {abserr1}\")\nprint(f\"The integral of the square root of x is equal to {val2} with absolute error {abserr2}\")\n</code></pre> <p>Note that even if computer do not understand what \\(\\infty\\) means, there exists in <code>numpy</code> such a concept corresponding to the largest/smallest possible value that your computer can assess. In <code>numpy</code>, <code>np.inf</code> and <code>-np.inf</code> corresponds to \\(\\infty\\) and \\(-\\infty\\) respectively.</p> <p>This is in particular useful to compute limit objects such as</p> \\[ \\begin{equation*} \\int_{-\\infty}^\\infty f(x) dx \\end{equation*} \\] Use of Nameless Functions <p>From a mathematical viewpoint, \"Consider the function \\(x\\mapsto x^2\\)...\" is not a good practice, even if we use it quite often. It is a so called nameless function. Python also offers (exactly) this shorthand functionality in terms of so called <code>lambda</code> functions that can be passed as argument to a further function.</p> <pre><code>val, abserr = quad(lambda x: np.exp(-x ** 2), -np.inf, np.inf)\n\nprint(\"numerical  =\", val, abserr)\n\nanalytical = np.sqrt(np.pi)\nprint(\"analytical =\", analytical)\n</code></pre>"},{"location":"lecture/02-Basics-Scientifics/023-scipy/#higher-dimensional-integration","title":"Higher Dimensional Integration","text":"<p>The same holds for higher dimensional integration</p> \\[ \\begin{equation} \\int_{a_0}^{b_0} \\int_{a_1(x)}^{b_1(x)} f(x, y)dy dx \\end{equation} \\] <p>where \\(x \\mapsto a_1(x), b_1(x)\\) are functions defining the domain as a function of \\(x\\) over which the function \\(y\\mapsto f(x,y)\\) has to be integrated.</p> <p>Using nameless functions (<code>lambda</code>) for \\(a_1\\) and \\(b_1\\), the procedure works as follows</p> <pre><code>from scipy.integrate import dblquad\n\ndef integrand(x, y):\n    return np.exp(-x**2-y**2)\n\nx_lower = 0  \nx_upper = 10\ny_lower = 0\ny_upper = 10\n\n# here the two lambda functions are constant.\nval, abserr = dblquad(integrand, x_lower, x_upper, lambda x : y_lower, lambda x: y_upper)\n\nprint(val, abserr) \n</code></pre> <p>Paying attention to the absolute error, you will notice that this one is immediately multiplied by a large factor. If we look at the the speed, there is also a large difference</p> <p>Single integral <pre><code>def f(x):\n    return x ** 0.5\n\n%timeit quad(f, 0, 1)\n</code></pre> vs double integral <pre><code>def g(x, y):\n    return np.exp(-x**2-y**2)\n\nx_lower = 0  \nx_upper = 10\ny_lower = 0\ny_upper = 10\n\n\n%timeit dblquad(g, x_lower, x_upper, lambda x : y_lower, lambda x: y_upper)\n</code></pre></p> <p>The main reason is linked to the so called curse of dimensionality. With any method, you will have to consider a grid of your domain over which you integrate to evaluate the function. Suppose that for the interval \\([0,1]\\) you need \\(N\\) points, for a \\(d\\)-dimensional integral over \\([0, 1]^d\\) you would roughly need \\(N^d\\) which is increasing exponentialy with the dimension. In other words, exact integration is excellent in terms of accuracy and speed but is limited to low, very low dimensions.</p>"},{"location":"lecture/02-Basics-Scientifics/023-scipy/#optimization","title":"Optimization","text":"<p>Optimization is also another corner stone of scientific computing. In a very approximative way you have the following two sets of problems</p> <ul> <li>Minimization</li> <li>Root finding</li> </ul> <p>Though root finding can often be expressed as a minimization problem, it is however a very important tool for instance for fixed points</p>"},{"location":"lecture/02-Basics-Scientifics/023-scipy/#minimization","title":"Minimization","text":"<p>Given a function \\(f:\\mathbb{R}^d\\to\\mathbb{R}\\) and a constrained set \\(\\mathcal{C}\\subseteq \\mathbb{R}^d\\), the goal is to find a solution \\(x^\\ast\\) in \\(\\mathcal{C}\\) such that</p> \\[ \\begin{equation} f(x^\\ast) = \\inf \\left\\{ f(x) \\colon x \\in \\mathcal{C}\\right\\} \\end{equation} \\] <p>with two quantity of interest, namely \\(f(x^\\ast)\\) the value of \\(f\\) at the minimum, and more importantly \\(x^\\ast\\) or argmin the point in \\(\\mathcal{C}\\) at which \\(f\\) achieves its minimum.</p> Constrained, Local, Global Minima <ul> <li> <p>The problem is called constrained if \\(\\mathcal{C}\\neq \\mathbb{R}^d\\). Solving this kind of problem might be challenging and in its simplest form assume some Lagrangian methods to transform the problem into an unconstrained optimization problem. These constraints are usually of linear type</p> \\[ \\begin{equation} \\mathcal{C}:= \\left\\{x \\in \\mathbb{R}^d \\colon Ax + b =0\\right\\} \\end{equation} \\] <p>requiring \\(x\\) to be in an affine subspace, or of inequality type</p> \\[ \\begin{equation} \\mathcal{C}:= \\left\\{x \\in \\mathbb{R}^d \\colon g(x) \\geq 0\\right\\} \\end{equation} \\] <p>for some function \\(g:\\mathbb{R}^d \\to \\mathbb{R}\\), for instance \\(g(x) = x\\) in one dimension requiring \\(x\\) to be positive.</p> </li> <li> <p>Local vs Global Minima:     Constrained problem can be transformed into unconstrained one most of the time.     For an unconstrained optimization problem, if \\(f\\) is smooth enough, a necessary condition for \\(x^\\ast\\) to be an argmin is that</p> \\[ \\begin{equation} \\nabla f(x^\\ast) = 0 \\end{equation} \\] <p>In other terms the derivative of \\(f\\) at \\(x^\\ast\\) should vanish. This gives a procedure called gradient descent to compute the argmin by sqeuntially running along the stepest gradient to converge to the argmin. However, this condition is just necessary and not sufficient. Indeed, you could end up in a saddle point of the function, or just a local minimima (minimum over a small neighborhood). If the function is convex, then the condition is necessary and sufficient.</p> </li> </ul> <p>In the following example we search for the global minimum of </p> \\[ \\begin{equation}     f(x_1, x_2) = (x_1-1)^2 + (x_2- 2.5)^2 \\end{equation} \\] <pre><code># import the minimize functionality\nfrom scipy.optimize import minimize\n\n# define the function to be minimized\ndef fun(x):\n    result = (x[0] - 1)**2 + (x[1] - 2.5)**2\n    return result\n\n# Plot in two dimensions how this function  looks like\nx0 = np.linspace(-5, 5, 20)\nx1 = np.linspace(-5, 5, 20)\n\nX0, X1 = np.meshgrid(x0, x1)\n\nZ = fun([X0, X1])\n\nfig =go.Figure()\nfig.add_surface(x = X0, y = X1, z = Z)\nfig.show()\n\n# Compute the minimimum\n# 1. provide a guess where the minimization will start\nguess = (0, 0)\n# 2. return the result\nres = minimize(fun, guess)\n\n# the result is a dictionarry with everything\nprint(res)\n\n# You can access the different components with\nprint(f\"The value of the function at the minimum is {res.fun}\")\nprint(f\"The value of the argmin is {res.x}\")\n</code></pre> <p>This function allows you to add boundaries and constraints, but we will see that later.</p>"},{"location":"lecture/02-Basics-Scientifics/023-scipy/#root-finding","title":"Root finding","text":"<p>Given a function \\(f:\\mathbb{R}^d\\to \\mathbb{R}\\), the goal is to find \\(x^\\ast\\) such that</p> \\[ f(x^\\ast)=0 \\] <p>This statement is equivalent to \\(f(x^\\ast) = a\\) by defining \\(\\tilde{f}(x) = f(x) -a\\).</p> <p>The main technique in that case are based on root finding methods and two methods are competing for that:</p> <ul> <li>a Newton based one <code>root</code> (give a guess)</li> <li>a Brent base method <code>brentq</code> (give similar in approach to the bissection)</li> </ul> <p>Advantages and inconvenient of both are subject to some mathematical consideration. On the one hand, Newton (<code>root</code>) even if fast is not sure to converge however only needs to be given a smart guess as start point. On the other hand, <code>brentq</code> will very likely converge (even in the case where there are several roots) but you have to specify the bounds in which the root has to be found, and the value of the function has to differ in sign on these two boundaries.</p> <p>We want to find the solution to the equation \\(x\\ln(x)=0.5\\) and therefore consider the function</p> \\[\\begin{equation} f(x) = x \\ln(x) - 0.5 \\end{equation}\\] <pre><code>def func(x):\n    result = x * np.log(x) - 0.5\n    return result\n\n# let us plot the function\nX = np.linspace(0.01, 3, 200)\nY = func(X)\n\nfig = go.Figure()\nfig.add_scatter(x = X, y = Y)\nfig.show()\n\n# we want to find the root of this function on the positive axis with newton\nfrom scipy.optimize import root, brentq\n\nguess = 1\nres1 = root(func, guess)\nprint(res1)\n\n#%%\n\n\nlb, ub = (0.1, 3)\nres2 = brentq(func, lb, ub)\nprint(res2)\n</code></pre>"},{"location":"lecture/02-Basics-Scientifics/023-scipy/#statistics-probability","title":"Statistics (Probability)","text":"<p>This sub-library is a comprehensive list of everything you need to have in terms of distributions. There are two big classes:</p> <ul> <li>discrete distributions</li> <li>continuous distributions</li> </ul> <p>You can access to the full list here </p> <p>Depending on the field you will work with, you will have to deal with many of them but the most important are</p> <ul> <li>normal distribution</li> <li>student t </li> <li>normal inverse Gaussian (as well as the more general gamma distribution family)</li> <li>binomial</li> <li>mixtures of them</li> </ul> <p>cdf, pdf, pmf, ppf, etc.</p> <p>Given a random variable \\(X:\\Omega \\to \\mathbb{R}\\) on some probability space \\((\\Omega, \\mathcal{F}, P)\\), we have</p> <ul> <li> <p>(cdf) Cumulative distribution function: The cumulative distribution of \\(X\\) given \\(P\\) is defined as</p> \\[ \\begin{equation*}     F_X(x) = P[X\\leq x] \\end{equation*} \\] <p>which is an increasing right continuous function (with left limits) from \\(\\mathbb{R}\\to [0, 1]\\).</p> <p>For any function \\(h:\\mathbb{R}\\to \\mathbb{R}\\) (under some integrability assumptions) it holds</p> \\[ \\begin{equation*}     E[h(X)] = \\int_{\\Omega} h\\left(X(\\omega)\\right) dP(\\omega) = \\int_{\\mathbb{R}} h(x)dF_X(x) \\end{equation*} \\] </li> <li> <p>(pdf) Probability distribution function:     Given the <code>cdf</code> of \\(X\\) \\(F_X\\), if it is differntiable, it follows that \\(dF_X(x) = f_X(x)dx\\).     The function \\(f_X\\) is called the probability distribution function or <code>pdf</code>.     In the previous statement, it futher holds</p> \\[ \\begin{equation*}     E[h(X)] = \\int_{\\Omega} h\\left(X(\\omega)\\right) dP(\\omega) = \\int_{\\mathbb{R}} h(x)dF_X(x) = \\int_{\\mathbb{R}} h(x)f_X(x) dx \\end{equation*} \\] </li> <li> <p>(pmf) Probability mass function:     Given the <code>cdf</code> of \\(X\\), \\(F_X\\), if on the other extreme, suppose that \\(X:\\Omega \\to \\mathbb{R}\\) only takes finite (or countable values) \\(\\{x_1&lt; \\ldots&lt; x_N\\}\\).     We denote by \\(A_k =\\{\\omega \\in \\Omega\\colon X(\\omega) = x_k\\}\\) where \\(A_i\\cap A_j=\\emptyset\\) as well \\(\\cup A_k = \\Omega\\).     It follows that </p> \\[ \\begin{equation}     p_k:=P[A_k] = P[X = x_k] \\end{equation} \\] <p>is such that \\(p_k \\geq 0\\) and \\(\\sum_k p_k = \\sum P[A_k] = P[\\cup A_k] = 1\\).</p> <p>Now given the function \\(h\\), it follows that</p> \\[ \\begin{equation}     E[h(X)] = E[h\\left(\\sum x_k 1_{A_k}\\right)] = \\sum E[h(x_k)1_{A_k}] = \\sum h(x_k)p_k \\end{equation} \\] <p>In terms of vectors (or numpy arrays), it follows that for discrete random variables the expectation is just a scalar product</p> \\[ \\begin{equation}     E[h(X)] = \\sum h(x_k)p_k = h(x)\\cdot p \\end{equation} \\] <p>In the case of a discrete distribution, we therefore call \\(f_X(x_k) := p_k\\) if \\(x= x+k\\) and zero otherwize, the probability mass function <code>pmf</code> so that it holds</p> \\[ \\begin{equation}     E[h(X)] = \\sum h(x_k)f_X(x_k)  \\end{equation} \\] </li> <li> <p>(ppf) Quantile:     Given a <code>cdf</code> \\(F_X\\colon \\mathbb{R} \\to [0,1]\\), since it is an increasing function we can compute its generalized inverse \\(q_X\\colon (0, 1) \\to \\mathbb{R}\\)     Depending on the choice of conventions, we here define the right inverse:</p> \\[ \\begin{equation*}     q_X(u) = \\inf\\{x \\in \\mathbb{R}\\colon P[X\\leq x]\\geq u\\} = \\inf\\{x \\in \\mathbb{R}\\colon F_X(x)\\geq u\\} \\end{equation*} \\] <p>If \\(F_X\\) is strictly increasing and continuous (as is often the case), it is invertible and it holds that</p> \\[ \\begin{equation*}     q_X(u) = \\inf\\{x \\in \\mathbb{R}\\colon F_X(x)\\geq u\\} = \\inf\\{x \\in \\mathbb{R}\\colon x\\geq F_X^{-1}(u)\\} = F_X^{-1}(u)  \\end{equation*} \\] <p>In other terms, it follows that \\(q_X\\) can be considered as the inverse of the <code>cdf</code> (or at least right inverse). For reasons explained in the lecture, it follows that \\(X\\) as well as \\(q_X\\) share the same <code>cdf</code>. Hence it follows that</p> \\[ \\begin{equation*}     E[h(X)] = \\int_{\\Omega} h\\left(X(\\omega)\\right) dP(\\omega) = \\int_{\\mathbb{R}} h(x)dF_X(x) = \\int_{0}^1 h(q_X(u))du \\end{equation*} \\] <p>Hence, integrating \\(h\\) with respect to \\(X\\) is the same as integrating \\(h(q_X)\\) with respect to leesgues \\(du\\) on \\((0,1)\\).</p> </li> </ul> <p>Following the note, distributions usually have the following methods</p> <ul> <li><code>cdf</code> (cumulative distribution) \\(F_X(x)\\)</li> <li><code>pdf</code> (density if the distribution is continuous) \\(f_X(x)\\)</li> <li><code>ppf</code> (probability mass function if the distribution is discrete) \\(f_X(x_k)\\) for \\(x = \\{x_1, \\ldots, x_N\\}\\)</li> <li><code>ppf</code> (quantile) \\(q_X(u)\\)</li> <li><code>rvs</code> (generation of independent draws of the random variable)</li> </ul> <p>In the following we illustrate the</p> <ul> <li> <p>Binomial distribution: distribution of a discrete random variable taking values in \\(\\{0, \\ldots, n\\}\\) with parameter \\(p\\) where the probability of \\(\\{X = k\\}\\) equal to the probability of getting \\(k\\) heads by throwing independently a coin \\(n\\) times with probability \\(p\\) of getting head.     The <code>pmf</code> of which (that is \\(P[X = k]\\)) is given by</p> \\[ \\begin{equation*}     f_X(k) = P[X = k] = C^k_n p^k (1-p)^{n-k} \\end{equation*} \\] </li> <li> <p>(standard) Normal distribution: central distribution for many reasons in probability.     It is spacial case of the Gaussian distribution denoted by \\(\\mathcal{N}(\\mu, sigma^2)\\), where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation from the mean.     The case where \\(X\\sim \\mathcal{N}(0, 1)\\), that is \\(0\\)-mean and standard deviation equal to \\(1\\) is called the normal distribution with <code>pdf</code> given by</p> \\[     f_X(x) = \\frac{1}{2\\sqrt{\\pi}} e^{-\\frac{x^2}{2}} \\] </li> </ul> <p>As for the binomial distribution</p> <pre><code># import the binominal distribution\nfrom scipy.stats import binom\n\n# paramters of the distribution\nn = 20\np= 0.7\n\nB = binom(n, p)\n\n# produce some random samples\nN = 1000000\nRV = B.rvs(N)\n\n# let us plot the histogram together with the true pmf\nX = np.arange(n+1)\nY1 = np.zeros(n+1)\nfor x in X:\n    count = (RV == x).sum()\n    Y1[x] = count / N\n\nY2 = B.pmf(X)\n\nfig = go.Figure()\nfig.add_bar(x = X, y = Y1, name = \"empirical with %i\"%N)\nfig.add_bar(x = X, y = Y2, name = \"theoretical\")\nfig.show()\n</code></pre> <p>As for the normal distribution</p> <pre><code># import the normal distribution\nfrom scipy.stats import norm\n\n# declare the distribution (without parameters it is a standard normal)\nRV = norm()\n\n# plot of the density\nX = np.linspace(-3,3, 100)\n\nn = RV.pdf(X)\nN = RV.cdf(X)\n\nfig = go.Figure()\n\nfig.add_scatter(x = X, y = n, name = \"pdf\")\nfig.add_scatter(x = X, y = N, name = \"cdf\")\nfig.show()\n\n\n# Generate some random samples and compare the histogram\nY = RV.rvs(1000000)\nfig = go.FigureWidget()\nfig.add_histogram(x = Y, nbinsx = 51, histnorm = \"probability\")\nfig.show()\n</code></pre>"},{"location":"lecture/02-Basics-Scientifics/024-rng-monte-carlo/","title":"Random Numbers and Monte Carlo","text":"<p>We saw that to integrate a function we have access to standard method of quadrature for exact approximation that is, a converging algorithm with exact error estimation.</p> <p>The method however suffers from the curse of dimensionality as the computational complexity increases exponentially with the dimension. Since most of the integrations nowadays involve large dimensional objects, this method is not appropriate.</p> <p>A true revolution in that direction is the so called Monte-Carlo method which is based on the fundamental mathematical results</p> <ul> <li>Law of Large Numbers: taking the arithmetic mean of of independent realisation of the same random variable converges to its mean.</li> <li>Central Limit Theorem: The adjusted error converges to a standard normal distribution.</li> </ul>"},{"location":"lecture/02-Basics-Scientifics/024-rng-monte-carlo/#monte-carlo-convergence-and-error","title":"Monte Carlo: Convergence and Error","text":"<p>Given an iid sequence of square integrable random variables \\(X_1, \\ldots, X_n, \\ldots\\), we denote by \\(X:=X_1\\) with \\(\\mu =E[X]\\) as well as \\(\\sigma = E[(X - \\mu)^2]^{1/2}\\) the mean and standard deviation of this random variable and further denote by \\(S_N\\) the random variable given by the arithmetic mean</p> \\[ \\begin{equation}     S_N = \\frac{1}{N}\\sum_{k=1}^N X_k  \\end{equation} \\] <p>And the residual error \\(\\varepsilon_N\\) given by</p> \\[ \\begin{equation}     \\varepsilon_N = S_N -\\mu \\end{equation} \\] <p>It follows that (Law of Large Numbers)</p> \\[ \\begin{equation} S_N(\\omega) = \\frac{1}{N}\\sum_{k=1}^N X_k(\\omega) \\longrightarrow E[X] = \\mu \\end{equation} \\] <p>for (almost) whatever chosen state \\(\\omega \\in \\Omega\\) and (central limit theorem):</p> \\[ \\begin{equation}     \\varepsilon_N \\approx \\mathcal{N}\\left(0,\\frac{\\sigma^2}{N}\\right) \\end{equation} \\] <p>for \\(N\\) large enough.</p> Law of Large Numbers and Central Limit Theorem <p>Suppose that we have a sequence \\(X_1, X_2, \\ldots, X_n, \\ldots\\) of square integrable random variables on some probability space \\((\\Omega, \\mathcal{F}, P)\\). Assume that this sequence is independent and identically distributed (iid) that is</p> <ul> <li> <p>Independent: for any finite choice of random variable in the sequence, it holds</p> \\[ \\begin{equation*}     P[X_{n_1} \\in A_{1}, \\ldots, X_{n_l} \\in  A_l] = P[X_{n_1} \\in  A_1]\\cdots P[X_{n_l} \\in A_l] = \\prod_{k=1}^l P[X_{n_k} \\in A_{n_k}] \\end{equation*} \\] </li> <li> <p>Identically distributed: Each random variable has the same CDF, that is</p> \\[ \\begin{equation}     F_{X_n}(x) := P[X_n \\leq x] = P[X_1\\leq x] = F_{X_1}(x) \\end{equation} \\] </li> </ul> <p>When this sequence is iid we denote by \\(X:=X_1\\) with \\(\\mu =E[X]\\) as well as \\(\\sigma = E[(X - \\mu)^2]^{1/2}\\) the mean and standard deviation of this random variable. We further denote by \\(S_N\\) the random variable given by the arithmetic mean</p> \\[ \\begin{equation}     S_N = \\frac{1}{N}\\sum_{k=1}^N X_k  \\end{equation} \\] <p>And the residual error \\(\\varepsilon_N\\) given by</p> \\[ \\begin{equation}     \\varepsilon_N = S_N -\\mu \\end{equation} \\] Law of Large Number Theorem: <p>The arithmetic (or sample) mean converges almost surely against the mean \\(\\mu\\), that is</p> \\[ \\begin{equation}     S_N(\\omega) = \\frac{1}{N}\\sum_{k=1}^N X_k(\\omega) \\longrightarrow E[X] = \\mu \\end{equation} \\] <p>for (almost) whatever chosen state \\(\\omega \\in \\Omega\\).</p> Central Limit Theorem: <p>The normalized error converges in distribution to a standard normal distribution, that is</p> \\[ \\begin{equation}     \\frac{\\sqrt{N}}{\\sigma}\\varepsilon_N = \\frac{\\sqrt{N}}{\\sigma}\\left(S_N - \\mu\\right) \\longrightarrow \\mathcal{N}(0, 1) \\end{equation} \\] <p>Or in other terms</p> \\[ \\begin{equation}     \\varepsilon_N \\approx \\mathcal{N}\\left(0,\\frac{\\sigma^2}{N}\\right) \\end{equation} \\] <p>For \\(N\\) large enough.</p> <p>In terms of integration, we therefore have a way to approximate the integral of \\(X\\) with \\(S_N\\), and have an estimate of the approximation error \\(\\varepsilon_N\\). However, in comparison to the standard integration, our approximation of the integral is now a random variable \\(S_N(\\omega)\\). Even if for every state \\(\\omega\\) it converges, the rate of the convergence depends on \\(\\omega\\). Looking at the error \\(\\varepsilon_N\\) it is similar, it is also a random variable that converges in distribution to a normal distribution.</p> <p>Let us have a look at the error. Since it is a random variable, I want to know what is the probability that this error will be larger than \\(\\delta&gt;0\\). Since \\(\\frac{\\sqrt{N}}{\\sigma}\\varepsilon_N \\approx \\mathcal{N}(0, 1)\\) which is the normal distribution, we denote by</p> \\[ \\begin{equation*}     \\varphi(x) = \\frac{e^{-x^2/2}}{\\sqrt{2\\pi}} \\quad \\text{and}\\quad \\Phi(x) = \\int_{-\\infty}^x \\varphi(y) dy \\end{equation*} \\] <p>which are respectively the <code>pdf</code> and the <code>cdf</code> of a standart normal distribution. Note that since the standard normal distribution is symetric, it follows that if \\(Z\\sim \\mathcal{N}(0,1)\\):</p> \\[ \\begin{equation}     P[|Z|&gt;\\alpha] = \\int_{-\\infty}^{-\\alpha} \\varphi(x) dx + \\int_{\\alpha}^\\infty \\varphi(x)dx = 2\\int_{-\\infty}^{-\\alpha}\\varphi(x)dx = 2\\Phi(-\\alpha) \\end{equation} \\] <p>Now looking at the error, we want to estimate the probability that it is going to be smaller than \\(\\delta\\). We have the following estimate</p> \\[ \\begin{align}     P\\left[\\left|S_N - \\mu \\right|\\geq \\delta\\right] &amp; = P\\left[\\left|\\varepsilon_N\\right|\\geq \\delta\\right]\\\\      &amp; = P\\left[\\frac{\\sqrt{N}}{\\sigma}\\left|\\varepsilon_N\\right|\\geq \\frac{\\sqrt{N}}{\\sigma}\\delta\\right]\\\\     &amp;\\approx P\\left[\\left|Z\\right|\\geq \\frac{\\sqrt{N}}{\\sigma}\\delta\\right]\\\\     &amp; = 2\\Phi\\left(-\\frac{\\sqrt{N}}{\\sigma}\\delta\\right)\\\\ \\end{align} \\] <p>Note that for whatever value of \\(\\delta\\) and \\(\\sigma\\), since \\(\\Phi(x)\\to 1\\) when \\(x\\to \\infty\\), hence, the probability of the error being larger than \\(\\delta\\) will converge to \\(0\\) as \\(N\\) is large.</p> <p>What is often done is the following: for an accuracy error \\(\\delta\\) given (for instance \\(\\delta = 10^{-9}\\)), and a confidence level \\(\\alpha\\) (for instance \\(\\alpha = 1\\%\\)), we want to estimate \\(N\\) such that</p> \\[ \\begin{equation}     P\\left[\\left|\\varepsilon_N\\right|\\geq \\delta\\right] \\leq \\alpha \\end{equation} \\] <p>It follows that \\(N\\) must satisfy</p> \\[ \\begin{equation*}     2\\Phi\\left(-\\frac{\\sqrt{N}}{\\sigma}\\delta\\right) = 2\\left(1- \\Phi\\left(\\frac{\\sqrt{N}}{\\sigma}\\delta\\right)\\right) \\leq \\alpha \\end{equation*} \\] <p>which is equivalent to</p> \\[ \\begin{equation} \\sqrt{N} \\geq \\frac{\\sigma}{\\delta} q\\left(1-\\frac{\\alpha}{2}\\right) \\end{equation} \\] <p>Where \\(q = \\Phi^{-1}\\) is the quantile of the standard normal distribution.</p> <p>Convergence speed</p> <p>The error is now given in terms of probability within a confidence interval. However, from the error rate, it turns out that the convergence speed is in square root of \\(N\\) which is particularly slow.</p> <p>Why are these results fundamental in order to solve our problem of dimensionality in terms of integration?</p> <p>Consider a function \\(f:\\mathbb{R}^d \\to \\mathbb{R}\\) and a vector of random variables \\(\\mathbf{X} = (X^1, \\ldots, X^d)\\). We want to compute</p> \\[ \\begin{equation} I = E\\left[f(\\mathbf{X})\\right] = \\int_{\\mathbb{R}^d} f(x^1, \\ldots, x^d)dF_{(X^1, \\ldots, X^d)}(x^1, \\ldots, x^d) \\end{equation} \\] <p>Define now \\(Z = f(X^1,\\ldots, X^d)\\) and consider independent copies \\(\\mathbf{X}_1, \\ldots, \\mathbf{X}_N, \\ldots\\) of \\(\\mathbf{X}=(X^1, \\ldots, X^d)\\). It follows that \\(Z^1, \\ldots, Z^N, \\ldots\\) are iid. Hence, considering \\(N\\) independent samples \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\) where \\(\\mathbf{x}_k = (x^1_k, \\ldots, x^d_k)\\) of \\(\\mathbf{X}\\) yields</p> \\[ \\begin{equation} \\frac{1}{N} \\sum_{k=1}^N f(\\mathbf{x}_k) \\approx E\\left[f(\\mathbf{X})\\right]  \\end{equation} \\] <p>where \\(N\\) is independent of \\(d\\).</p> <p>Out of the main questions</p> <ol> <li>Does the Monte Carlo integral converges to the ture value of the integral?</li> <li>If so, how fast and how accurate?</li> <li>How do I generate indepedent samples of a distribution?</li> </ol> <p>We already answered theoretically to the first 2 questions. It remains the central question number 3 which is a priori quite difficult since a computer (the standard ones) are deterministic machine and per definition can not generate random numbers.</p>"},{"location":"lecture/02-Basics-Scientifics/024-rng-monte-carlo/#generation-of-pseudo-random-numbers","title":"Generation of Pseudo Random Numbers","text":"<p>We first we address the problem of independent realisations of a uniform distribution \\(U\\). A computer that is working with finite arythmetic and rational numbers can naturally not provide algorythmicaly true random numbers. The task is to find deterministic sequences of numbers which distribution can not be distinguished from a uniform distribution by programatic statistical tests. Once again, it is obvious that if you know the trick generating the random numbers, you can always program a test that will violate the uniformity, but we assume that we are agnostic from this viewpoint.</p> Don't program it by yourself for applications! <p>We present here techniques on how random numbers can be generated. Be aware that the random number generators -- RNG -- are highly tested for robustnes and refined. If you have a wrong or weak RNG, this may have dramatical consequences in terms of accuracy. Furthermore, these RNG are optimized so as to be fast. Indeed, for some medelization problems the number of random numbers needed may well reach several billions. Just to warn you that it is a VERY bad idea to program your homemade RNG for Monte Carlo pruposes, we just present the idea and shortcomings of some of the techniques.</p> <p>Given a set \\(\\{0,1,\\ldots,M-1\\}\\), we want to find a sequence \\((i_l)\\) in this set such that \\((x_l)=(i_l/M)\\) is uniformly distributed on \\([0,1]\\).</p> <p>Definition: Random Number Generator</p> <p>A RNG consists of a state space \\(X\\), a transition function \\(T:X\\to X\\) as well as a mapping \\(G:X\\to \\{0,\\ldots,M-1\\}\\). Given an element \\(x_0 \\in X\\) -- the seed -- the pseudorandom numbers \\(i_l=G(x_l)\\) are computed recursively by \\(x_l=T(x_{l-1})\\).</p> <p>By definition, since \\(X\\) is finite, it follows that the sequence of pseudo-random numbers is periodic. Indeed, there must exists \\(l\\) such that \\(x_l=x_0\\) and so you will get a period of \\(l\\). Note also that \\(G\\) is not a bijection, so it may well happen that \\(i_l=i_k\\) without necessarily having \\(x_l=x_k\\).</p> <p>A RNG is tested along the following lines:</p> <ul> <li>Statistical uniformity: not distinguishable by feaseable statistical tests from uniformity</li> <li>Speed: sometimes \\(10e18\\) numbers are needed...</li> <li>Period length: Rule of thumb it should be larger than the square of the number necessary to construct.</li> <li>Reproducibility: For debugging and testing reason, a random number should be reproducible.</li> <li>Jumping ahead: Possibility to generate \\(x_{l+n}\\) for a given \\(n\\) and \\(l\\) without the intermidiary values. Important for parallelization.</li> </ul> <p>The Prototypical class are the linear where \\(X=\\{0,\\ldots,M-1\\}\\), \\(G=Id\\) and </p> \\[ T(x)=(ax+c) \\quad\\text{modulo }M \\] <p>By basic number theory, the period length is exactly \\(M\\) if </p> <ul> <li>\\(c\\neq 0\\).</li> <li>every prime number dividing \\(M\\) also divide \\(a-1\\).</li> <li>if \\(M\\) is divisible by \\(4\\) then so is \\(a-1\\).</li> </ul> <p>Depending on the architecture and OS of your computer, different random number generator are implmented. For Linux and many other scientific libraries, the GCC implementation uses</p> \\[ M = 2^{32} \\quad a = 1103515245 \\quad c = 12345 \\] <pre><code># basic parameters\na= 1103515245\nc= 12345\nM = 2**32\n\ndef T(x, a = 1103515245, c = 12345 , M = 2 ** 32):\n    term = a * x + c\n    return term % M\n\ndef RN(seed, N):\n    X = np.zeros(N)\n    for n in range(N):\n        if n == 0:\n            X[n] = seed\n        else:\n            X[n] = T(X[n-1])\n    return X/M\n\n# SEED:\nseed = 14\n\nprint(f\"RN: {RN(seed, 10)*100}\")\n</code></pre> <p>Let us compare the speed between our implementation and numpy implementation</p> <pre><code>%timeit RN(10, 100000)\n</code></pre> <pre><code>%timeit np.random.rand(100000)\n</code></pre> <p>We generate some random numbers and provide a scatter plot of which</p> <pre><code># set the number\nN = 100000\n\nX1 = RN(10, N)\nX2 = RN(10, N)\n\nfig = go.Figure()\nfig.add_scatter(x = X1, y = X2, mode = 'markers')\n\nfig.layout.width = 800\nfig.layout.height = 800\n\nfig.show()\n</code></pre> <p>Some estimation technique from John von Neuman</p> <p>Consider the following code, what does it estimate? <pre><code>def estimate_what(N):\n    m = 0\n    X = np.random.rand(N)\n    Y = np.random.rand(N)\n    for i in range(N):\n        if X[i] ** 2 + Y[i] ** 2 &lt;= 1:\n            m+=1\n    return 4 * m / N\n\nestimate_what(200000)\n</code></pre></p>"},{"location":"lecture/02-Basics-Scientifics/024-rng-monte-carlo/#sampling-from-another-distribution","title":"Sampling from another distribution","text":"<p>We have seen how to sample from a uniform distribution. Sampling from another one relies on the following proposition:</p> <p>Proposition</p> <p>Let \\(X\\) be a random variable with CDF \\(F_X\\colon \\mathbb{R}\\to [0,1]\\), \\(x \\mapsto F_X(x) = P[X\\leq x]\\) and \\(U\\) a uniformly distributed random variable. Considering the quantile \\(q_X \\colon (0,1)\\to \\mathbb{R}\\)</p> \\[ \\begin{equation} q_X(u) = \\inf \\{m\\colon F_L(m)\\geq u\\} =F_X^{-1}(u) \\end{equation} \\] <p>it follows that the random variable \\(q_X(U)\\) has the same distribution as \\(X\\), that is, both random variable have the same CDF.</p> Proof <p>By definition of the quantile function, it follows that \\(q_X(u)\\leq x\\) if and only if \\(u\\leq F_X(x)\\). Furthermore, \\(q_X\\colon (0,1) \\to \\mathbb{R}\\) can be considered as a random variable on \\((\\tilde{\\Omega}, \\tilde{F}, \\tilde{P})\\) where \\(\\tilde{\\Omega} = (0,1)\\), \\(\\tilde{F}= \\mathcal{B}(0,1)\\) is the borel \\(\\sigma\\)-algebra generated by open subsets of \\((0,1)\\) and \\(\\tilde{P}=\\lambda\\) the Lebesgue measure on \\((0,1)\\), that is the measure of intervals with \\(d\\lambda = dx\\). Considering the CDF of \\(q_X\\), it holds</p> \\[ \\begin{equation}     F_{q_X}(x)=\\lambda (\\{u \\colon q_X(u)\\leq x\\})=\\lambda(\\{u\\colon u\\leq F_X(x)\\}=\\lambda (0,F_X(x)]=F_X(x) \\end{equation} \\] <p>If we know the quantile distribution explicitely, then we can generate any sample of that distribution. If we consider the exponential distribution for instance where</p> \\[ \\begin{equation} F_X(x) = 1- e^{-\\lambda x}, \\quad x\\geq 0 \\end{equation} \\] <p>Then it follows that</p> \\[ \\begin{equation} q_X(u) = -\\ln\\left(1-u\\right) / \\lambda \\end{equation} \\] <p>From our proposition, sampling from \\(X\\) is the same as sampling from \\(q_X(U)\\) where \\(U\\) is a \\((0,1)\\) uniform distribution.</p> <pre><code># Example with the exponential distribution.\nlamb = 5\ndef q(u, lamb):\n    return - np.log(1-u) / lamb\n\n# as comparison we consider the scipy exponential distribution (scale = 1/lamb)\n\nRV = sp.stats.expon(scale = 1/lamb)\n\nN = 10000\n\n# generate N samples from the uniform distribution\nU = np.random.rand(N)\n\n# plug those random numbers into the quantile\nX= q(U, lamb)\n\n# generate N samples from the exponential RV\nY = RV.rvs(N)\n\n# Plot the histogram for both samples\nfig = go.Figure()\nfig.add_histogram(x = X, name = 'quantile')\nfig.add_histogram(x = X, name = 'rvs')\nfig.show()\n</code></pre> <p>There is however usually no analytical form for the quantile distribution, therefore one has to use numerical inverse of it. These one are known for most of the classical distributions and accessible through <code>python.stats</code> by means of the <code>ppf</code> method.</p> <p>In the following we compute the following integral</p> \\[ \\begin{equation}     E[(X-K)^+] = \\int_K^\\infty (x-K)dF_X(x) \\end{equation} \\] <pre><code>from scipy.integrate import quad\nfrom scipy.stats import norm\n\n# standard integration of the random variable with quad\ndef Int1(K, RV):\n    def integrand(x):\n        term = np.maximum(x - K,0) * RV.pdf(x)\n        return term\n    result, err = quad(integrand, -np.Inf, np.Inf)\n    return result\n\n# Same integration using monte carlo\ndef Int2(K, RV, N):\n    # generate $N$ samples\n    x = RV.rvs(N)\n    result = np.maximum(x - K, 0)\n    result = np.mean(result)\n    return result\n\n#\nN = 100000\nRV = norm(loc = 1, scale = 2)\nK = 1.5\n\n\nprint(f\"Standard integral {Int1(K, RV)}\")\nprint(f\"MC integral {Int2(K, RV)}\")\n</code></pre>"},{"location":"lecture/02-Basics-Scientifics/024-rng-monte-carlo/#variance-reduction","title":"Variance Reduction","text":"<p>While integrating, the domain or the nature of the function to integrate influence a lot the convergence of Monte-Carlo. Consider the following simple example. Suppose that \\(X\\) is Cauchy distributed, that is, have a pdf of the form</p> \\[ \\begin{equation} dF_X(x)=f_X(x)=\\frac{1}{\\pi (1+x^2)} \\end{equation} \\] <p>and we want to compute \\(P_X[L&gt;5] = 1-F_X(5)\\), that it compute the expectation</p> \\[ \\begin{equation}     P_X[L&gt;5]=E[1_{\\{X&gt;5\\}}]=\\int_{5}^\\infty f_X(x) dx \\end{equation} \\] <p>If we draw a sample of the distribution, only very few of them will enter in the relevant part of the computation of the integral</p> <p><pre><code>x = sp.stats.cauchy()\n# here is the true value P[L&gt;3]\nh_true = 1-x.cdf(5)\nprint(f\"Value we are searching for {h_true}\")\n</code></pre> It means that only 6% of the sample of our Monte-Carlo will be useful in the computation of this integral. This is quite often a problem as many applications involves evaluation with respect to extreme values of a random variables, events that are large but rare. And since the Monte-Carlo method is typically slow in terms of convergence like \\(\\sqrt{N}\\), we will need a huge amount of samples. </p> <pre><code># %%\nN = 1000000\nX = x.rvs(N)\n# We count the average number of the sample above 5\nprint(np.absolute(h_true - 1/N * np.sum(X&gt;5))/ h_true)\n</code></pre> <p>However, a simple change of variables lets us use 100% of draws We are trying to estimate the quantity</p> \\[ \\int_5^\\infty \\frac{1}{\\pi (1 + x^2)} dx \\] <p>Using the substitution \\(y = 5/x\\) (and a little algebra), we get</p> \\[ \\int_0^1 \\frac{5}{\\pi(25 + y^2)} dy \\] <p>Hence, a much more efficient MC estimator is </p> \\[ \\frac{1}{N} \\sum_{k=0}^{N-1} \\frac{5}{\\pi(25 + y_k^2)} \\] <p>where the sample \\(y_0, y_1, \\ldots, y_N\\) is drawn from a uniform \\((0,1)\\) random variable.</p> <pre><code>N = 100000\nY = np.random.rand(N)\nh_cv = 1.0/n * np.sum(5/(np.pi * (25 + Y**2)))\n\nprint(f\"Integral relative error: {np.abs(h_cv - h_true)/h_true}\")\n</code></pre>"},{"location":"lecture/03-Data/031-pandas-first-steps/","title":"Handeling Data with Pandas","text":"<p>The major input to any scientific computations are data (for calibration purposes, further computations, etc.). Several difficulties comes to mind in this direction:</p> <ul> <li>One dimensional: basically, long sequences of data such as time series that needs vertical efficient access</li> <li>Multi dimensional: data that are large in dimension (such as matrices, tensor) that need to be accessed horizontaly for efficient computations</li> <li>Heterogeneity: handeling data of different nature at the same time (numbers, vectors, strings, time, etc.)</li> </ul> <p>Furthermore, before even organized those data along these dimensions, data have to be</p> <ul> <li>collected: how? where to store? which format? how to read?</li> <li>cleaned: missing data? mesurement errors?</li> <li>converted: strings of dates to datetime? flags to binary values?</li> <li>pre-processed: up/down sampling? Partial selection? etc.</li> </ul> <p>We saw that numpy provides efficient single objects (modulo memory size) to handle the dimensionality and types (numerical, strings, etc.). However, it is quite convoluted to handle heterogeneity as a numpy array shall only contain fixed typed data.</p> <p>Historically, databases, which we will address later, are the answer to these questions. However, based on numpy, the library <code>pandas</code> provides a very powerfull and handy answer to most of the points raised here above.</p> <p>The Pandas library is mature and deep with a lot of functionalities. As numpy it is imported as follows with the usual nickname <code>pd</code></p> <pre><code>import pandas as pd\nimport numpy as np\n</code></pre> <p>Pandas is based on two major components:</p> <ul> <li>Series: One dimensional labelled array or data with the following components: <ul> <li><code>data</code>: Numpy array (one dimensional usually)</li> <li><code>index</code>: labels of any type for each row</li> <li><code>name</code>: denomination of the series</li> </ul> </li> <li>Dataframes: Collection of series of different types with common index (tabular format)</li> </ul>"},{"location":"lecture/03-Data/031-pandas-first-steps/#pandas-series","title":"Pandas Series","text":"<p>A <code>pandas</code> series is a sequence of <code>data</code> (a numpy array) together with an explicit <code>index</code> as well as a <code>name</code>.</p> <pre><code>import pandas as pd\nimport numpy as np\n\ns = pd.Series(data = [\"NY\", \"NY\" , \"London\", np.nan, \"SG\"])\n\ndisplay(s)\n</code></pre> <p>In this case we just provide the <code>data</code> part of the series, the <code>index</code> is automatically integers and the <code>name</code> is None.</p> <p>To specify the labels (or index) of the series as well as the name we proceed as follows</p> <pre><code>pop = pd.Series(\n    data = [25, 22 , 8, 5],\n    index = ['SH', 'BJ', 'London', 'SG'],\n    name = 'Population'\n    )\ndisplay(pop)\n</code></pre>"},{"location":"lecture/03-Data/031-pandas-first-steps/#pandas-dataframe","title":"Pandas Dataframe","text":"<p>A dataframe is a tabular collection of series with a common index</p> <pre><code># Create a plain dataframe with 3 columns and 10 rows \n# filled with random numbers\n\n# generate 10x3 random data\ndata = np.random.rand(10, 3)\nprint(data)\n\n# create the dataframe\ndf = pd.DataFrame(\n    data = data\n)\ndisplay(df)\n</code></pre> <p>Since each column is a pandas series, the name of the column is the corresponding name of the series. As for the index, it is common for each column.</p> <pre><code># generate a 10*3 random set of data\ndata = np.random.rand(10, 3)\nprint(\"Data:\")\nprint(data)\n\n# provides a list of names\ncols = ['Tencent', 'Alibaba', 'Baidu']\nprint(\"Columns\")\nprint(cols)\n\n# Create a datetime index of size 10\nidx = pd.date_range(start = '2024-04-01', periods = 10)\nprint(\"Index\")\nprint(idx)\n\n# create the dataframe\ndf = pd.DataFrame(\n    data = data,\n    index = idx,\n    columns = cols\n)\ndisplay(df)\n</code></pre>"},{"location":"lecture/03-Data/031-pandas-first-steps/#short-infos-about-the-dataframe","title":"Short Infos about the Dataframe","text":"<p>Several options allows to inspect rapidly the nature of the Dataframe/Series</p> <ul> <li><code>head</code>/<code>tail</code>: shows the first/last rows of the dataframe</li> <li><code>index</code>/<code>columns</code>: show the index/columns</li> <li><code>info</code>: show generic infos from the dataframe</li> <li><code>describe</code>: returns basic statistics about the dataframe</li> <li><code>plot</code>: allows to visualize data from the dataframe</li> </ul> <pre><code># showing the first lines (by default 5)\ndisplay(df.head())\n# showing the last 7 lines\ndisplay(df.tail(7))\n# show the index\ndisplay(df.index)\n# shows the columns\ndisplay(df.columns)\n# show the info from the dataframe\ndisplay(df.info())\n# discplay a summary statistics about the dataframe\ndisplay(df.describe())\n</code></pre> <p>A more easier way to explore a dataframe is to plot the data available.</p> Note <p>By default <code>pandas</code> will use the ploting library <code>matplotlib</code> to show data, it is however not particulary nice. Here we make use of <code>plotly</code> as the library to plot data</p> <pre><code># import plotly and tell pandas to use it as backend\nimport plotly.graph_objs as go\npd.options.plotting.backend = \"plotly\"\n\n# generate a large dataframe\nidx = pd.date_range(start = '2014-04-08', end = '2024-04-18', freq = 'B')\nN = len(idx)\ncols = ['Tencent', 'Baidu', 'Alibaba']\n# generate random data between -0.05 and 0.05\ndata = 0.05 - np.random.rand(N, 3) / 10\n# take the cumulative product of 1+data along the time axis\ndata = (1+data).cumprod(axis = 0)\n\ndf = pd.DataFrame(\n    data = data,\n    index = dates,\n    columns=['Tencent', 'Alibaba', 'Baidu']\n)\n\n# Plot each columns \ndf.plot()\n\n# plot a specific column\ndf['Tencent'].plot()\n\n# plot histogram of each\ndf.plot(kind = 'histogram')\n</code></pre>"},{"location":"lecture/03-Data/031-pandas-first-steps/#selecting-data","title":"Selecting Data","text":""},{"location":"lecture/03-Data/031-pandas-first-steps/#selecting-columns","title":"Selecting Columns","text":"<p>A collection of series, selecting specific columns of a dataframe is done as follows</p> <pre><code># get the column Tencent (which is a series)\ndisplay(df['Tencent'])\n\n# get a list of columns (hence a dataframe)\ndisplay(df[['Tencent', 'Baidu']])\n</code></pre>"},{"location":"lecture/03-Data/031-pandas-first-steps/#selecting-rows","title":"Selecting Rows","text":"<p>Two main methods for the selection in terms of rows</p> <ul> <li><code>iloc</code>: positional, integer wize, location (as <code>numpy</code>)</li> <li><code>loc</code>: in terms of labels</li> </ul> <p>As for <code>iloc</code> is works basically as <code>numpy</code></p> <pre><code># getting the 3rd row\ndisplay(df.iloc[3])\n\n# getting the last row\ndisplay(df.iloc[-1])\n\n# getting the 3rd to the 5th row\ndisplay(df.iloc[2:6])\n</code></pre> <p>As for <code>loc</code>, the selection is done in terms of label or slice <pre><code># show a specific row for 2024-04-03\ndisplay(df.loc['2024-04-03'])\n\n# show a range (note that the labels starts from 2024-04-01)\ndisplay(df.loc['2024-03-15': '2024-04-10'])\n\n# show everything after '2024-04-07'\ndisplay(df.loc['2024-04-07':])\n\n# the output of a range is a dataframe, we can select a subsection of columns\ndf.loc['2024-04-03':'2024-04-18'][['Tencent', 'Alibaba']]\n</code></pre></p>"},{"location":"lecture/03-Data/031-pandas-first-steps/#read-and-write-data","title":"Read and Write Data","text":"<p>Generating random dataframe or defining per hand the data has limited use, since our goal is to handle large sets of data of heterogeneous type. Data are usually available in different format</p> <ul> <li>file data: (among others)<ul> <li><code>csv</code>: text files comma (or tab) separated values files. Most basic, most common. Extension is usually <code>.csv</code> or <code>.tvs</code>.</li> <li><code>excel</code>: excel files. More advanced files with particular structure (several spreadsheets for instance). Extension is usually <code>.xls</code> or <code>.xlsx</code></li> <li><code>pickle</code>:  python data storage. Rarely used as it is not that efficient and dependent on the version of python used to save or load. Contains however all the information of the dataframe. Extension is <code>.pkl</code></li> <li><code>hdf</code>: cross platform storage of data and structure for large datasets. Used quite often for bio data or physics. Extension is <code>.hdf</code> or <code>hd5</code>.</li> <li>...</li> </ul> </li> <li>databases: Either online or offline. We will see that later.</li> </ul> <p>To read specific data files stored in folder <code>./data/</code> where <code>./</code> is where your python script is located and running, loading data is done as follows:</p> <p><pre><code># read csv file\ndf = pd.read_csv('./data/my_dataset.csv')\n\n# read tab separated values file \ndf = pd.read_csv('./data/my_dataset.tvs', sep = '\\t')\n\n# read (the first sheet of) an excel file\ndf = pd.read_excel('./data/my_dataset.xls')\n\n# read the third sheet of an excel file\ndf = pd.read_excel('./data/my_dataset.xls', sheet_name = 2)\n\n# read the sheet of an excel file with sheet name 'sheet05'\ndf = pd.read_excel('./data/my_dataset.xls', sheet_name = 'sheet05')\n</code></pre> Once done with cleaning and handeling of data you can save the resulting dataframe into a file (preferably <code>csv</code>). <pre><code># save the new dataframe into a new file `my_data_20240428.csv` in the subfolder ./data/\ndf.to_csv('./data/my_data_20240428.csv')\n</code></pre></p> <p>DUMP EXCEL</p> <p>Excel has been used for many years as the prefered hybrid solution from your uncle to handle tabular data with point and click. However, firstly, the format is not open source, complex, buggy, not able to handle large datasets, slow, and usually not platform compatible. Secondly, you are younger and more knowladgable about tech than your uncle: It is a bad idea in 2024 to handle data in this format, so forget about excel alltogether as a format and platform.</p>"},{"location":"lecture/03-Data/031-pandas-first-steps/#organizing-and-cleaning-data","title":"Organizing and Cleaning Data","text":"<p>When you read a dataset into a dataframe, pandas tries to guess as well as it can the structure and type (detecting if there is a header for the column names, or the type of each column). However, most of the time, you have to proceed first first with the following operations</p> <ul> <li>Convert each column to the correct format</li> <li>Change the column names</li> <li>Set an index (add a new one or set one existing column as an index)</li> </ul> <p>For this section we will use the dataset <code>csi_short.csv</code> as well as <code>movies.xls</code> and suppose that it is stored into the folder <code>./data/csi_short.csv</code> and <code>./data/movies.xls</code>.</p> <p>You can download the datasets from here if you don't have them already.</p> <ul> <li>movies</li> <li>csi_short</li> </ul> <p>We start with <code>csi_short</code></p> <pre><code># load the dataset\ndfcsi = pd.read_csv('./data/csi_short.csv')\n# Check content\ndisplay(dfcsi.head())\n\n# Check datatype of the columns\ndisplay(dfcsi.dtypes)\n</code></pre> <p>The data set consists of 5 daily stock prices.</p> <ul> <li>We convert the column of dates to a datetime format</li> <li>We rename the column <code>Date</code> to <code>date</code> </li> <li>We set the newly renamed column <code>date</code> as an index</li> <li>We rename the stocks columns to <code>SXX</code> where <code>XX</code> stands for <code>01</code>, <code>02</code>, ..., <code>15</code>, ...</li> </ul> <pre><code># conversion to a datetime format of the column date\ndfcsi['Date'] = pd.to_datetime(dfcsi['Date'])   \n\n# Rename the column `Date` to `date` using dictionarry assignment\ndfcsi = dfcsi.rename(columns = {'Date': 'date'})\n\n# Set the colum date as index\ndfcsi = dfcsi.set_index('date')\n\n# rename the stock columns from their names to SXX (we have N stocks where N is the number of columns)\nCOLSNAMES = [f\"S{i:02}\" for i in range(1,len(dfcsi.columns)+1)]\nprint(COLSNAMES)\n# Assignment from the column names as array\ndfcsi.columns = COLSNAMES\n\ndisplay(dfcsi)\n</code></pre> <p>Let us handle <code>movies.xls</code></p> <pre><code># load the excel data\ndfmovies = pd.read_excel('./data/movies.xls')\n# show the basic infos\ndisplay(dfmovies.info())\n</code></pre> <p>while looking at the infos you will see that pandas overdone it in terms of conversion of columns</p> <ul> <li><code>year</code> in the original file is a string which it converted into an int. We want a date</li> <li><code>duration</code> is in minutes which is converted into an int. We want a timedelta.</li> <li><code>Aspect ratio</code> is given as 16/3, 16/9... in the original file which has been converted to a float. We want a string.</li> <li><code>Facebook Likes - Actor 3</code> as well as <code>Reviews by Crtiics</code> unlike the other columns is not converted to an int. We want all Facebook and revies by users likes to be converted to int.</li> <li><code>Reviews by Crtiics</code> is mispelled and shoud be renamed to <code>Reviews by Critics</code></li> </ul> <p><code>NaN</code> values and <code>int</code></p> <p>If you rapidly check, you will see that you can not plainly convert some float columns you know are integers such as <code>Facebook Likes - Actor 3</code> since there are some nan values.</p> <pre><code>dfmovies[\"Facebook Likes - Actor 3\"].astype(int)  # This will throw an error\n</code></pre> <p>In this case, the column shall be casted to the large int datatype of pandas <code>pd.Int64Dtype()</code></p> <pre><code>dfmovies[\"Facebook Likes - Actor 3\"].astype('Int64')  # This will work\n</code></pre> <p>You can naturally handle these columns one by one, however, you can specify pandas while loading the csv file the dtype you want and then convert the columns that are necessary.</p> <pre><code># Set the dtypes as dictionarry before loading the csv\nDTYPES = {\n    \"Title\": str,\n    \"Year\": str,  # We will convert the year into datetime object so as string it is easier\n    \"Genres\": str,\n    \"Language\": str,\n    \"Country\": str,\n    \"Content Rating\": str,\n    \"Duration\": pd.Int64Dtype(),  # the duration is in minutes, we will convert it in timedelta\n    \"Aspect Ratio\": str,  # The aspect ratio is of the form 16/9\n    \"Budget\": pd.Int64Dtype(),\n    \"Gross Earnings\": pd.Int64Dtype(),\n    \"Director\": str,\n    \"Actor 1\": str,\n    \"Actor 2\": str,\n    \"Actor 3\": str,\n    'Facebook Likes - Director': pd.Int64Dtype(),\n    'Facebook Likes - Actor 1': pd.Int64Dtype(),\n    'Facebook Likes - Actor 2': pd.Int64Dtype(),\n    'Facebook Likes - Actor 3': pd.Int64Dtype(),\n    'Facebook Likes - cast Total': pd.Int64Dtype(),\n    'Facebook likes - Movie': pd.Int64Dtype(),\n    'Facenumber in posters': pd.Int64Dtype(),\n    'User Votes': pd.Int64Dtype(),\n    'Reviews by Users': pd.Int64Dtype(),\n    'Reviews by Crtiics': pd.Int64Dtype(),\n    'IMDB Score': float,\n}\n\n# reload the data by specifying the dictionarry\n\ndfmovies = pd.read_excel(\"./data/movies.xls\", dtype=DTYPES)\n\n# rename the column Reviews by Crtiics to Reviews by Critics\ndfmovies = dfmovies.rename(columns={\"Reviews by Crtiics\": \"Reviews by Critics\"})\n\n# Convert the column year to datetime (string %Y)\ndfmovies[\"Year\"] = pd.to_datetime(dfmovies[\"Year\"], format=\"%Y\")\n\n# Convert the column duration from number of minutes to a timedelta\ndfmovies[\"Duration\"] = pd.to_timedelta(dfmovies[\"Duration\"], unit=\"m\")\n\n# show the info\ndfmovies.info()\n</code></pre>"},{"location":"lecture/03-Data/032-pandas-computations/","title":"Computations Based on Data","text":"<p>So far we learned how to fenerate data containers, load, clean and organized data. This data container (<code>Series</code> or <code>DataFrame</code>) is the basis to further computations.</p> <p>Throughout we illustrate the basic abilities using the data set <code>csi_short</code>.</p> <p><pre><code># load the dataset, convert date column to datetime and set as index\ndf = pd.read_csv('./data/csi_short.csv')\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf = df.set_index(\"Date\")\n\n# show the data set\ndf.head()\n</code></pre> This is a collection of time series of stock prices over a certain number of years. We denote by \\(\\mathbf{S} = (S^{0}, \\ldots, S^{d-1})\\) the \\(d\\) different stocks and \\(S^k(t)\\) the price of stock \\(k\\) at time \\(t\\). In other terms \\(S^k = (S^k(t))\\) represents the \\(k\\)-th column in the dataframe, while \\(\\mathbf{S}(t) = (S^0(t), \\ldots, S^{d-1}(t))\\) represents the \\(t\\)-th row. </p>"},{"location":"lecture/03-Data/032-pandas-computations/#broadcasting-functions-on-columns","title":"Broadcasting Functions on Columns","text":"<p>Basic operations such as additions/multiplications, etc apply pointwize and return a series/column</p> <pre><code># Adding/multiplying, dividing two columns\ndisplay(df['000001.SZ'] + df['000002.SZ'])\ndisplay(df['000001.SZ'] * df['000002.SZ'])\ndisplay(df['000001.SZ'] / df['000002.SZ'])\n</code></pre> <p>Numpy functions are designed to be broadcasted (that is apply element wize).</p> <pre><code># Compute the exponential\ndisplay(np.exp(df['000001.SZ']))\n# Compute the log of the full dataframe\ndisplay(np.log(df))\n</code></pre> <p>Applying these kind of operations elementwize necessitate some care about the dimension of objects. For instance dividing the dataframe by the first column, means that each column should be divided by the first one. In that case <code>add</code>, <code>sub</code>, <code>mult</code>, <code>div</code>, <code>floor</code>, <code>pow</code> etc are the correct way to do</p> <pre><code># Divide the dataframe by the first row\ndisplay(df.div(df['000001.SZ'], axis = 0))\n</code></pre>"},{"location":"lecture/03-Data/032-pandas-computations/#mean-standard-deviation","title":"Mean, Standard Deviation,","text":"<p>Classical operations can be processed on columns or rows using basic functionalities of pandas</p> <pre><code># Compute the mean and standard deviation of each column\ndisplay(df.mean())\ndisplay(df.std())\n</code></pre> <p>These functions apply column wize (in the direction of time <code>axis = 0</code>). You can decide to compute for each row the mean or standard deviation of all columns.</p> <pre><code># Compute the mean and standard deviation of each row\ndisplay(df.mean(axis = 1))\ndisplay(df.std(axis = 1))\n</code></pre>"},{"location":"lecture/03-Data/032-pandas-computations/#shifting-cumsumprod-rolling","title":"Shifting, Cumsum/prod, Rolling","text":"<p>The previous operations are standard and aligned, they do not allow to combine values in different rows and columns.</p>"},{"location":"lecture/03-Data/032-pandas-computations/#shifting","title":"Shifting","text":"<p>In our example, we want to compute the returns of the stocks</p> \\[ \\begin{equation*} R^k(t):=\\frac{S^k(t) - S^{k}(t-1)}{S^k(t)}, \\quad R^k(0) = 0 \\end{equation*} \\] <p>for which holds</p> \\[ \\begin{equation*} S^k(t) = S^k(t-1)(1+R^k(t)) = S^k(0) \\prod_{s=1}^t(1+R^k(s)) \\end{equation*} \\] <p>As for the division, and substraction we can apply the previous operations, however we need to specify how to shift the dataframe with the <code>shift</code> operator.</p> <p><pre><code># shifting the dataframe by one row\ndisplay(df.shift(1))\n\n# Computing the returns\ndisplay(\n    (df - df.shift(1))/df.shift(1)\n)\n</code></pre> You will note an issue for the first row since <code>df.shif[1].iloc[0]</code> is not well defined and therefore set to <code>nan</code>. We need to fill the <code>nan</code> values with <code>0</code>.</p> <pre><code># set a new dataframe dfret \ndfret = (df - df.shift(1))/df.shift(1)\n\n# Fill the nan values with 0\ndfret = dfret.fillna(0)\n\ndfret.head()\n</code></pre>"},{"location":"lecture/03-Data/032-pandas-computations/#cumulative-operations","title":"Cumulative Operations","text":"<p>According to the definition, mathematically we should have</p> \\[ \\begin{equation*} \\frac{S^k(t)}{S^k(0)} = \\prod_{s=0}^t (1+R^k(t)) \\end{equation*} \\] <p>where \\(R^k(0) = 0\\). Computing the cumulative product on the right hand side requires a sequential function that is present in pandas in the form of the function <code>cumprod</code> (<code>cumsum</code> as you imagine is also available).</p> <pre><code># compute the cumulative product\ndisplay((1+dfret).cumprod())\n\n# compare with df/df.iloc[0]\ndisplay(df/df.iloc[0])\n</code></pre>"},{"location":"lecture/03-Data/032-pandas-computations/#rolling-operations","title":"Rolling Operations","text":"<p>Rolling operations are operations of the kind</p> \\[ \\bar{S}^k(t) = \\frac{1}{N} \\sum_{T- N&lt; s&lt;t } S^k(t) \\] <p>which is here a rolling average of the last \\(N\\) values.</p> <pre><code># compute the moving average 7 days of the stock prices\ndf.rolling(7).mean()\n</code></pre> <p>Some more complex computations can be performed, for example the maximum drop down defined as</p> \\[ \\begin{equation} MDD(t) = \\inf_{t-N &lt;s \\leq t}\\frac{S^\\ast(t) - S(t)}{S^\\ast(t)} \\end{equation} \\] <p>where</p> \\[ \\begin{equation} S^\\ast(t) = \\sup_{t-N &lt; s \\leq t} S(s) \\end{equation} \\] <pre><code># Shifting window\nN = 200\n\n# define the rolling running maximum\ndfmax = df.rolling(N).maximum()\n\n# define the maximum drop down\nmdd = ((dfmax - df)/dfmax).rolling(N).minimum()\n</code></pre>"},{"location":"lecture/03-Data/032-pandas-computations/#apply-function-iterrows","title":"Apply Function, Iterrows","text":"<p>Sometimes, you want to apply complex functions to the dataframe that can not be summarized by simple broadcasting functions and operations.</p> <p>In this case, we pass recursively the rows of the dataframe to a function.</p> <p>Two possibilities</p> <ul> <li><code>apply</code>: take as input a function that will act on each row. Internally pandas will loop and return a dataframe with the result indexed by the former dataframe.</li> <li><code>iterrows</code>: this will iterate through the dataframe row by row in a loop fashion. Does not return a dataframe.</li> </ul> <p>The preferred solution when possible is <code>apply</code>.</p> <pre><code># Compute the ratio of the rolling mean 7 divided by 30\ndfmaratio = df.rolling(7).mean() / df.rolling(30).mean()\n\n# Return a dataframe columns for each stock equal to 0 if dfmratio &lt;1 and 1 otherwize all normalized by the sum.\n# Definition of the function acting on each row\ndef strategy(row):\n    # return an array where 1 if row[i]&gt;1 and 0 otherwize\n    w = np.where(row&gt;1, 1, 0)\n    # normalized the array\n    sum = w.sum()\n    if sum != 0:\n        w = w/sum\n    # we return a pandas series with w as data and row.index as index (here row.index are the column names)\n    return pd.Series(data = w, index = row.index)\n\n# Now we apply the function to the dataframe\nstrategy = dfmaratio.apply(fun, axis = 1)\n\ndisplay(strategy)\n</code></pre> <p>Sometimes <code>iterrows</code> is preferable. It is an interator on the rows of the dataframe with access to the index.</p> <pre><code># creating a DataFrame \ndf = pd.DataFrame({\n    'Names': ['Alice', 'Bob', 'Charlie'],\n    'Scores': [85, 70, 92]\n})\n\n# iterate over each row of the DataFrame\nfor index, row in df.iterrows():\n    print(f\"Index of the current row: {index}\") # will print the index\n    print(f\"Content of the current row (series): {row}\")\n\n# this interation does not do anything except printing.\n# If we want to act and store the result in the dataframe we use the function at\nfor index, row in df.iterrows():\n    # assign grades based on scores\n    if row['Scores'] &gt;= 90:\n        grade = 'A'\n    elif row['Scores'] &gt;= 80:\n        grade = 'B'\n    elif row['Scores'] &gt;= 70:\n        grade = 'C'\n    else:\n        grade = 'F'\n\n    # set the grade in the 'Grade' column for the current row\n    df.at[index, 'Grade'] = grade\n\n# display DataFrame with names, scores, and the assigned grades\nprint(df)\n</code></pre>"},{"location":"lecture/03-Data/033-pandas-manipulations/","title":"Data Manipulations","text":"<p>Those manipulations are usually akin to database functionalities. For simplicity we separate into two parts</p> <ul> <li><code>concat</code>, <code>merge</code>, <code>pivot</code>: combine data from different dataframes or modify the structure of the organization of the current dataframe. Usually does not involve any computations.</li> <li><code>groupby</code>: Aggregation of data (partitioning) to perform further computations.</li> </ul>"},{"location":"lecture/03-Data/033-pandas-manipulations/#combiningreorganizing-data","title":"Combining/Reorganizing data","text":"<p>We handle here some most used functionalities. For an overall presentation, we refer to the pandas tutorial Merge, join, concatenate and compare.</p>"},{"location":"lecture/03-Data/033-pandas-manipulations/#concatenation","title":"Concatenation","text":"<p>Given a list of dataframe <code>[df1, ..., dfN]</code>, concatenation intends to return a single dataframe with <code>df1</code>,..., <code>dfN</code> stacked either vertically (<code>axis = 0</code>) or horizontally (<code>axis = 1</code>). Clearly, in order to get a meaningful concatenation, the columns or index among the dataframes to be concatenated should share common elements.</p> <pre><code>import pandas as pd\n\ndata1 = [\n    [1, 'x'],\n    [2, 'y'],\n    [3, 'z']\n]\ndata2 = [\n    [2, 'a'],\n    [3, 'b'],\n    [4, 'z']\n]\n\n# two dataframes with common columns\ndf1 = pd.DataFrame(\n    data = data1,\n    columns = ['A', 'B'],\n    index = [0, 1, 2]\n)\ndf2 = pd.DataFrame(\n    data = data2,\n    columns = ['A', 'B'],\n    index = [3, 4, 5]\n)\n\n# concatenate vertically (common columns, distinct index)\ndf = pd.concat([df1, df2], axis = 0)\ndisplay(df1)\ndisplay(df2)\nprint(\"vertical concatenation\")\ndisplay(df)     # (1)\n\n# two dataframes with common columns\ndf1 = pd.DataFrame(\n    data = data1,\n    columns = ['A', 'B'],\n    index = [0, 1, 2]\n)\n\ndf2 = pd.DataFrame(\n    data = data2,\n    columns = ['C', 'D'],\n    index = [0, 1, 2]\n)\n\n# concatenate horizontally (common index, distinct columns)\ndf = pd.concat([df1, df2], axis = 1)\ndisplay(df1)\ndisplay(df2)\nprint(\"horizontal concatenation\")\ndisplay(df)     # (2)\n</code></pre> <ol> <li> <p>df1</p> index A B 0 1 x 1 2 y 2 3 z <p>df2</p> index A B 3 2 a 4 3 b 5 4 z <p>result into </p> index A B 0 1 x 1 2 y 2 3 z 3 2 a 4 3 b 5 4 z </li> <li> <p>df1</p> index A B 0 1 x 1 2 y 2 3 z <p>df2 </p> index C D 0 2 a 1 3 b 2 4 z <p>result into </p> index A B C D 0 1 x 2 a 1 2 y 3 b 2 3 z 4 z </li> </ol> <p>When to concat and when better not?</p> <p>Even if <code>concat</code> handles mismatched in index and columns, I strongly advise to limit the use <code>concat</code> in the following situations</p> <ul> <li>Vertical concatenation (<code>axis=0</code>): <ul> <li>the index of each dataframe are disjoint</li> <li>the columns of each dataframe are the same</li> </ul> </li> <li>horizontal concatenation (<code>axis=1</code>):<ul> <li>the index of each dataframe are the same</li> <li>the columns of each dataframe are disjoint</li> </ul> </li> </ul> <p>For other situations, the <code>merge</code> functionality is better though it handles only two dataframes at the same time.</p>"},{"location":"lecture/03-Data/033-pandas-manipulations/#merging","title":"Merging","text":"<p>Given two dataframes <code>left_df</code> and <code>right_df</code>, the <code>merge</code> operation will provide a <code>SQL</code> type of merging between these two.</p> <p>Though it can be done on indexed dataframes, it is easier to get on non indexed ones (use the <code>reset_index()</code> to get the dataframes to be merged correctly).</p> <p>The basic operation is <code>pd.merge(left_df, right_df, on = ..., how = ...)</code></p> <p>Where</p> <ul> <li><code>on</code>: is the common column name (or a list thereof) in the two dataframes.</li> <li> <p><code>how</code>: is the method how the two dataframes are combined according to <code>on='col'</code> where <code>'col'</code> is the common column name in each dataframe.</p> <ul> <li><code>left</code>: use the left dataframe <code>col</code>. That is, for each row in the left dataframe, it concatenate with the row from the right dataframe if the <code>col</code> value is identical, otherwize it fill the content with <code>nan</code>.</li> <li><code>right</code>: use the right dataframe <code>col</code> (same as swapping left and right) in the previous point.</li> <li><code>inner</code>: Use the intersection of values in left and right <code>col</code>.</li> <li><code>outer</code>: use the union of the values in left and right <code>col</code></li> </ul> </li> </ul> <pre><code>data1 = [\n    [1, 'x'],\n    [2, 'y'],\n    [3, 'z']\n]\ndata2 = [\n    [2, 23.5],\n    [3, 11.2],\n    [4, 6.0]\n]\n\n# two dataframe without specified index\n# they share the sam column name `ints` #(1)\nleft_df = pd.DataFrame(\n    data = data1,\n    columns = ['ints', 'letters']\n)\nright_df = pd.DataFrame(\n    data = data2,\n    columns = ['ints', 'floats']\n)\n\n# left/right/inner/outer merging\nl_join = pd.merge(left_df, right_df, on='ints', how = 'left')\nr_join = pd.merge(left_df, right_df, on='ints', how = 'right')\ni_join = pd.merge(left_df, right_df, on='ints', how = 'inner')\no_join = pd.merge(left_df, right_df, on='ints', how = 'outer')\n\n\ndisplay(l_join)  # (2)\ndisplay(r_join)  # (3)\ndisplay(i_join)  # (4)\ndisplay(o_join)  # (5)\n</code></pre> <ol> <li> <p>left_df</p> ints letters 1 x 2 y 3 z <p>right_df</p> ints floats 2 23.5 3 11.2 4 6.0 </li> <li> <p>left join results into</p> ints letters floats 1 x <code>NaN</code> 2 y 23.5 3 z 11.2 </li> <li> <p>right join results into</p> ints letters floats 2 y 23.5 3 z 11.2 4 <code>NaN</code> 6.0 </li> <li> <p>inner join results into</p> ints letters floats 2 y 23.5 3 z 11.2 </li> <li> <p>outer join results into</p> ints letters floats 1 x <code>NaN</code> 2 y 23.5 3 z 11.2 4 <code>NaN</code> 6.0 </li> </ol> <p>There many more options</p> <ul> <li>the <code>on</code> can be a list of columns can be passed, on which each tuple of this list is the key for the merging operation.</li> <li>If the names of the columns in the two dataframes are different you can use <code>left_on = 'some_col</code> with <code>right_on='other_col'</code></li> <li>If you want to merge on index rather than some col, you can use <code>left_index = True</code> and/or <code>right_index = True</code></li> <li>Columns outside of the merge condition that share the same name will be appended with a suffix <code>_x</code> and <code>_y</code> in the resulting dataframe.</li> </ul>"},{"location":"lecture/03-Data/033-pandas-manipulations/#pivot","title":"Pivot","text":"<p>The function <code>pivot</code> allows to transform a dataframe along a new column and index. As illustration, let us consider the simple dataframe without specific index.</p> Item CType USD EUR Item0 Gold 1.2$ 1\u20ac Item0 Bronze 2.4$ 2\u20ac Item1 Gold 3.6$ 3\u20ac Item1 Silver 4.8$ 4\u20ac <pre><code># declare the data\ndata = np.array(\n    [\n        [\"Item0\", \"Item0\", \"Item1\", \"Item1\"],\n        [\"Gold\", \"Bronze\", \"Gold\", \"Silver\"],\n        [\"1.2$\", \"2.4$\", \"3.6$\", \"4.8$\"],\n        [\"1\u20ac\", \"2\u20ac\", \"3\u20ac\", \"4\u20ac\"],\n    ]\n)\n# create the dataframe\ndf = pd.DataFrame(data=data)\n# the dataframe is not in the right direction, we transpose\ndf = df.T\n# set the column names\ndf.columns = [\"Item\", \"CType\", \"USD\", \"EU\"]\ndisplay(df)\n</code></pre> <p>We want to pivot the table to get <code>Item</code> as index and <code>CType</code> as columns with values in $ as data. <pre><code>p = df.pivot(index=\"Item\", columns=\"CType\", values=\"USD\")\n\ndisplay(p) # (1)\n</code></pre></p> <ol> <li> <p>Results into</p> Items Gold Bronze Silver Item0 1.2$ 2.4$ <code>NaN</code> Item1 3.6$ <code>NaN</code> 4.8$ <p>Note that pivoting that way remove the informations about EU</p> </li> </ol> <p>We want to pivot the table to get <code>Item</code> as index and <code>CType</code> as columns but with all the values ($ and E). Clearly this is not possible in a plain tabular format so that it will create a multilevel index for the columns</p> <pre><code>p = df.pivot(index=\"Item\", columns=\"CType\")\n\ndisplay(p) # (1)\n</code></pre> <ol> <li> <p>Results into</p> Items Gold Bronze Silver USD EU USD EU USD EU Item0 1.2$ 1\u20ac 2.4$ 2\u20ac NaN NaN Item1 3.6$ 3\u20ac NaN NaN 4.8$ 4\u20ac </li> </ol>"},{"location":"lecture/03-Data/033-pandas-manipulations/#groupby","title":"Groupby","text":"<p>The <code>groupby</code> functionality is another <code>SQL</code> type functionality to perform computation based on a partitioning of the data.</p> <p>The functionality can be entailed into three steps</p> <ul> <li>Partition: split the dataframe into a partition of sub dataframes</li> <li>Apply: Apply a function on each of these partitions</li> <li>Combine: return the results of the function on each element of this partition</li> </ul> <pre><code>import pandas as pd\n\ndata = [\n    [\"Math\", \"Master\", 87],\n    [\"Math\", \"Master\", 76],\n    [\"CS\", \"PhD\", 91],\n    [\"Physics\", \"Master\", 84],\n    [\"Math\", \"PhD\", 96],\n    [\"CS\", \"Master\", 72],\n    [\"CS\", \"Master\", 81],\n    [\"Math\", \"PhD\", 98],\n    [\"Physics\", \"PhD\", 87],\n    [\"Physics\", \"PhD\", 85],\n    [\"Physics\", \"Master\", 45],\n]\ncols = [\"Dept\", \"Level\", \"Grade\"]\ndf = pd.DataFrame(data = data, columns = cols)\ndisplay(df) # (1)\n\n# Computation of average grade\n# per dept\navg_dept = df.groupby('Dept')['Grade'].mean()\n# per level\navg_level = df.groupby('Level')['Grade'].mean()\n# per dept and level (multiindex dept/level)\navg_dept_level = df.groupby(['Dept', 'Level'])['Grade'].mean()\n\ndisplay(avg_dept)       # (2)\ndisplay(avg_level)      # (3)\ndisplay(avg_dept_level) # (4)\n</code></pre> <ol> <li> <p>Return</p> Dept Level Grade 0 Math Master 87 1 Math Master 76 2 CS PhD 91 3 Physics Master 84 4 Math PhD 96 5 CS Master 72 6 CS Master 81 7 Math PhD 98 8 Physics PhD 87 9 Physics PhD 85 10 Physics Master 45 </li> <li> <p>Average per Dept</p> Dept Grade CS 81.3333 Math 89.25 Physics 75.25 </li> <li> <p>Average per Level</p> Level Grade Master 74.1667 PhD 91.4 </li> <li> <p>Average per Dept and Level</p> Grade CS Master 76.5 PhD 91 Math Master 81.5 PhD 97 Physics Master 64.5 PhD 86 </li> </ol> <p>In this simple example, we performed the full chain partition (<code>groupby</code>), apply (<code>mean</code> to column 'Grade').</p> <p>The <code>groupby</code> performs the partition, it returns an iterator running through each sub dataframe from that partition. (1)</p> <ol> <li>More precisely, the result of <code>groupby</code> is a dictionary with <code>key</code> being the unique element along which it is partitioned and <code>value</code> being the corresponding sub dataframe for that key.</li> </ol> <pre><code>dept_grp = df.groupby('Dept')\n\nfor name, subdf in dept_grp:\n    print(f\"Name:\\t{name}\")\n    print(\"with sub dataframe as content:\")\n    print(subdf)\n</code></pre> <p>In the main example we just performed one standard function one one of the columns of the groupby. It is possible to apply</p> <ul> <li>user defined functions (do not invent the wheel, if the function is already defined in pandas use it)</li> <li>several functions</li> <li>a single function that treats everything.</li> </ul> <pre><code># take a numpy array and return the max - min\ndef maxmin(x):\n    return x.max() - x.min()\n\nresult1 = df.groupby('Dept')['Grade'].agg(maxmin).rename(columns = {'Grade': 'Maxmin'})\ndisplay(result1)        # (1)\n\n# apply different functions to different columns and return the result\nresult2 = df.groupby('Dept').agg(\n    Counting = ('Level', 'count'),\n    Average = ('Grade', 'mean'),\n    Std = ('Grade', 'std'),\n    Maxmin = ('Grade', maxmin)\n)\nprint(result2)          # (2)\n\n# define a single function that will apply to each subdataframes\n\ndef treatment(dftmp):\n    # the Function will get as input each subdataframes\n    count = dftmp.size\n    mean = dftmp['Grade'].mean()\n    std = dftmp['Grade'].std()\n    mixing_twocols = dftmp['Level'].iloc[0] + ' with grade ' + dftmp['Grade'].astype(str).iloc[-1]\n    # we return a pandas series with the results\n    return pd.Series(\n        {\n            'Count': count,\n            'Mean': mean,\n            'Std': std,\n            'Some stupid things': mixing_twocols\n        }\n    )\nresult3 = df.groupby('Dept').apply(treatment)\nprint(result3)          # (3)\n</code></pre> <ol> <li> <p>Returns</p> Dept MaxMin CS 19 Math 22 Physics 42 </li> <li> <p>Returns</p> Dept Counting Average Std Maxmin CS 3 81.3333 9.50438 19 Math 4 89.25 10.0457 22 Physics 4 75.25 20.2052 42 </li> <li> <p>Returns</p> Dept Count Mean Std Some stupid things CS 6 81.3333 9.50438 PhD with grade 81 Math 8 89.25 10.0457 Master with grade 98 Physics 8 75.25 20.2052 Master with grade 45 </li> </ol>"},{"location":"lecture/03-Data/034-datetime/","title":"Dealing with Date and Time.","text":"<p>Date and time concept are central in data analysis but also beyond as it is the underlying for the task scheduler from any computer. However, they are particularly difficult objects: On the one hand, as integers or float they are ordered. On the other hand, they have many particularities:</p> <ul> <li>The basis is not uniform: 60 for minutes and seconds, 24 for hours, 365/366 for years, 28/29/30/31 for months, etc.)</li> <li>What is the origin?</li> <li>What is the granularity: milliseconds, micro, nano?</li> <li>Different formats: <code>YYYY-MM-DD</code> vs <code>MM/DD/YYYY</code>, <code>13H35 23'</code> vs <code>13:30:23</code> usually expressed as strings.</li> <li>different time zones.</li> </ul> Time <p>Time is usually measured in the number of the smallest quantity (second/milliseconds/microseconds/nanoseconds) starting from an origin date. This is called the epoch time. On Unix machines (now more or less the standard everywhere), the origin is set to <code>1970-01-01 00:00</code> and from there any date is computed as a deviation in number of the smallest unit from this date. As for the smallest unit from this date, it depends very much on the operating system as well as the machine itself. Second is pretty standard and accepted on every machine and OS, but resolution up to the microsecond is also nowadays normal. For particular applications (finance, physics), nanosecond resoltion is required (in one nanosecond the light moves by about 30 cm...).</p> <p>Two concepts emerged:</p> <ul> <li><code>timedelta</code>: a difference between two times measured in number of the smallest time interval</li> <li><code>timestamp</code>: a time point on the axis measured as a deviation in terms of time delta from an origin. In the UNIX framework it is <code>1970-01-01 00:00</code></li> </ul>"},{"location":"lecture/03-Data/034-datetime/#datetime-library","title":"Datetime library","text":"<p>The standard library to work with date time objects in python is <code>datetime</code>. It provides a representation for</p> <ul> <li><code>datetime</code> objects</li> <li><code>timedelta</code> amount of time between two dates.</li> </ul> <p>Granulaity</p> <p>The <code>datetime</code> library as for now provides a resolution of microseconds. To check on your computer/os/python version you can use the function <code>timedelta.resolution</code> as well as <code>timedelta.max</code> and <code>timedelta.min</code> for the resolution, maximum and minimum.</p>"},{"location":"lecture/03-Data/034-datetime/#datetime","title":"Datetime","text":"<ul> <li>declare datetime directly</li> <li>datetime from string</li> <li>string from datetime</li> </ul> <pre><code># import from datetime library the relevant classes (the names are self explanatory)\nfrom datetime import date, time, datetime, timedelta\n\n# Current date according to your computer\nmydate_now = date.today()\nprint(f\"Today is {mydate_now} with type {type(mydate_now)}\")\n\n# Current datetime according to your computer\nmydatetime_now = datetime.now()\nprint(f\"Date and time now {mydatetime_now} with type {type(mydatetime_now)}\")\n\n# specify a date provinding year, month, day\nmydate = date(2023, 2, 28)\nprint(f\"Specified date: {mydate} with type {type(mydate)}\")\n\n# specify a time: hour, minute, second\nmytime = time(14, 30, 59)\n\nprint(f\"Specified time: {mytime} with type {type(mytime)}\")\n\n# Specify a datetime: year, month, day, hour, minute, second\nmydatetime = datetime(2024, 2, 29, 14, 30, 59)\nprint(f\"Specified datetime: {mydatetime} with type {type(mydatetime)}\")\n</code></pre> <p>Each date, time and datetime object has methods to retrieve the different elements of the object (the month, year, day, hour, minute, etc.)</p> <pre><code># get from a datetime\n# the year\n# the month\n# the day\n# the hour\n# the minute\n\nmydatetime = datetime(2024, 2, 29, 14, 30, 59)\nprint(\n    f\"\"\"\nFor the datetime:\\t{mydatetime}\nYear:\\t {mydatetime.year}\\t with type:\\t {type(mydatetime.year)}\nMonth:\\t {mydatetime.month}\\t with type:\\t {type(mydatetime.month)}\nDay:\\t {mydatetime.day}\\t with type:\\t {type(mydatetime.day)}\nHour:\\t {mydatetime.hour}\\t with type:\\t {type(mydatetime.hour)}\nMinute:\\t {mydatetime.minute}\\t with type:\\t {type(mydatetime.minute)}\n\"\"\"\n)\n</code></pre> <p>The datetime class is calendar aware in terms of basis:</p> <pre><code># 2024 is bisectile year\ndatetime_ok = datetime(2024, 2, 29, 15, 15)\nprint(datetime_ok)\n# 2023 is not\ntry:\n    date_not_ok = datetime(2023, 2, 29, 15, 15)\nexcept Exception as ex:\n    print(\"This does not work because:\")\n    print(ex)\n\ntry:\n    time_not_ok = datetime(2024, 2, 29, 15, 60)\nexcept Exception as ex:\n    print(\"This does not work because:\")\n    print(ex)\n</code></pre> <p>Usually, datetime are expressed in text files as strings, or datetime shall be returned as human readable strings. The <code>datetime</code> library provides methods</p> <ul> <li><code>strptime</code>: convert a string to datetime object</li> <li><code>strftime</code>: convert a datetime object to a string</li> </ul> <p>In order to convert, we must indicate how a string has to be understood in terms its elements. There is a standard for it however here is a table for the most used ones</p> Directive Meaning Example <code>%a</code> Weekday as locale\u2019s abbreviated name. Mon <code>%A</code> Weekday as locale\u2019s full name. Monday <code>%w</code> Weekday as a decimal number, where 0 is Sunday. 0 (Sunday) <code>%d</code> Day of the month as a zero-padded decimal. 30 <code>%b</code> Month as locale\u2019s abbreviated name. Sep <code>%B</code> Month as locale\u2019s full name. September <code>%m</code> Month as a zero-padded decimal number. 09 <code>%y</code> Year without century as a zero-padded decimal. 23 <code>%Y</code> Year with century as a decimal number. 2023 <code>%H</code> Hour (24-hour clock) as a zero-padded decimal. 14 <code>%I</code> Hour (12-hour clock) as a zero-padded decimal. 02 <code>%p</code> Locale\u2019s equivalent of either AM or PM. AM <code>%M</code> Minute as a zero-padded decimal number. 05 <code>%S</code> Second as a zero-padded decimal number. 05 <code>%f</code> Microsecond as a decimal number, zero-padded. 000000 <code>%z</code> UTC offset in the form +HHMM or -HHMM. -0400 <code>%Z</code> Time zone name. EST <code>%j</code> Day of the year as a zero-padded decimal number. 365 <code>%U</code> Week number of the year (Sunday as the first day). 52 <code>%W</code> Week number of the year (Monday as the first day). 52 <code>%c</code> Locale\u2019s appropriate date and time representation. Tue Sep 30 14:05:02 2023 <code>%x</code> Locale\u2019s appropriate date representation. 09/30/23 <code>%X</code> Locale\u2019s appropriate time representation. 14:05:02 <pre><code># from string to datetime The % indicates the key holders\nstring_date01 = '2021-01-05 17H45'\nstring_date02 = 'Thu May 9 15:49 2024'\n\ndatetime01 = datetime(2021, 1, 5, 17, 45)\ndatetime02 = datetime(2024, 5, 9, 15, 49)\n\n# convert string to datetime\nmy_datetime01 = datetime.strptime(string_date01, \"%Y-%m-%d %HH%M\")\nmy_datetime02 = datetime.strptime(string_date02, \"%a %b %d %H:%M %Y\")\n\nprint(f\"\"\"\nThe string time:\\t {string_date01}\nInterpreted as:\\t\\t {my_datetime01}\\t correctly:\\t {my_datetime01 == datetime01}\n---------\nThe string time:\\t {string_date02}\nInterpreted as:\\t\\t {my_datetime02}\\t correctly:\\t {my_datetime02 == datetime02}\n\"\"\")\n\nmy_stringdate01 = datetime01.strftime(\"%Y-%m-%d %HH%M\")\nmy_stringdate02 = datetime02.strftime(\"%a %b %d %H:%M %Y\")\n\nprint(f\"\"\"\nThe datetime:\\t {datetime01}\nInterpreted as:\\t {my_stringdate01}\\t correctly:\\t {my_stringdate01 == string_date01}\n---------\nThe datetime:\\t {datetime02}\nInterpreted as:\\t {my_stringdate02}\\t correctly?:\\t {my_stringdate02 == string_date02}\n\"\"\")\n# not that my_stringdate02 is not equal to string_date02, why?\n</code></pre>"},{"location":"lecture/03-Data/034-datetime/#timedelta","title":"Timedelta","text":"<p>A <code>timedelta</code> represents the duration between two dates, that is \\(dt = t_2 - t_1\\). As expected \\(t_1+t_2\\) is not well defined, however \\(t_1 + dt = t_2\\).</p> <pre><code>t1 = datetime.strptime(\"2024-02-28 14:30:59\", \"%Y-%m-%d %H:%M:%S\")\nt2 = datetime.strptime(\"2024-02-29 15:35:49\", \"%Y-%m-%d %H:%M:%S\")\n\ndt1 = t2 - t1\ndt2 = t1 - t2\n\n\nprint(f\"The time difference between {t1} and {t2} is {dt1}\")\nprint(f\"The time difference between {t2} and {t1} is {dt2}\")\nprint(f\"The sum of deltas is {dt1 + dt2}\")\n\n\n# Time stamp can be defined as the difference between two times.\n# however it can be defined directly\n\nt1 = datetime(2024, 2, 28, 14, 30, 59)\ndt = timedelta(days=2)\n\nprint(\n    f\"\"\"\nOriginal date: {t1}\nTime delta: {dt}\nResulting date: {t1 + dt}\nTwo time delta: {2 * dt}\nResulting date: {t1 + 2*dt}\n\"\"\"\n)\n\n# Simple operations with time delta (day/hour/minutes/seconds/milliseconds/microseconds)\ndt1 = timedelta(hours = 1)\ndt2 = timedelta(seconds = 3601)\ndt3 = timedelta(hours = 1, seconds = 3601)\n\nprint(f\"\"\"\n{dt1} plus {dt2} is {dt1 + dt2} which clearly coincides with {dt3}\n\"\"\")\n</code></pre>"},{"location":"lecture/03-Data/034-datetime/#pandas-timestamps","title":"Pandas Timestamps","text":"<p>Pandas as well as Numpy also handle datetimes as a type with slight differences. It is usually better to work directly with pandas datetimes since it handles more complex operations with datetimes (resampling, arithmetics on arrays of datetime as well as empty values). There is a good introduction about it on the pandas webpage.</p> <p>By default Pandas datetime are nanosecond resolution.</p> <p>The basic functionalities can be summarized as follows</p> Concept Class Name Datatype Creation Datetime <code>Timestamp</code> <code>datetime64[ns]</code> <code>to_datetime</code> or <code>date_range</code> Timedelta <code>Timedelta</code> <code>timedelta64[ns]</code> <code>to_timedelta</code> or <code>timedelta_range</code> <pre><code>import pandas as pd\n\n# Declaring a time can be done from a string (pandas will infer the format if not crazy)\nmydatetime = pd.to_datetime('2021-12-01 12:30:25.0003')\nprint(f\"\"\"Pandas datetime {mydatetime} with type {type(mydatetime)}\"\"\")\n\n# same for timedelta\nmytimedelta = pd.to_timedelta('2 days 12 hours 30 minutes 25 seconds 3 nanoseconds')\nprint(f\"\"\"Pandas Timedelta {mytimedelta} with type {type(mytimedelta)}\"\"\")\n</code></pre> <p>The functionalities are identical to datetime previously.</p>"},{"location":"lecture/03-Data/034-datetime/#datetimetimedelta-index","title":"Datetime/Timedelta Index","text":"<p>The place where pandas shines is when it handles datetimes as arrays.</p> <ul> <li>Apply <code>to_datetime</code> to an array</li> <li>Create a range of datetimes <code>date_range</code> (see offset aliases)</li> </ul> <pre><code>import pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\nt_idx = pd.to_datetime(['2021-01-01', np.nan, datetime(2021, 1, 2)])\ndisplay(t_idx)\n\nt_idx = pd.date_range(start = '2021-01-01', end = '2021-01-07', freq = 'D')\ndisplay(t_idx)\n\n# business day frequency \nt_idx = pd.date_range(start = '2021-01-01', periods = 7 , freq = 'B')\ndisplay(t_idx)\n\n# every half second \nt_idx = pd.date_range(start = '2021-01-01 09:30:00', periods = 7 , freq = '500 ms')\ndisplay(t_idx)\n</code></pre> <p>The same hold for time deltas </p> <pre><code>import pandas as pd\n\ndt_idx = pd.to_timedelta(['3 days 4 hours', np.nan, timedelta(days = 3, hours = 1, minutes = 2)])\ndisplay(dt_idx)\n\ndt_idx = pd.timedelta_range(start = '1 day', periods = 4)\ndisplay(dt_idx)\n\ndt_idx = pd.timedelta_range(start = '1 day', end = '2 days', freq = '6h')\ndisplay(dt_idx)\n</code></pre> <p>You can perfom operations between those index as for the scalars as well as perform axis operations (<code>shift</code>, <code>diff</code>).</p> <pre><code>t_idx = pd.date_range(start = '2021-01-01 09:30:00', periods = 3 , freq = '1h')\ndt_idx = pd.timedelta_range(start = '1 hour', periods = 3, freq = '2h')\n\nprint(f\"\"\"\nOriginal dates:\n{t_idx}\nOriginal timedelta:\n{dt_idx}\n--------\n\nShifting dates by 5 days:\n{t_idx.shift(10, freq = 'D')}\n\nShifting timedelta by 2 hours:\n{dt_idx.shift(2, freq = 'h')}\n\nTimedelta from dates:\n{dt_idx.diff(1)}\n\nDates plus timedelta:\n{t_idx + dt_idx}\n\"\"\")\n</code></pre>"},{"location":"lecture/03-Data/034-datetime/#groupby-rolling-resampling","title":"Groupby, Rolling, Resampling","text":"<p>We already saw that <code>groupby</code> allows to partition a dataframe along the unique values of a given column (or an index) while <code>rolling</code> provides a rolling window on the dataframe. These can be combined with date time functionalities (for instance grouping hourly dates into days, or rolling by the hours).</p> <p><code>resample</code> is however particular. One can distinguish between</p> <ul> <li>downsampling, which means partitioning a higher frequency in terms of time dataframe into a lower frequency (like <code>groupby</code> above)</li> <li>upsampling which provides an index at a higher frequency. In this case you have to explain how to fill the data in for the additional times.</li> </ul> <p>To illustrate these functionalities we create a dataframe with a datetime index <code>Time</code> and two columns <code>Temperature</code> and <code>Pressure</code>.</p> <pre><code>N = 1000\n# create a datetime index by 6 hours\nt_idx = pd.date_range(start = '2021-01-01', periods = N, freq = '15min')\n\ndf = pd.DataFrame(\n    data = {\n        'Temperature': np.random.normal(loc = 20, scale = 4, size = N),\n        'Pressure': np.random.normal(loc = 1000, scale = 50, size = N)\n    },\n    index = t_idx\n)\ndf.index.name = 'Time'\ndisplay(df)\n</code></pre> <p>Temperature and pressure are measured hourly, we want to get the daily average.</p> <pre><code># using groupby\n# here df.index are the timestamps\n# date is the accessor to the date part of the timestamp\nresult = df.groupby(by = df.index.date).mean()\nprint(\"Result using groupby\")\ndisplay(result)\n\n# performing the same using resample\nresult = df.resample('D').mean()\nprint(\"Result using resample\")\ndisplay(result)\n</code></pre> <p>As for the rolling functionality, we want to get the 6 hours rolling average of the temperature and pressure. In other terms, every 15 minutes we want the last 6 hours average of each.</p> <pre><code>result = df.rolling('6h').mean()\nprint(result)\n</code></pre> Rolling minimum interval <p>If you apply rolling functionalities on a dataframe with an integer (lets say 7), then by default, it will only start when it has 7 elements in the window. In the case of time rolling with a frequency indicated, it will consider anything within this time window. In this example, it starts at the datetime for which it has one element, then proceed to the next hour where it has two, then to the third where it has 3 until it reaches 6 hours where it has 6 elements, and will go ahead with only 6.</p>"},{"location":"lecture/03-Data/035-sql/","title":"Databases and SQL","text":"<p>Pandas is a handy and advanced way to handle data. However, storing, organizing and querying data has a long history related to databases. There are many ways to consider data as objects in a computer and in a broad sense they can be distinguished between two categories:</p> <ul> <li>SQL: tabular data with predefined structure (collection of tuples with predefined columns and types).     For instance collection of student names (string), student id (int) and grades (float).</li> <li>NoSQL: As the name indicates, it is not of the previous type. Those datasets are of variable length, content. They can be thought as dictionaries with key values inputs.</li> </ul> <p>In the following we will mainly introduce the concept of SQL databases.</p> What SQL stands for? <p>The three letters SQL stands for standardized query language. It has a long history dating back to the 70ies and is based on tuple calculus. It has been standardized and the language itself is, modulo some minor modifications, similar across all platforms and database provider. </p>"},{"location":"lecture/03-Data/035-sql/#tuple-calculus","title":"Tuple Calculus","text":"<p>The foundation of SQL language is based on tuple calculus that is a handy representation in set theoretical sense. We won't enter into the full details of it, we present however a succinct overview how it works.</p> <p>In this context, we consider two sets of data:</p> <ul> <li>\\(A\\subseteq X \\times Y\\);</li> <li>\\(B\\subseteq X \\times Z\\);</li> </ul> <p>that is collections of tuples in the sets \\(X\\), \\(Y\\) and \\(Z\\). The generic elements of those sets are labelled as \\(x\\), \\(y\\) and \\(z\\), respectively.</p> <p>For instance:</p> <ul> <li>\\(X= \\mathbb{N}\\) is a set of students id</li> <li>\\(Y\\) is the set of all possible names</li> <li>\\(Z = \\mathbb{R}\\) is the set of grades in the lecture </li> </ul> <p>In this case:</p> <ul> <li>\\(A\\) is the set of tuples students id and corresponding names of SJTU Bachelor students</li> <li>\\(B\\) is the set of tuples students id and corresponding grades of those students that took the lecture Advanced programming SS2024</li> </ul> <p>If you want to get the set of all students id, names and grades in the lecture for those students that took the lecture, you look at the following set</p> \\[ \\begin{equation} \\left\\{(x, y, z)\\colon (x, y) \\in A \\text{ and }(x, z) \\in B\\right\\}\\subseteq X \\times Y \\times Z \\end{equation} \\] <p>If you just want the name and grade you want</p> \\[ \\begin{equation} \\left\\{(y, z)\\colon (x, y) \\in A \\text{ and }(x, z) \\in B\\right\\}\\subseteq Y \\times Z \\end{equation} \\] <p>These operation can be summarized as intersection/union between cartesian products and then projections. We define</p> \\[ \\begin{equation} \\begin{split}     \\sigma_{X} \\colon X \\times Y \\times Z &amp; \\longrightarrow X\\\\ (x, y, z) &amp;\\mapsto \\sigma_X(x, y, z) = x \\end{split} \\end{equation} \\] <p>which is the projection of \\(X\\times Y\\times Z\\) to \\(X\\) and similarly \\(\\sigma_{XY}\\), \\(\\sigma_{YZ}\\), etc. It follows that</p> \\[ \\begin{equation}     \\left\\{(x, y, z)\\colon (x, y) \\in A \\text{ and }(x, z) \\in B\\right\\} = \\sigma^{-1}_{XY}(A)\\cap \\sigma^{-1}_{XZ}(B) \\end{equation} \\] <p>and</p> \\[ \\begin{equation}     \\left\\{(x, y, z)\\colon (x, y) \\in A \\text{ and }(x, z) \\in B\\right\\} = \\sigma_{YZ}\\left(\\sigma^{-1}_{XY}(A)\\cap \\sigma^{-1}_{XZ}(B)\\right) \\end{equation} \\] <p>The intersection and projections are the easiest set operation on cartesian sets. Things gets more complicated when you want to get all the students id and names and if they are in table \\(B\\), then the grades of those students otherwize en empty value.</p> <p>The exact formalism of SQL language is based on relational algebra and is relatively complex in terms of definition and operations.</p> <p>However, we present here after the basics using SQLite. As the name indicates, it is a very portable sql database stored in a single text file and is open sourced. It implements most of the SQL functionalities and is very efficient. You can start using sqlite by just installing a database query program such as Beekeeper Studio Community Edition which is open sourced.</p>"},{"location":"lecture/03-Data/035-sql/#sql-language","title":"SQL Language","text":""},{"location":"lecture/03-Data/035-sql/#create-tables","title":"Create Tables","text":"<p>A database is a collection of tables.</p> <p>Three main actions can be used:</p> <ul> <li><code>CREATE</code>: create a new table in the database</li> <li><code>DROP</code>: drop an existing table</li> <li><code>ALTER</code>: modify the structure of a table</li> </ul> <p>A table is declared as a list of columns labels and a type (like a panda dataframe). The available datatypes in SQL are very limited:</p> Name Type NULL Null value standing for no data INT Integers REAL Floats TEXT Strings BLOB Pieces of data in binary form <p>The SQL Commands are usually in capital letters (but do not have to). The command to create a table is as follows</p> <pre><code>--- The dashes are condidered as escaped characters like the # for python\nCREATE TABLE students (\n    id INT PRIMARY KEY,\n    name TEXT NOT NULL,\n    lev TEXT NOT NULL,\n    email TEXT\n);\n--- A command is terminated with a semi column ;\n</code></pre> <p>This code will create a table students with the columns</p> <ul> <li><code>id</code>: represent the student id. It is an integer and we say that it is a primary key in the sense that the value will be unique.</li> <li><code>name</code>: name of the student is text. We precise that the value shall not be empty</li> <li><code>lev</code>: level of the student (Bachelor, Master, PhD). Shall not be null</li> <li><code>email</code>: email of the student</li> </ul> <p>Be careful with the following command, it deletes the table with all the data inside. <pre><code>DROP TABLE students;\n</code></pre></p> <p>If you deleted the table create it again.</p> <pre><code>--- Will rename the table students to student\nALTER TABLE students RENAME TO student;\n\n--- revert to the original name\nALTER TABLE student RENAME TO students;\n\n--- Will add a new column age\nALTER TABLE students ADD COLUMN age INT;\n</code></pre>"},{"location":"lecture/03-Data/035-sql/#data","title":"Data","text":"<p>Now that we have our students table, we can do the following</p> <ul> <li><code>INSERT</code>: insert data in the table</li> <li><code>UPDATE</code>: update data in the table</li> <li><code>DELETE</code>: delete data in the table</li> <li><code>SELECT</code>: query data in the table</li> </ul> <p>The last one is the most important so we will see it in a separate section. However let us have a look at the most simple select function that will show the current content of the table.</p> <pre><code>--- Show the content of the table students\nSELECT * FROM students;\n</code></pre> <p>Delete and recreate the table students.</p>"},{"location":"lecture/03-Data/035-sql/#insert","title":"Insert","text":"<pre><code>--- We insert one student in the table\nINSERT INTO students (id, name, lev, email)\nVALUES (1, 'WANG Xin', 'Master', 'wch@some_address.cn');\n\n--- We insert several students in the table\nINSERT INTO students (id, name, lev, email)\nVALUES \n    (2, 'LI Xuan', 'Master', 'lxu@some_address.cn'),\n    (3, 'DRAPEAU Samuel', 'Bachelor', 'sd@some_address.cn'),\n    (4, 'TURGEDIEV Maximilian', 'PhD', 'tm@some_address.cn'),\n    (5, 'FISCHER Johannes', 'Bachelor', 'fj@some_address.cn');\n\n--- Check the content of the table\nSELECT * FROM students;\n</code></pre>"},{"location":"lecture/03-Data/035-sql/#update-or-delete","title":"Update or Delete","text":"<pre><code>--- We update Samuel from Bachelor to Master and change its email address\nUPDATE students\nSET lev = 'Master', email = 'sdrapeau@some_address.cn'     --- set the values for the columns to be updated\nWHERE id = 3;                                              --- select the rows to be updated\n\n--- We delete FISHER Johannes since he left\nDELETE FROM students\nWHERE name = 'FISCHER Johannes';\n\n--- Check the content of the table\nSELECT * FROM students;\n</code></pre>"},{"location":"lecture/03-Data/035-sql/#query","title":"Query","text":""},{"location":"lecture/03-Data/035-sql/#simple","title":"Simple","text":"<p>The basic functionality to query data is the <code>SELECT</code> statement. It can become very complicated but in a basic form it is of the kind </p> <p><code>SELECT &lt;what&gt; FROM &lt;which_table&gt; WHERE &lt;some_constraints&gt;</code></p> <p>In the following code you have to run each statement independently to see the result. <pre><code>--- query everything (* stands for all)\nSELECT * FROM students;\n\n--- query only name and level (indentation is easier to read)\nSELECT\n    name,\n    lev\nFROM\n    students;\n\n--- query only name and level and rename the column names\nSELECT\n    name as Name,\n    lev as Level\nFROM\n    students;\n\n--- Query id, name, mail for all the master students\nSELECT\n    id as StudentID,\n    name as Name,\n    email as Email\nFROM\n    students\nWHERE                       --- here we put the condition clause\n    lev = 'Master';\n</code></pre></p>"},{"location":"lecture/03-Data/035-sql/#join-tables","title":"Join Tables","text":"<p>The interest of SQL is to orgainzed data in more than tabular (2d form) format. Those tables can then be logically connected in the following way</p> <p>Let us consider the following example with 4 tables.</p> <ol> <li><code>students</code>: with columns <code>student_id</code>, <code>name</code></li> <li><code>informations</code>: with columns <code>student_id</code>, <code>email</code>, <code>phone</code>, <code>lev</code></li> <li><code>lectures</code>: with columns <code>lecture_id</code>, <code>lecture_name</code></li> <li><code>grades</code>: with columns <code>lecture_id</code>, <code>student_id</code>, <code>grade</code></li> </ol> <p>The relation between tables can be of different nature</p> <ul> <li>1 -&gt; 1: one record in table \\(A\\) is uniquely mapped to a record in table \\(B\\).     For instance <code>students</code> has a unique corresponding record in <code>informations</code> with <code>student_id</code> linking both.</li> <li>1 -&gt; n: one record in table \\(C\\) can be mapped to different records in table \\(D\\).     For instance a record in <code>lectures</code> can have several grades records in <code>grades</code>.      However a single grade record in <code>grades</code> can only be mapped to a single record in <code>lectures</code>.</li> <li>n -&gt; n: different record in table \\(A\\) are mapped to different records in table \\(C\\).     In order to do so you need a so called intermediary table to realize n-&gt;1 and 1-&gt;n.     For example, different students can attend different lectures.</li> </ul> <p>Note</p> <p>Note that the exact 1&lt;-&gt;1 is redundant and could be put into a single table, eventually with <code>NULL</code> values for missing informations if it is just a 1-&gt;1</p> <p>Let us generate different tables for the previous example</p> <pre><code>CREATE TABLE students (\n    student_id INT PRIMARY KEY,\n    name TEXT NOT NULL,\n    lev TEXT NOT NULL,\n    email TEXT\n);\nCREATE TABLE lectures (\n    lecture_id INT PRIMARY KEY,\n    name TEXT\n);\nCREATE TABLE grades (\n    lecture_id INT,\n    student_id INT,\n    grade INT\n);\n\n\nINSERT INTO students (student_id, name, lev, email)\nVALUES\n    (1, 'WANG Xin', 'Master', 'wch@some_address.cn'),\n    (2, 'LI Xuan', 'Master', 'lxu@some_address.cn'),\n    (3, 'DRAPEAU Samuel', 'Bachelor', 'sd@some_address.cn'),\n    (4, 'TURGEDIEV Maximilian', 'PhD', 'tm@some_address.cn'),\n    (5, 'FISCHER Johannes', 'Bachelor', 'fj@some_address.cn'),\n    (6, 'XYZ Theo', 'Bachelor', 'xyz@some_address.cn');\nINSERT INTO lectures (lecture_id, name)\nVALUES\n    (1, 'Stochastic Processes'),\n    (2, 'Advanced Programming');\n\nINSERT INTO grades (lecture_id, student_id, grade)\nVALUES\n    (1, 1, 78),\n    (1, 3, 87),\n    (1, 4, 56),\n    (1, 5, 98),\n    (2, 1, 97),\n    (2, 2, 93),\n    (2, 4, 84);\n--- Note that Li Xuan did not follow any lectures\n\n--- Run the following statement one after the other to check each table\n\nSELECT * FROM students;\nSELECT * FROM lectures;\nSELECT * FROM grades;\n</code></pre> <p>There are many different kind of join (how to combine data from different tables with common content).</p> <ul> <li><code>INNER JOIN</code>: (has default when <code>JOIN</code> is called). It takes the intersection on both tables on the condition.</li> <li><code>LEFT JOIN</code>: takes all the keys from the left table and add the data from the right table if the condition is met otherwize it fills with <code>NULL</code> data.</li> <li><code>RIGHT JOIN</code>: Same as left join but on the right table condition.</li> <li><code>FULL JOIN</code>: generate the union of the rows in each (basically union of left and right join).</li> <li><code>CROSS JOIN</code>: generate the cartesian product of the two tables. To be prevented since it might get very large and is not that useful.</li> </ul> <p>We just illustrate here inner and left join:</p> <ul> <li>Inner Join:</li> </ul> <pre><code>--- We want the student_id, grade and lecture name\nSELECT \n    grades.student_id as student_ID,\n    grades.grade as Grade,\n    lectures.name as LectureName\nFROM\n    grades\n    JOIN lectures ON grades.lecture_id = lectures.lecture_id;\n</code></pre> <ul> <li>Left Join:</li> </ul> <pre><code>--- We want all the students name who are Master\n--- if they have lectures we show lecture_id and grade otherwize none\nSELECT \n    students.name as Name,\n    students.lev as Level,\n    grades.lecture_id,\n    grades.grade as Grade\nFROM\n    students\n    LEFT JOIN grades ON students.student_id = grades.student_id\nWHERE\n    students.lev = 'Master';\n</code></pre>"},{"location":"lecture/03-Data/035-sql/#perform-computationsaggregations","title":"Perform Computations/Aggregations","text":"<p>As for Pandas, data can be aggregated in order to perform computations. As for pandas, the keyword is <code>GROUPBY</code>.</p> <p>For instance, get for each lecture name the average grade, maximum and minimum. We need to join with the table <code>lectures</code> in order to get the name and will aggregate on it</p> <pre><code>SELECT\n    lectures.name as LectureName,\n    MEAN(grades.grade) as mean,\n    MAX(grades.grade) as max,\n    MIN(grades.grade) as min\nFROM\n    grades\n    JOIN lectures ON grades.lecture_id = lectures.lecture_id\nGROUP BY\n    lectures.name;\n</code></pre> <p>We complicate a bit by getting the same results however detailed by level of the students. In this case we need two joins</p> <pre><code>SELECT\n    lectures.name as LectureName,\n    students.lev as Level,\n    MEAN(grades.grade) as mean,\n    MAX(grades.grade) as max,\n    MIN(grades.grade) as min\nFROM\n    grades\n    JOIN lectures ON grades.lecture_id = lectures.lecture_id\n    JOIN students ON grades.student_id = students.student_id\nGROUP BY\n    lectures.name, students.lev;\n</code></pre>"},{"location":"lecture/03-Data/035-sql/#sql-pandas","title":"SQL &amp; Pandas","text":"<p>Depending on your needs, you can consider SQL as a large container of many dataframes. Some computations can be performed directly in SQL but the main purpose it to query data directly from SQL and get them in pandas for further use.</p> <p>Any database has libraries that allows to connect to any programming language. In the case of SQLite the library is called <code>sqlite3</code> which can be installed with anaconda or using pip <code>pip install sqlite3</code>. With this API at hand, pandas can directly retrieve data from the database.</p> <p>In our example, we have a database <code>test.db3</code> stored in the folder <code>./data/</code> with a table <code>grades</code> as in the previous example.</p> <p>Read the table grades from the database</p> <pre><code>import pandas as pd\nimport sqlite3 \n\n# you first need to establish a connection to the database\ncon = sqlite3.connect('./data/test.db3')\n\n# write the query\nQUERY = \"\"\"\nSELECT \n    *\nFROM \n    students\nWHERE\n    students.lev = 'Master'\n\"\"\"\n\n# get the result of the query in the dataframe\ndf = pd.read_sql_query(QUERY, con)\n\ndisplay(df)\n</code></pre> <p>You can design complex queries against many tables to get the result in the dataframe. You can also just query the full table</p> <pre><code>import pandas as pd\nimport sqlite3 \n\n# connection to the database\ncon = sqlite3.connect('./data/test.db3')\n\n# get the full table grades in the dataframe\ndf = pd.read_sql_table('grades', con)\n\ndisplay(df)\n</code></pre>"},{"location":"lecture/03-Data/036-applications/","title":"Applications","text":"<p>As application involving data the folling three are quite basics.</p>"},{"location":"lecture/03-Data/036-applications/#principal-component-analysis","title":"Principal Component Analysis","text":""},{"location":"lecture/03-Data/036-applications/#clustering","title":"Clustering","text":""},{"location":"lecture/04-ML-101/041-LR/","title":"Linear Regression","text":"<p>Regression is a widely used way to decompose some output \\(Y\\) into a meaningful function \\(f\\) of inputs \\(\\mathbf{X}= (X_1 , \\ldots , X_d)\\) and some noise \\(\\varepsilon\\)</p> \\[ \\begin{equation*}     Y \\approx f(\\mathbf{X}) + \\varepsilon \\end{equation*} \\] <p>Ideally, \\(\\varepsilon\\) should be somehow \"orthogonal\" to \\(f(\\mathbf{X})\\). A natural way is to consider the \\(L^2\\) norm between \\(Y\\) and \\(f(\\mathbf{X})\\) as \\(L^2\\) is an Hilbert space and projections are easily understandable. In other terms, the best function \\(f^\\ast\\colon \\mathbb{R}^d \\to \\mathbb{R}\\) shall satisfy</p> \\[ \\begin{equation*}     f^\\ast = \\mathrm{argmin} \\left\\{E\\left[\\left(Y - f(\\mathbf{X}) \\right)^2\\right] \\colon f \\in \\mathcal{C}\\right\\} \\end{equation*} \\] <p>where \\(\\mathcal{C}\\) is a set of possible functions. If considering any possible function \\(f\\), the optimization problem is too large. Therefore, most regression approaches consider a smaller parametrized subset of functions of finite dimension. The most prominent example are the affine functions</p> \\[ \\begin{equation*}     \\mathcal{C} = \\left\\{f(\\mathbf{x})=a + \\mathbf{b}\\cdot \\mathbf{x}: (a, b) \\in \\mathbb{R}\\times \\mathbb{R}^d\\right\\} \\end{equation*} \\] <p>refered to as Ordinary Least Square regression.</p> Further Constraints or Specifications can be Considered <p>For additional robustness, further constraints are imposed on the set \\(\\mathcal{A}\\) of those \\((a, \\mathbf{b})\\) allowed in the linear regression, or fixed transformation of the input variables \\(\\mathbf{x}\\). But the problem is still linear or convex in \\((a, \\mathbf{b})\\).</p> <ul> <li> <p>Ridge Regression: intends to keep the possible factors into some constraint set in the \\(\\ell^2\\) sense</p> \\[ \\begin{equation*}     \\mathcal{A} = \\left\\{(a, \\mathbf{b})\\colon \\sum |b_k|^2 = \\|\\mathbf{b}\\|_2^2 \\leq \\lambda\\right\\} \\end{equation*} \\] <p>for a given smoothing factor \\(\\lambda\\). Convex constraint to prevent dispersion of the coefficients.</p> </li> <li> <p>Lasso Regression: same as Lasso but in terms of absolute value or \\(\\ell^1\\) sense, that is</p> \\[ \\begin{equation*}     \\mathcal{A} = \\left\\{(a, \\mathbf{b})\\colon \\sum |b_k| = \\|\\mathbf{b}\\|_1 \\leq \\lambda\\right\\} \\end{equation*} \\] <p>For a given smoothing factor \\(\\lambda\\).</p> </li> </ul> <p>Some mix of Lasso and Ridge yields to further regression types (elastic net, group lasso, fuse lasso, etc.).</p> <p>Sometimes the data output in clearly not linear in the input, but of polynomial order \\(n\\). If we define \\(\\bar{\\mathbf{x}}\\) as the vector of all monomial powers of \\(\\mathbf{x}\\) (that is the vector of those \\(\\prod_{i=1}^d x_i^{\\alpha_i}\\) where \\(\\sum \\alpha_i\\leq n\\)), then we can consider the polynomial regression where</p> \\[ \\begin{equation*}     \\left\\{f(\\mathbf{x}) = a + \\bar{b}\\cdot \\bar{x}\\right\\} \\end{equation*} \\] <p>which remains linear in the factors \\((a, \\bar{\\mathbf{x}})\\) even if we try to match \\(Y\\) as a polynomial function of the factor. Note that the number of factors increases very fast in that case, and can lead to a problem of over fitting.</p>"},{"location":"lecture/04-ML-101/041-LR/#some-math-preliminaries","title":"Some Math Preliminaries","text":"<p>Note</p> <p>For ease of notations and further computations (in the end \\(\\mathbf{b}\\) is a gradient), we assume that \\(\\mathbf{b}\\) is a column vector and write the affine function in terms of matrix calculus \\(a + \\mathbf{b}\\cdot \\mathbf{x} = a + \\mathbf{x}\\mathbf{b}\\).</p> <p>Furthermore, the affine problem can be transformed into a linear one by adding a dimension. Indeed, for \\(\\bar{\\mathbf{X}} =(1, \\mathbf{X})\\) and \\(\\bar{\\mathbf{b}} = [a,  \\mathbf{b}^\\top]^\\top\\) we have \\(a + \\mathbf{X}\\mathbf{b} = \\bar{\\mathbf{X}}\\bar{\\mathbf{b}}\\). Hence modulo adding a dimension, we just consider linear functions, and throughout we will always assume that the first entry of \\(\\mathbf{X}\\) is identically \\(1\\).</p> <p>In the OLS case, solving the minimization problem \\(\\mathbf{b}^\\ast = \\mathrm{argmin} \\{E[(Y - \\mathbf{b}\\cdot \\mathbf{X}) )^2] \\colon \\mathbf{b} \\in \\mathbb{R}^d\\}\\) is straightforward. The objective function is convex and smooth in \\(\\mathbf{b}\\) and uniformly bounded from below by \\(0\\). Hence, the global minimum if it exists is characterized by the gradient (column vector) of the objective function being equal to \\(0\\), that is</p> \\[ \\begin{equation*}     \\nabla_{\\mathbf{b}} E\\left[\\left(Y - \\mathbf{X}\\mathbf{b}\\right)^2\\right] = -2E\\left[ \\mathbf{X}^\\top\\left(Y - \\mathbf{X} \\mathbf{b} \\right)\\right] = 0 \\end{equation*} \\] <p>Proposition</p> <p>Given square integrable random variables \\(Y\\) and \\(\\mathbf{X}\\) (\\(d\\)-dimensional). Provided that the Gram matrix \\(\\mathbf{Q}:=E[\\mathbf{X}^\\top\\mathbf{X}]\\) (which is positive semidefinite) is invertible, it follows that</p> \\[ \\begin{equation*}     \\mathbf{b}^\\ast =E\\left[\\mathbf{X}^\\top\\mathbf{X}\\right]^{-1}E\\left[\\mathbf{X}^\\top Y\\right] \\quad \\text{and} \\quad \\mathbf{\\varepsilon}^\\ast := Y - \\mathbf{X}\\mathbf{b}^\\ast \\end{equation*} \\] <p>are the optimal OLS coefficient and error value, respectively. It holds</p> \\[ \\begin{equation*}     E\\left[ \\mathbf{X}^\\top \\varepsilon^\\ast \\right] =  E\\left[ X^\\top\\left(Y - \\mathbf{X} \\mathbf{b}^\\ast \\right)\\right]=0 \\end{equation*} \\] <p>In order to estimate the quality of the error, we consider the following</p> <ul> <li> <p>Residual Variance: variance of the residual error \\(\\varepsilon\\). (1)</p> <ol> <li>Since we extended the problem by one dimension for the linear regression, the mean of the error is equal to \\(0\\).</li> </ol> \\[ \\begin{equation*}     \\mathrm{RVAR} = \\mathrm{VAR}(\\varepsilon) = E\\left[ (Y - \\mathbf{X}\\mathbf{b}^\\ast)^2 \\right] \\end{equation*} \\] </li> <li> <p>Total Variance: variance of \\(Y\\)</p> \\[ \\begin{equation*}     \\mathrm{TVAR} = \\mathrm{VAR}(Y) = E[(Y - E[Y])^2] \\end{equation*} \\] </li> <li> <p>Coefficient of Determination: number between \\(0\\) and \\(1\\)</p> \\[ \\begin{equation*}     R^2 = 1 - \\frac{\\mathrm{RVAR}}{\\mathrm{TVAR}} = 1 - \\frac{\\mathrm{VAR}(\\varepsilon)}{\\mathrm{VAR}(Y)} \\end{equation*} \\] </li> </ul> <p>In particular if the approximation is perfect, then \\(\\mathrm{RVAR}\\) is equal to \\(0\\) and \\(R^2 = 1\\). If the approximation can only explain the mean, then \\(\\mathrm{RVAR} = \\mathrm{TVAR}\\) and \\(R^2 = 0\\).</p>"},{"location":"lecture/04-ML-101/041-LR/#from-random-variables-to-data-and-back","title":"From Random Variables to Data (and Back)","text":"<p>Random variables are idealized objects, while in reality we usually observe samples from those.</p> <p>What does it mathematically mean \"Sampling\"?</p> <p>Drawing \\(N\\) iid samples \\(x_1, \\ldots, x_N\\) from the distribution of a random variable \\(X\\) is as follows.</p> <ol> <li>Take \\(N\\) iid copies \\(X_1, \\ldots, X_N\\) of \\(X\\).</li> <li>Choose a state \\(\\omega\\) in \\(\\Omega\\).</li> <li>Define \\(x_1 = X_1(\\omega)\\), ...,\\(x_N = X_N(\\omega)\\)</li> </ol> <p>From this definition, a sample is itself a sequence of the evaluation in a given state \\(\\omega\\) of those identical and independent copies of \\(X\\). From now on we denote by \\(\\mathbf{x}^{(N)}\\) a sample of size \\(N\\) of the random variable and if we want to stress the dependence of the choice of \\(\\omega\\), we write \\(\\mathbf{x}^{(N)}(\\omega)\\).</p> <p>From a computer perspective, you fix a seed that you can think of \\(\\omega\\), and then generate the random sequence \\(\\mathbf{x}^{(N)} = \\mathbf{x}^{(N)}(\\omega) = (x_1,\\ldots, x_N)\\) which corresponds to \\(x_n = X_n(\\omega)\\).</p> <p><pre><code>import numpy as np\n\n# fix the number of samples\nN = 5\n\n# fix a seed omega\nomega = 150\nrng = np.random.default_rng(seed = omega)\n\n# Generate N random samples from a normal distribution with mean 0 and standard deviation 0.2\nx = rng.normal(1, 0.2, size = N)      # generate N independent copies of N(1, 0.2) evaluated at omega\nprint(f\"First draw with omega = {omega} result into:\\n{x}\")\n\n# draw again with the same omega\nomega = 150\nrng = np.random.default_rng(seed = omega)\nx = rng.normal(1, 0.2, size = N)\nprint(f\"Second draw with omega = {omega} result into:\\n{x}\")\n\n# change omega\nomega = 20\nrng = np.random.default_rng(seed = omega)\nx = rng.normal(1, 0.2, size = N)\nprint(f\"Third draw with omega = {omega} result into:\\n{x}\")\n\n# set omega to a 'random' choice depending on the clock of the computer\nomega = None\nrng = np.random.default_rng(seed = omega)\nx = rng.normal(1, 0.2, size = N)\nprint(f\"fourth draw with None (random) omega = {omega} result into:\\n{x}\")\n\n# Draw again with omega set to None\nomega = None\nrng = np.random.default_rng(seed = omega)\nx = rng.normal(1, 0.2, size = N)\nprint(f\"Another draw with None set as omega = {omega} result into:\\n{x}\")\n</code></pre> </p> Warning <p>Throughout, we assume that those samples are identically drawn from the joint distribution \\(\\mathcal{L}aw(Y, \\mathbf{X})\\). This doesn't always need to be the case. For instance for time series, there night be some past dependence between consecutive draws, but in that case the arguments below strongly need to be modified.</p> <p>For ease of notations, we stack the sample \\((\\mathbf{y}^N, \\mathbf{x}^{(N)})\\) into vectors and matrices</p> \\[ \\begin{equation*}     \\mathbf{y} := \\mathbf{y}^{(N)}=     \\begin{bmatrix}         y_1\\\\         \\vdots         \\\\         y_N     \\end{bmatrix}     \\quad     \\text{and}     \\quad     \\mathbf{x}:= \\mathbf{x}^{(N)} =     \\begin{bmatrix}     \\mathbf{x}_1\\\\     \\vdots\\\\     \\mathbf{x}_N     \\end{bmatrix}=     \\begin{bmatrix}         x_{11}&amp; \\cdots &amp; x_{1d}\\\\         \\vdots &amp; \\ddots &amp; \\vdots\\\\         x_{N1}&amp; \\cdots &amp; x_{Nd}\\\\     \\end{bmatrix} \\end{equation*} \\] <p>By the law of large numbers it holds</p> \\[ \\begin{equation*}   E\\left[\\mathbf{X}^\\top Y\\right] \\approx \\frac{1}{N}\\sum_{n=1}^N \\mathbf{x}_n^\\top y_n = \\mathbf{x}^\\top\\mathbf{y} \\quad \\text{and} \\quad E\\left[\\mathbf{X}^\\top \\mathbf{X}\\right] \\approx \\frac{1}{N}\\sum_{n=1}^N \\mathbf{x}_n^\\top \\mathbf{x}_n = \\frac{1}{N} \\mathbf{x}^\\top \\mathbf{x} \\end{equation*} \\] <p>showing that given a sample \\((\\mathbf{y}^{(N)}, \\mathbf{x}^{(N)})\\) we have an approximation of the OLS given by</p> \\[ \\begin{equation*}     \\hat{\\mathbf{b}}^{(N)} = \\left(\\mathbf{x}^\\top \\mathbf{x}\\right)^{-1}\\mathbf{x}^\\top \\mathbf{y}  \\quad \\text{and} \\quad \\hat{\\varepsilon}^{(N)} := \\mathbf{y} - \\mathbf{x}\\hat{\\mathbf{b}} \\end{equation*} \\] <p>How Good is the Sample Approximation?</p> <p>Two things happened here.</p> <ol> <li>We have a number \\(N\\) of independent copies of \\((Y, \\mathbf{X})\\)</li> <li>We choose a state \\(\\omega\\) and get the sample \\(\\mathbf{y}^{(N)}(\\omega)\\), \\(\\mathbf{x}^{(N)}(\\omega)\\).</li> </ol> <p>From the law of large numbers we know that when \\(N\\) goes to infinity then \\(\\hat{\\mathbf{b}}^{(N)}(\\omega) \\to \\mathbf{b}^\\ast\\) for almost any choice of \\(\\omega\\).</p> <p>However, this error depends on the choice of \\(\\omega\\) as well as the number \\(N\\) of samples drawn.</p> <p>To estimate the quality of the error we define</p> <ul> <li> <p>reduced \\(\\chi\\)-square statistics:</p> \\[ \\begin{equation*}     \\hat{s}^2 = \\hat{s}^{2, (N)}(\\omega) = \\frac{\\hat{\\varepsilon}^\\top \\hat{\\varepsilon}}{N-d} \\end{equation*} \\] <p>which is an unbiased estimation of the variance \\(\\hat{\\sigma}^2 = \\hat{\\varepsilon}^\\top \\hat{\\varepsilon}/N\\) which almost coincide for \\(N\\) large.</p> <p>Furthermore, by the law of large numbers, \\(\\hat{s}^{2, (N)}(\\omega)\\) \\(\\hat{\\sigma}^{2,(N)}(\\omega)\\) both converges to the residual variance \\(\\mathrm{RVAR}\\).</p> </li> <li> <p>Empirical Coefficient of Determination:</p> \\[ \\begin{equation*}     \\hat{R}^2 = \\hat{R}^{2, (N)} = 1 - \\frac{\\mathrm{VAR}(\\hat{\\epsilon}^{(N)})}{\\mathrm{VAR}(\\mathbf{y}^{(N)})} \\end{equation*} \\] <p>It also holds that \\(\\hat{R}^{2, (N)}(\\omega)\\) converges for almost all \\(\\omega\\) as \\(N\\) goes to \\(\\infty\\) to \\(R^2\\).</p> </li> </ul>"},{"location":"lecture/04-ML-101/041-LR/#implementation-in-numpy","title":"Implementation in Numpy","text":"<p>A naive implementation of the OLS linear regression is relatively simple and is done as follows. In this context, we just generate some random data to illustrate the situation. We give ourselves a fixed dimension \\(d\\) and consider the model</p> \\[ \\begin{equation*}     Y = \\mathbf{X} \\mathbf{b} + \\varepsilon \\end{equation*} \\] <p>where \\(\\mathbf{b}\\) is given, \\(\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\) is a multidimensional normal distribution with given mean \\(\\mathbf{\\mu}\\) in \\(\\mathbb{R}^d\\) and given covariance matrix \\(\\mathbf{\\Sigma}\\) in \\(R^{d\\times d}\\). However, we augment \\(\\mathbf{X}\\) by a dimension equal to \\(1\\).</p>"},{"location":"lecture/04-ML-101/041-LR/#model","title":"Model","text":"<p>We will fix</p> <ol> <li><code>d</code> is fixed as the begining for the dimension</li> <li><code>N</code> is the number of samples</li> <li><code>omega0</code>: is a seed to generate and fix given \\(d\\) the parameters <code>b</code>, <code>mu</code> and <code>Sigma</code>.</li> <li><code>omega1</code>: is a seed to draw <code>N</code> random samples <code>x</code> and <code>epsilon</code> from \\(\\mathbf{X}\\) and \\(\\varepsilon\\).</li> </ol> Generating an arbitrary multidimensional random variable <p>There are many multivariate random variable specifications in terms of their distribution, moment generating function or copulas. One of the easiest way is in the class of elliptical distributions that mainly depends on giving a positive semi-definite matrix \\(\\Sigma\\) in \\(R^{d\\times d}\\). It is cumbersome to generate a an arbitrary (random) positive semi definite matrix for a given dimension for testing purposes that is sufficiently random so that it does not influence too much the effect we want to see from real world data problems.</p> <p>A simple way is to take an orthonormal matrix \\(Q\\) (for instance from the QR factorization of a random matrix which with probability one is going to be orthonormal) and a diagonal matrix of positive eigenvalues.</p> <p>Set the parameters of the theoretical model.</p> <pre><code># import the libraries\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objs as go\n# for nice printing use rich library (pip install rich)\nfrom rich import print\nfrom rich.markdown import Markdown\n\n# set the fixed variables\nd = 3\nomega0 = 10  # seed to generate the constants\n\n# set the fixed variables\nd = 10\nomega0 = 10  # seed to generate the constants\nomega1 = 20  # seed to generate the sample.\n\n# %%\n# We set the constant for our test by fixing the random number generator\nrng0 = np.random.default_rng(omega0)\n\n# b, and mu, Sigma and sigma (for the error)\nb = rng0.uniform(low=-2, high=2, size=d + 1)  # random vector between -2 and 2\nmu = rng0.uniform(low=-0.1, high=0.1, size=d)  # random vector between -0.1 and 0.1\nsigma = 2\n# create a random d*d matrix, a random eigenvalue one, get q from qr and generate Sigma\neigenval = np.diag(rng0.uniform(low=0.1, high=1, size=d))\nq, _ = np.linalg.qr(rng0.normal(size=(d, d)))\nSigma = q.dot(eigenval).dot(q.T)\n\n# We compute the theoretical values of sigma2 and r2\nsigma2 = sigma**2\nr2 = 1 - sigma**2 / (sigma**2 + b[1:].dot(Sigma).dot(b[1:].T))\n\nprint(\n    Markdown(\n        f\"\"\"\n# Model values\n* Characteristics of the input:\n    * Mean:\\n\n        {mu}\n    * Covariance:\\n\n        {Sigma}\n    * Error volatility:\\n\n        {sigma: 0.2f}\n\n* Characteristic of the output:\n    * Coefficient:\\n\n        {b}\n\n# Theoretical values\n\n| Name       | Variable   | Value         |\n|:-----------|:-----------|--------------:|\n| s^2        | sigma2     | {sigma2:.02f} |\n| R^2        | r2         | {r2:.02%}     |\n\"\"\"\n    )\n)\n</code></pre> <p>For a given number of sample <code>N</code> and a seed <code>omega1</code> generate the model and run the linear regression.</p> <pre><code># Fix N and the random number generator\nN = 50\nrng1 = np.random.default_rng(omega1)\n\n# Data generation\nepsilon = rng1.normal(loc=0, scale=sigma, size=N)\nx = rng1.multivariate_normal(mean=mu, cov=Sigma, size=N)\n\n# expand x and get y\nx = np.column_stack((np.ones(N), x))\ny = x.dot(b.T) + epsilon\n\n# Perform linear regresssion\nb_hat = np.linalg.inv(x.T.dot(x)).dot(x.T).dot(y)\nepsilon_hat = y - x.dot(b_hat)\n\n# Empirical sigma2 and r2\nsigma2_hat = epsilon_hat.dot(epsilon_hat) / (N - d - 1)\nr2_hat = 1 - (epsilon_hat.std() ** 2) / (y.std() ** 2)\n\nprint(\n    Markdown(\n        f\"\"\"\n# Result\n\n* Coefficient:\n    * Theoretical:\\n\n    {b}\n    * Empirical:\\n\n    {b_hat}\n\n| Name       | Empirical          | Theoretical   |\n| :--------- | :---------         | ----------:   |\n| s^2        | {sigma2_hat: .02f} | {sigma2:.02f} |\n| R^2        | {r2_hat:.02%}      | {r2:.02%}     |\n\"\"\"\n    )\n)\n\n# Plot difference between empirical and theoretical coeficients\nfig = go.Figure()\nfig.add_bar(x=np.arange(d + 1), y=b, name=\"Theoretical\")\nfig.add_bar(x=np.arange(d + 1), y=b_hat, name=\"Empirical\")\nfig.show()\n</code></pre> <p>The results depends on the choice of <code>N</code> and the seed <code>omega1</code> at the beginning.</p>"},{"location":"lecture/04-ML-101/041-LR/#asymptotical-accuracy-of-ols-with-respect-to-n","title":"Asymptotical Accuracy of OLS with respect to <code>N</code>","text":"<p>We first have a look at the influence of the number of samples. For ease of data manipulation we use pandas</p> <pre><code>import pandas as pd\npd.options.plotting.backend = \"plotly\"\n\n# create a series of different number of samples\ndfN = pd.Series(np.arange(d + 10, 500, 10), name=\"N\")\ndfN.index = dfN\n\n# we use pandas apply to generate and store the values\ndef lin_reg(row):\n    N = row\n    rng1 = np.random.default_rng(omega1)\n    epsilon = rng1.normal(loc=0, scale=sigma, size=N)\n    rng1 = np.random.default_rng(omega1)\n    x = rng1.multivariate_normal(mean=mu, cov=Sigma, size=N)\n    # expand the dimension of x by adding a column of ones\n    x = np.column_stack((np.ones(N), x))\n    # compute y\n    y = x.dot(b.T) + epsilon\n    # We now estimate the parameters of the model\n    b_tmp = np.linalg.inv(x.T.dot(x)).dot(x.T).dot(y)\n    epsilon_tmp = y - x.dot(b_tmp)\n    # error estimation\n    sigma2_tmp = epsilon_tmp.dot(epsilon_tmp) / (N - d - 1)\n    r2_tmp = 1 - (epsilon_tmp.std() ** 2) / (y.std() ** 2)\n    # put the constant value\n    result = {\"a\": b_tmp[0]}\n    # put the further coeeficients\n    result.update({f\"b_{k}\": b_tmp[k] for k in range(1, d + 1)})\n    result.update({\"sigma2\": sigma2_tmp, \"r2\": r2_tmp})\n    return pd.Series(result)\n\nresult = dfN.apply(lin_reg)\n\ndisplay(results)\n\n# plot the empirical coefficients as a function of N with their theoretical values\nfig = go.Figure()\n# the constant coefficient\nfig.add_scatter(x = result.index, y = result[\"a\"], name = \"a_hat\")\nfig.add_hline(\n    y = b[0],\n    line_dash=\"dot\",\n    line_color=\"grey\",\n    annotation_text=\"a\",\n    annotation_position=\"top right\"\n)\nfor k in range(1, d + 1):\n    # the variable coefficients\n    fig.add_scatter(x = result.index, y = result[f\"b_{k}\"], name = f\"b_{k}_hat\")\n    fig.add_hline(\n        y = b[k],\n        line_dash = \"dot\",\n        line_color = \"grey\",\n        annotation_text = f\"b_{k}\",\n        annotation_position=\"top right\"\n    )\nfig.show()\n\n# Plot the empirical s2 and r2 as a function of N with their theoretical values\nfig = go.Figure()\nfig.add_scatter(x = result.index, y = result[\"sigma2\"], name = \"s2_hat\")\nfig.add_hline(\n    y = sigma2,\n    line_dash = \"dot\",\n    line_color = \"grey\",\n    annotation_text = \"s2\",\n    annotation_position = \"top right\"\n)\nfig.show()\n\nfig = go.Figure()\nfig.add_scatter(x = result.index, y = result[\"r2\"], name = \"r2_hat\")\nfig.add_hline(\n    y = r2,\n    line_dash = \"dot\",\n    line_color = \"grey\",\n    annotation_text = \"r2\",\n    annotation_position = \"top right\"\n)\nfig.show()\n</code></pre> <p>This gives you a feeling how the convergence of the different parameters and error estimations behaves as a function of the number of samples <code>N</code>.</p>"},{"location":"lecture/04-ML-101/041-LR/#distributional-behavior-of-the-ols-for-given-n","title":"Distributional Behavior of the OLS for given <code>N</code>","text":"<p>A similar question is related to the fact that given a number of <code>N</code> of samples, how \"accurate\" are your estimations? In the previous example even while modifying <code>N</code> we kept using the same random generator with a fixed seed (you can run the code many times, the result will be the same). We now, fix <code>N</code> but let the computer choose a seed at will at each draw (either draw random seeds or set it to <code>None</code>)</p> <pre><code>N = 100\nnumber_seeds = 5000\n\ndfomega = pd.Series(np.arange(number_seeds), name=\"omega\")\ndfomega.index = dfomega\n\n\n# we use pandas apply to generate and store the values\ndef lin_reg(row):\n    rng1 = np.random.default_rng(seed=None)\n    epsilon = rng1.normal(loc=0, scale=sigma, size=N)\n    rng1 = np.random.default_rng(None)\n    x = rng1.multivariate_normal(mean=mu, cov=Sigma, size=N)\n    # expand the dimension of x by adding a column of ones\n    x = np.column_stack((np.ones(N), x))\n    # compute y\n    y = x.dot(b.T) + epsilon\n    # We now estimate the parameters of the model\n    b_tmp = np.linalg.inv(x.T.dot(x)).dot(x.T).dot(y)\n    epsilon_tmp = y - x.dot(b_tmp)\n    # error estimation\n    sigma2_tmp = epsilon_tmp.dot(epsilon_tmp) / (N - d - 1)\n    r2_tmp = 1 - (epsilon_tmp.std() ** 2) / (y.std() ** 2)\n    # put the constant value\n    result = {\"a\": b_tmp[0]}\n    # put the further coeeficients\n    result.update({f\"b_{k}\": b_tmp[k] for k in range(1, d + 1)})\n    result.update({\"sigma2\": sigma2_tmp, \"r2\": r2_tmp})\n    return pd.Series(result)\n\n\nresult = dfomega.apply(lin_reg)\n\n# plot the histogram of the variables\ndata = result.loc[:, ~result.columns.isin([\"r2\", \"sigma2\"])] - b\ncols = 2\nif len(b) % cols == 0:\n    rows = len(b) // cols\nelse:\n    rows = (len(b) // cols) + 1\nprint(rows)\n\nfig = make_subplots(\n    rows=rows,\n    cols=cols,\n    shared_xaxes=True,\n    vertical_spacing=0.1,\n    shared_yaxes=True,\n    horizontal_spacing=0.1,\n)\n\nfor i, col in enumerate(data.columns):\n    fig.add_trace(\n        go.Histogram(x=data[col], name=col, histnorm=\"probability\"),\n        row=(i // cols) + 1,\n        col=(i % cols) + 1,\n    )\n\nfig.show()\n\n# plot the histogram of r2 and sigma2\nfig = make_subplots(rows=1, cols=2, shared_yaxes=True)\nfig.add_trace(\n    go.Histogram(x=result[\"sigma2\"], name=\"sigma2\", histnorm=\"probability\"),\n    row=1,\n    col=1,\n)\nfig.add_trace(\n    go.Histogram(x=result[\"r2\"], name=\"r2\", histnorm=\"probability\"), row=1, col=2\n)\nfig.show()\n</code></pre>"},{"location":"lecture/04-ML-101/041-LR/#statsmodels-with-real-data","title":"Statsmodels with Real Data","text":"<p>The statistical package <code>statsmodels</code> as well as the machinlearning one <code>sklearn</code> do have implementations of any linear regressions.</p> <p>For illustration, we make use of <code>statsmodel</code> with a wine dataset (see homework)</p>"},{"location":"lecture/04-ML-101/042-PCA/","title":"Principal Component Analysis","text":"<p>Principal component analysis, commonly referred to as PCA, is a linear method to reduce dimension in large sets of data. For instance, in linear regression, you want to approximate the output by a linear combination of inputs. Two problems then come to mind:</p> <ol> <li>The dimension \\(d\\) might be extremely large resulting into overfiting problems.</li> <li>Some of the inputs might themselves be collinear reducing the explanatory power of one factor with respect to the other one.</li> </ol> <p>The idea is to proceed to an orthonormal basis change under which the inputs will be orthogonal and select the vectors with the largest eigenvalue.</p>"},{"location":"lecture/04-ML-101/042-PCA/#theory","title":"Theory","text":"<p>Theorem: Spectral Decomposition</p> <p>Any \\(d\\times d\\) dimensional real valued symetric matrix \\(\\mathbf{A}\\) can be decomposed into.</p> \\[ \\begin{equation*}     \\mathbf{A} = \\mathbf{\\Gamma} \\mathbf{\\Lambda} \\mathbf{\\Gamma}^\\top \\end{equation*} \\] <p>where</p> <ol> <li>\\(\\mathbf{\\Lambda} = \\mathrm{diag}(\\lambda_1, \\ldots, \\lambda_d)\\) is a matrix of real valued eigenvalues.</li> <li>\\(\\mathbf{\\Gamma}\\) is an orthogonal matrix whose columns are normalized eigenvectors satisfying \\(\\mathbf{\\Gamma}^\\top \\mathbf{\\Gamma} = \\mathbf{\\Gamma} \\mathbf{\\Gamma}^\\top = I\\).</li> </ol> <p>Furthermore, if the matrix is positive semi-definite, that is \\(\\mathbf{x}^\\top \\mathbf{A}\\mathbf{x} \\geq 0\\) for any column vector \\(\\mathbf{x}\\), the eigenvalues are then positives and strictly positive in case of positive definiteness.</p> <p>Consider a column vector \\(\\mathbf{X}\\) of random variable and define the mean vector and covariance matrix</p> \\[ \\begin{equation*}     \\mathbf{\\mu} = E\\left[\\mathbf{X}\\right] \\quad \\text{and} \\quad \\mathbf{\\Sigma} :=\\mathrm{Cov}(\\mathbf{X})= E\\left[\\left(\\mathbf{X}-\\mathbf{\\mu}\\right)\\left(\\mathbf{X} - \\mathbf{\\mu}\\right)^\\top\\right] \\end{equation*} \\] Properties of Covariance and Mean <p>Clearly, the covariance is a real valued symetric matrix.</p> <p>However, since for a matrix \\(\\mathbf{B} \\in \\mathbb{R}^{l\\times d}\\) and a column vector \\(\\mathbf{a}\\in \\mathbb{R}^l\\), it holds</p> \\[ \\begin{equation*}     E\\left[ \\mathbf{B}\\mathbf{X} + \\mathbf{a} \\right] = \\mathbf{B}\\mathbf{\\mu} + \\mathbf{a}\\quad \\text{and}\\quad \\mathrm{Cov}\\left(\\mathbf{B}\\mathbf{X} + \\mathbf{a}\\right) = \\mathbf{B}\\mathrm{Cov}(\\mathbf{X})\\mathbf{B}^\\top \\end{equation*} \\] <p>it follows that \\(\\mathrm{Cov}(\\mathbf{X})\\) is positive semi-definite. Indeed, for any column vector \\(\\mathbf{y}\\), it follows that</p> \\[ \\begin{equation*}     \\mathbf{y}^\\top \\mathbf{\\Sigma}\\mathbf{y} = \\mathrm{Cov}(\\mathbf{y}^\\top \\mathbf{X}) = \\mathrm{Var}(\\mathbf{y}^\\top \\mathbf{X})\\geq 0 \\end{equation*} \\] <p>Since \\(\\mathbf{\\Sigma}\\) is positive semidefinite, let \\(\\lambda_{1}, \\ldots, \\lambda_{d}\\) be the real valued eigenvalues and \\(\\mathbf{\\Gamma}_1, \\ldots,\\mathbf{\\Gamma}_d\\) be the the corresponding eigenvectors for which holds</p> \\[ \\begin{equation*}     \\mathbf{\\Sigma} =      \\begin{bmatrix}     \\mathbf{\\Gamma}_1 &amp; \\ldots &amp; \\mathbf{\\Gamma}_d     \\end{bmatrix}     \\begin{bmatrix}     \\lambda_1 &amp; \\cdots &amp; 0 \\\\     \\vdots &amp; \\ddots &amp; \\vdots \\\\     0 &amp; \\cdots &amp; \\lambda_d     \\end{bmatrix}     \\begin{bmatrix}     \\mathbf{\\Gamma}^\\top_1 \\\\ \\vdots \\\\ \\mathbf{\\Gamma}^\\top_d     \\end{bmatrix} \\end{equation*} \\] <p>Without loss of generality up to permutation of the columns of \\(\\mathbf{\\Gamma}\\) we can consider that \\(\\lambda_1\\geq \\ldots \\geq \\lambda_0 =0\\).</p> Permuting the Representation <p>The spectral decomposition holds under permutation. Indeed, let \\(\\sigma\\) be a permutation of \\(\\{1, \\ldots, d\\}\\), we define the column permutation matrix \\(P\\) as \\(P_{ij} = 1\\) if \\(j =\\sigma(i)\\) and zero otherwise. This permutation matrix is invertible with inverse \\(P^\\top\\) which coincide with the row permutation matrix. In particular \\(P\\) is orthonormal.</p> <p>Given a matrix \\(A\\), the product \\(P^\\top A\\) permuts the rows of \\(A\\) while the product \\(AP\\) permuts the columns of \\(A\\). Hence, it follows that</p> \\[ \\begin{equation*}     \\mathbf{\\Sigma} = \\mathbf{\\Gamma}\\mathbf{\\Lambda}\\mathbf{\\Gamma} = \\mathbf{\\Gamma}P P^\\top \\mathbf{\\Lambda}P P^\\top \\mathbf{\\Gamma} = (\\mathbf{\\Gamma}P) (P^\\top \\Lambda P)(\\mathbf{\\Gamma}P) \\end{equation*} \\] <p>Since the set of orthogonal matrices is a group we can define \\(\\tilde{\\mathbf{\\Gamma}} = \\mathbf{\\Gamma}P\\) and \\(\\tilde{\\mathbf{\\Lambda}}=P^\\top \\mathbf{\\Lambda}P\\).</p> <p>PCA Decomposition</p> <p>The Principal component transform of \\(\\mathbf{X}\\) is the random vector</p> \\[ \\begin{equation*}     \\mathbf{Y} = \\mathbf{\\Gamma}^\\top\\left(\\mathbf{X} - \\mathbf{\\mu}\\right) \\end{equation*} \\] <p>It clearly follows that</p> \\[ \\begin{equation*}     E\\left[\\mathbf{Y}\\right] = 0 \\quad \\text{and}\\quad \\mathrm{COV}(\\mathbf{Y}) = \\mathbf{\\Lambda} \\end{equation*} \\] <p>In particular, with a total variance \\(\\sum_{k=1}^d \\mathrm{VAR}\\left(Y_k\\right) = \\sum_{k=1}^d \\lambda_k = \\mathrm{Trace}(\\mathbf{\\Lambda}) = \\mathrm{Trace}(\\mathbf{X}) = \\sum_{k=1}^d \\mathrm{VAR}(X_k)\\) for which we define the ratio</p> \\[ \\begin{equation*}     \\mathrm{tv}(k):=\\frac{\\sum_{l=1}^k \\lambda_k}{\\mathrm{Trace}(\\mathbf{\\Lambda})} \\end{equation*} \\] <p>that can be interpreted as the percentage of variance explained by the first \\(k\\) components. This ratio is clearly increasing from \\(0\\) to \\(100\\%\\) and concave since we ordered the components decreasingly. Given a threshold \\(\\theta\\) (for instance \\(\\theta = 50\\%\\)), the smallest \\(k_0\\) such that \\(\\mathrm{tv}(k)\\geq \\theta\\) is such that the first \\(k_0\\) components explains at least \\(\\theta \\%\\) of the total variation of \\(\\mathbf{X}\\).</p> <p>We then define \\(\\mathbf{Y}^{(1)}\\) and \\(\\mathbf{Y}^{(2)}\\) as the first \\(k_0\\) and last \\(d-k_0+1\\) rows of \\(\\mathbf{Y}\\) respectively as well as (remember \\(\\Gamma\\) is the orthogonal matrix of eigenvectors after permuting the columns) \\(\\mathbf{\\Gamma}^{(1)}\\) and \\(\\mathbf{\\Gamma}^{(2)}\\) as the matrix with the first \\(k_0\\) and last \\(d-k_0+1\\) eigenvectors, respectively. It holds that</p> \\[ \\begin{equation*}   \\mathbf{X} = \\mathbf{\\mu} + \\mathbf{\\Gamma}^{(1)}\\mathbf{Y}^{(1)}+ \\mathbf{\\Gamma}^{(2)}\\mathbf{Y}^{(2)} = \\mathbf{\\mu} + \\mathbf{\\Gamma}^{(1)}\\mathbf{Y}^{(1)} + \\mathbf{\\varepsilon} \\end{equation*} \\] <p>where \\(\\mathbf{\\varepsilon}\\) is a random vector only contributing to less than \\((1-\\theta)\\%\\) of the total variance of \\(\\mathbf{X}\\).</p> <p>Normalization</p> <p>Note that the PCA is not invariant with scaling of one or the other input. Depending on the situation (in particular when there is a large variation in terms of the amplitude of the inputs) a normalization (by the standard deviation or with quantiles) can be performed. However the interpretation is totally different as the projection might differ a lot.</p>"},{"location":"lecture/04-ML-101/042-PCA/#numpy-implementation","title":"Numpy Implementation","text":"<p>To perform a PCA decomposition, the functionalities of <code>np.linalg.eig</code> are entirely sufficient to provide a PCA decomposition. For this we will make use of the dataset CSI that represents the price of 208 stocks over 7 years of the CSI Index. We want to consider as \\(\\mathbf{X}\\) the normalized returns of these stock prices and proceed to a PCA decomposition of which (basically we do a PCA of the correlation matrix rather than covariance matrix.)</p> <p>Import the data and </p> <p><pre><code># import numpy, pandas to load data and plotly to plot\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n# load the dataset, convert the dates to datetime and set as index\ndfstocks = pd.read_csv('./data/CSI.csv')\ndfstocks[\"Date\"] = pd.to_datetime(dfstocks[\"Date\"])\ndfstocks = dfstocks.set_index(\"Date\")\n\n# get the returns and dropna as it destroys positive definiteness\ndfret = dfstocks.pct_change().dropna().copy()\n\n# compute the correlation matrix\nCorr = dfret.corr() \n\n# and plot it as a heat map\nfig = px.imshow(Corr, color_continuous_scale='Inferno_r')\nfig.show()\n</code></pre> The correlation matrix is quite mixed most of which are positive. We now proceed to the PCA decomposition of this matrix.</p> <pre><code># use linalg.eig to get eigenvalues and eigenvectors (eigenvector[:, i] is the ith eigenvector)\neigenvalues, eigenvectors = np.linalg.eig(Corr)\n\n\n# %% plot the cumulative normalized eigenvalues and their relative weights with two different y-axi\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\nfig.add_trace(\n    go.Bar(\n        x=dfeigenval.index,\n        y=dfeigenval / dfeigenval.sum(),\n        name=\"Eigenvalues Relative Weights\",\n    ),\n    secondary_y=False,\n)\nfig.add_trace(\n    go.Scatter(\n        x=dfeigenval.index,\n        y=dfeigenval.cumsum() / dfeigenval.sum(),\n        mode=\"lines\",\n        name=\"Eigenvalues Cumulative Weights\",\n    ),\n    secondary_y=True,\n)\n# update the axis layout for each axis\nfig.update_yaxes(tickformat=\",.0%\", title=\"Relative Weights\", secondary_y=False)\nfig.update_yaxes(tickformat=\",.0%\", title=\"Cumulative Weights\", secondary_y=True)\nfig.show()\n</code></pre> <p>Now decide for a threshold (here \\(60\\%\\)) and define the principal factor loading matrix.</p> <pre><code># set the threshold\ntheta = 0.6\n\n# get the index of those factor that only contributes beyond theta\nidx = dfeigenval.cumsum() / dfeigenval.sum() &gt;= theta\nprint(f\"The index of those factors that explain at least {theta:.0%} of the variance are:{~idx}\")\n\n# now slice the dfeigenvectors dataframe by keeping only the columns that are in ~idx\npca = dfeigenvec.loc[:, ~idx].copy()\ndisplay(pca)\nprint(f\"The original dimension of {len(dfeigenvec.columns)} has been reduced to {len(pca.columns)}\")\n</code></pre>"},{"location":"lecture/04-ML-101/042-PCA/#using-scikitlearn","title":"Using Scikitlearn","text":"<p>Scikit learn provides a package performing PCA entirely.</p>"},{"location":"lecture/04-ML-101/043-Cluster/","title":"Clustering","text":"<p>The principle of clustering is to gether similar data points among groups. Given a set \\(X = \\{x_1, \\ldots, x_N\\}\\) of \\(N\\) points in \\(\\mathbb{R}^d\\), the goal is to find a partition (cluster) \\(C_1, \\ldots, C_K \\subseteq X\\) which somehow group similar points.</p> <p>There are several ways to cluster in one way or another depending on how you want to define similarity and also how is your dataset. For instance</p> <ul> <li>Perspecified Cluster: or basic classification of data.     For instance partition companies in different categories (<code>banking</code>, <code>insurance</code>, <code>construction</code>, etc.).     Those categories are pre given, the only action is to allocate each point to the closest category.     This is not really what people understand under clustering as the cluster itself should be derived rather than given.</li> <li>Hierarchical Clustering: Build a hierarchy of clusters from coarse to fine, similar to a tree.</li> <li>Centroid Clustering: Cluster data according to geometry and distance.</li> <li>etc.</li> </ul> <p>In the following we concentrate on a simple but prominent clustering method involving grouping points according to within distance in a group. Similarity between points is defined in terms of a (pseudo) distance \\(d(x,y)\\). The clustering aims to find an optimal cluster \\(C_1^\\ast, \\ldots, C_K^\\ast\\) such that</p> \\[ \\begin{equation} \\sum_{k=1}^K \\frac{1}{\\# C^\\ast_k}\\sum_{x, y \\in C_k^\\ast} d(x, y) \\leq \\sum_{k=1}^K \\frac{1}{\\# C_k}\\sum_{x, y \\in C_k} d(x, y) \\end{equation} \\] <p>for any other cluster \\(C_1, \\ldots, C_K\\).</p> <p>We denote by \\(\\mathfrak{C}\\) the set of all clusters (or partitions) \\(\\mathcal{C} = \\{C_1, \\ldots, C_K\\}\\) of \\(X\\) in \\(K\\) elements and define</p> \\[ \\begin{equation*}     F(\\mathcal{C}) = \\sum_{C \\in \\mathcal{C}} \\frac{1}{\\# C}\\sum_{x, y \\in C} d(x, y) \\end{equation*} \\] <p>Clustering can therefore be reformulated into an optimization problem</p> \\[ \\begin{equation*}     \\mathcal{C}^\\ast = (C_1^\\ast, \\ldots, C_K^\\ast) = \\mathrm{argmin}\\left\\{F(\\mathcal{C})\\colon \\mathcal{C} \\in \\mathfrak{C}\\right\\} \\end{equation*} \\] <p>Computing \\(F\\) for a given cluster \\(\\mathcal{C}\\) is relatively fast as long as the distance is quick to compute. However, the optimization problem itself is very difficult. Indeed, it is a minimization problem on a set \\(\\mathfrak{C}\\) which does not have a suitable topology to define derivatives, for instance. Hence, the only a priory way is a brute force optimization, that is, running through every possible partition. It is however not suitable since the cardinality of \\(\\mathfrak{C}\\) is gigantic. It corresponds to the stirling number of the second kind \\({ N \\brace K}\\):</p> \\[ \\begin{equation*}     \\#\\mathfrak{C} := {N\\brace K} = \\sum_{k=0}^K \\frac{(-1)^{K-k} k^N}{(K-k)!k!} \\sim_{N\\to \\infty} \\frac{K^N}{K!} \\end{equation*} \\] <p>meaning that for a fixed number \\(K\\), the cardinality is growing exponentially in the size of the set. The problem can be refined and some better approximation can be found but in general this is NP-Hard. However, with some assumptions about the distance, and geometrical considerations, an honnest and fast algorithm can be designed to achieve some local optimum. We consider as distance the square of the euclidean norm, that is \\(d(x,y) = \\|x - y\\|^2\\).</p> <p>In this specific case, it turns out that the problem can be reformulated in terms of barycenter or centroid with respect to the distance</p> \\[ \\begin{equation*}     F(\\mathcal{C}) = \\sum_{C \\in \\mathcal{C}} \\sum_{x \\in X} \\|x - \\mu_{C}\\|^2 \\end{equation*} \\] <p>where \\(\\mu_C = \\frac{1}{\\# C} \\sum x\\) is the average/barycenter or centroid of \\(C\\).</p> <p>With this reformulation in term of the geometric center of \\(C\\), it leads to the following idea for an algorithm to select a partition.</p> <ol> <li>Initialize \\(K\\) centers \\(\\mu_1(0), \\ldots, \\mu_K(0)\\) by choosing \\(K\\)-points in \\(X\\).</li> <li> <p>Recursively: While \\(\\mathcal{C}(n+1) \\neq \\mathcal{C}(n)\\) at the end of the following do:</p> <ul> <li> <p>Given \\(\\mu_1(n), \\ldots \\mu_K(n)\\) define the sets \\(C_1(n+1), \\ldots, C_K(n+1)\\): </p> <p>\\(C_k(n+1) = \\left\\{x \\in X\\colon \\|x - \\mu_K(k)\\|^2 \\leq \\|x - \\mu_K(j)\\|^2 \\text{ for any }j\\neq k\\right\\}\\)</p> </li> <li> <p>update the new centers \\(\\mu_1(n+1), \\ldots, \\mu_K(n+1)\\):</p> <p>\\(\\mu_k(n+1) = \\frac{1}{\\# C_k(n+1)} \\sum_{x \\in C_k(n+1)} x\\)</p> </li> </ul> </li> </ol> <p>With this algorithm, modulo care about common points in clusters, at each step \\(F(\\mathcal{C}(n+1))\\leq F(\\mathcal{C}(n)\\). Hence, there exists a sequence of clusters along which the cost function is decreasing. Since the number of cluster, though large, is finite, the algorithm ends after a finite number of steps.</p>"},{"location":"lecture/04-ML-101/043-Cluster/#implementation-in-numpy-and-pandas","title":"Implementation in Numpy and Pandas","text":"<p>In the following implementation, we assume that we have \\(N\\) samples of \\(d\\)-dimensional data points, that is, a <code>Nxd</code>-array</p> <pre><code>import numpy as np\n# we assume that we have a set of data N*d with N&gt;d\n\ndef basic_cluster(data, K, max_it = 1e3, tol = 1e-4):\n    N = data.shape[0]\n    # we check that there are enough datapoints.\n    if N &lt; K:\n        print(f\"The number of datapoints {N} is not sufficient to group in {K} clusters.\")\n        break\n\n    # create a numpy array of size N which entries will corresponds to the cluster value for data point n\n    cluster = np.zeros(N)\n\n    # initialize a set of centroids taken randomly from the data using random choice without replacement\n    rng = np.random.default_rng()\n    mu = rng.choice(data, size = K, replace = False)\n\n    # perform the while loop until centers are the same or max_it is reached\n    i = 0\n    diff = 1e8\n\n    while i &lt; max_it or diff &lt; tol:\n        # we make a copy of the data\n        old_mu = mu.copy()\n        data_tmp = data.copy()\n        # Compute the distance matrix (KxN) of the joint distance between each point and each centroid.\n        distmatrix = np.sum(mu**2, axis = 1)[..., None] + np.sum(data**2, axis = 1)[None, ...] - 2 * np.dot(mu, data.T)\n\n        # update the labels for the clusters \n        cluster = np.argmin(distances, axis = 0) \n\n        # update the centroids\n        for k in range(K):\n            mu[k] = np.mean(data[cluster == k], axis = 0)\n\n        # update the difference between the old centers and the new ones.\n        diff = np.linalg.norm(mu - old_mu)\n</code></pre>"},{"location":"lecture/04-ML-101/043-Cluster/#using-k-mean-on-real-data","title":"Using K-Mean on real data","text":"<p>K-Mean has many refinements: the algorithm above does not guarantee for instance that at each step an element of a cluster is non-empty. Also, the algorithm only converges to a local minimum and is therefore highly dependent on the initialization, therefore some procedure with some randomization and running several rounds to get closer to a possible global minimum.</p> <p>The library <code>sklearn</code>, install the package <code>scikit-learn</code> with <code>conda</code> or <code>pip</code>, has a functionality for clustering. We consider the dataset California Housing which represents the housing data for California. It contains housing location (<code>longitude</code>, <code>latitude</code>) as well as <code>medianIncome</code> for each house.</p> <pre><code># some package to deal with the following\nimport numpy as np\nimport pandas as pd\nimport ploty.express as px\nfrom sklearn.cluster import KMeans as km\n\n# we import the data\ndf = pd.read_csv(\"./data/housing.csv\")\n\n# it has many fields\nprint(df.info())\n\n# We plot the median income as a function of the location \npx.scatter(\n    df,\n    x=\"longitude\",\n    y=\"latitude\",\n    color=\"median_income\",\n    size=\"population\",\n)\n</code></pre> <p>There is a discrepancy in the housing income and the location but not that clear. We want to group the datapoints according to the features if we can see a clearer picture. Beforehand, we clean the data (we remove the field <code>ocean_proximity</code>, though you could give him a label) and we normalize the data (the values are not in the same range). <code>sklearn</code> has a functionality for it with <code>preprocessing.normalize</code> but we do it by hand. We then process Kmean.</p> <pre><code># drop the column ocean_rpoximity\ndf = df.drop(columns = \"ocean_proximity\")\n# drop nan values\ndf = df.dropna()\n\n# normalize the values\ndata = df.copy()\ndata = (data - data.mean())/data.std()\n\n# perform Kmean into three regions\nkmean = km(n_clusters=3)\nkmean.fit(data)\n\n# we add the labels in the original dataframe\ndf['labels'] = kmean.labels_\ndf['labels'] = \"K\" + df['labels'].astype(str)\n\n# we scatter plot the labels in terms of group\npx.scatter(\n    df,\n    x=\"longitude\",\n    y=\"latitude\",\n    color=\"labels\",\n    size=\"population\",\n)\n\n# We can provide the statistical description per group\ndf.groupby(\"label\").describe()\n</code></pre>"},{"location":"lecture/05-Derivatives/051-differentiation/","title":"Differentiation","text":"<p>A central aspect of mathematics, counterpart to integration, is computing derivatives. Many different applications depends on it such as ODE, optimization, PDE, AI, etc.</p> <p>Mathematically, given a smooth function \\(f:\\mathbb{R}\\to \\mathbb{R}\\), the derivative of \\(f\\) at point \\(x\\) is defined as</p> \\[ \\begin{equation*}   f^\\prime(x) = \\lim_{h \\to 0}\\frac{f(x+h)-f(x)}{h} \\end{equation*} \\] <p>Unless the derivative is explicitely known, since limit are unatainable ojects for computer an approximation has to be done and a direction has to be choosen. Let \\(h\\) be small we can consider</p> <ul> <li>Forward difference: \\(f^\\prime(x)\\approx (f(x+h) - f(x))/h\\)</li> <li>Backward difference: \\(f^\\prime(x)\\approx (f(x) - f(x-h))/h\\)</li> <li>Central difference: \\(f^\\prime(x)\\approx (f(x+h) - f(x-h))/2h\\)</li> </ul> <p>Further higher order differences can also be considered at point \\(h\\), \\(2h\\), etc.</p> <pre><code>def diff_for(f, x, h):\n    return (f(x + h) - f(x))/h\n\ndef diff_back(f, x, h):\n    return (f(x) - f(x-h))/h\n\ndef diff_cent(f, x, h):\n    return (f(x+h) - f(x-h))/(2 * h)\n</code></pre> <p>In the limit all these object coincide (provided you are smooth) but the forward difference corresponds to the right derivative while the backward corresponds to the left one and the central is the average between both. Depending on the slope of the function where the derivative is computed, the approximation might differ.</p> <p>For example, consider \\(f(x) = \\exp(x + x^2)\\) with derivative \\(f^\\prime(x) = (2x+1)\\exp(x+x^2)\\). In particular \\(f^\\prime(0) = 1\\).</p> <pre><code>import numpy as np\nimport plotly.graph_objs as go\n\ndef f(x):\n    return np.exp(x + x**2)\n\nN = np.linespace(-8, -1, 100)\nh = 10 ** N\n\nYfor = diff_for(f, 0, h)\nYback = diff_back(f, 0, h)\nYcent = diff_cent(f, 0, h)\n\nfig = go.Figure()\nfig.add_scatter(x = h, y = Yfor, name = 'Forward diff')\nfig.add_scatter(x = h, y = Yback, name = 'Backward diff')\nfig.add_scatter(x = h, y = Ycent, name = 'Central diff')\nfig.show()\n</code></pre> <p>As the slope is strongly different before and after \\(0\\), it turns out that the convergence of the central diff is faster to the true value.</p> Computational Complexity <p>One shall however consider also the numerical cost of computing those numerical derivatives. Indeed, consider a smooth function \\(F:\\mathbb{R}^d \\to \\mathbb{R}\\), the derivative of \\(F\\) at point \\(\\mathbf{x}\\) is given by the gradient vector</p> \\[ \\begin{equation*}   \\nabla_{\\mathbf{x}}F(\\mathbf{x}) = \\begin{bmatrix} \\partial_{x_1} F(\\mathbf{x})\\\\ \\vdots\\\\ \\partial_{x_d}F(\\mathbf{x}) \\end{bmatrix} \\quad \\text{where}\\quad \\partial_{x_i}F(\\mathbf{x}) = \\lim_{h\\to 0}\\frac{F(x_1, \\ldots, x_i + h,\\ldots, x_d ) - F(\\mathbf{x})}{h} \\end{equation*} \\] <p>Since you always have to compute the value of \\(F(\\mathbf{x})\\), it turns out that the number of evaluation of \\(F\\) to compute the gradient is</p> <ul> <li>\\(1+d\\) for the backward and forward derivative</li> <li>\\(1+2d\\) for the central derivative (and higher for higher order differences).</li> </ul> <p>In analysis, \\(h\\) is actually infinitely small, so how small shall we choose \\(h\\). The computer accuracy is pretty small, however by taking too small you run into issues</p> <pre><code># we consider smaller values in the range of 10^{-15} for h\nN = np.linespace(-15, -1, 100)\nh = 10 ** N\n\nYfor = diff_for(f, 0, h)\nYback = diff_back(f, 0, h)\nYcent = diff_cent(f, 0, h)\n\nfig = go.Figure()\nfig.add_scatter(x = h, y = Yfor, name = 'Forward diff')\nfig.add_scatter(x = h, y = Yback, name = 'Backward diff')\nfig.add_scatter(x = h, y = Ycent, name = 'Central diff')\nfig.show()\n</code></pre> <p>It turns out that for very small value of \\(h\\), computer precision and rounding issues goes against accuracy (dividing by \\(10^{-15}\\) is multiplying rounding errors of the difference by \\(10^{15}\\)).</p>"},{"location":"lecture/05-Derivatives/051-differentiation/#ordinary-differential-equations","title":"Ordinary Differential Equations","text":"<p>One of the first application of the computing derivative is to solve numerically ODEs of the kind</p> \\[ \\begin{equation*}   y^\\prime(t) = f(t, y(t)), \\quad \\text{with}\\quad y(0) = y_0 \\end{equation*} \\] Higher dimension and higher derivative <p>You can also consider </p> \\[ \\begin{equation*}   \\mathbf{y}^\\prime(t) = \\mathbf{F}(t, \\mathbf{y}(t)) \\quad \\text{with} \\quad \\mathbf{y}(0) = \\mathbf{y}_0 \\end{equation*} \\] <p>where \\(\\mathbf{y}\\) is \\(d\\)-dimensional and \\(\\mathbf{F}\\colon \\mathbb{R}\\times \\mathbb{R}^d\\to \\mathbb{R}^d\\).</p> <p>If the ODE is of higher order of integration, with linear structure, you can also write it in vector form of a first order linear ODE.</p> <p>The very basic and first scheme is the Euler scheme that takes a forward difference as approximation:</p> \\[ \\begin{equation*} \\mathbf{y}(t+h) = \\mathbf{y}(t) + h \\mathbf{F}(t, \\mathbf{y}(t)) \\end{equation*} \\] <p>the implementation of which is straightforward</p> <pre><code>import numpy as np\n\n# Generic Euler scheme for one dimension \n# input:\n# * f: given function with t and y as input and real output\n# * y0: initial value\n# * t: is the meshgrid [t0, t1,..., tN]\ndef ode_euler(f, y0, t):\n    y = np.zeros(len(t))\n    y[0] = y0\n\n    for n in range(0, len(t)-1):\n        y[n+1] = y[n] + f(t[n], y[n]) * (t[n+1] - t[n])\n    return y\n</code></pre> \\[ y^\\prime(t) = 2(1-y(t)) - 4 e^{-4t} \\quad \\text{with} y(0)=1 \\] <p>with explicit solution</p> \\[ \\begin{equation*}   y(t) = 1-2 e^{-2t}\\left(1-e^{-2t}\\right) \\end{equation*} \\] <pre><code>import plotly.graph_objs as go\n\ndef f(t, y):\n    return 2 * (1 - y) - 4 * np.exp(-4 * t)\n\n\ndef solution(t):\n    return 1 - 2 * np.exp(-2 * t) * (1 - np.exp(-2 * t)) \n\n# Now we solve for several mesh size and compare with the optimal value\n\nT = np.linspace(0, 2, 100)\nDT = np.array([0.5, 0.1, 0.05, 0.01])\ny0 = 1\n\nfig = go.Figure()\nfig.add_scatter(x=T, y=solution(T), name=\"theoretical\")\n\nfor dt in DT:\n    t = np.arange(0, 2, dt)\n    y = ode_euler(f, t, y0)\n    fig.add_scatter(x=t, y=y, name=f\"num {dt}\")\nfig.show()\n</code></pre> <p>The Euler scheme is a simple (each step is fast to compute) but as a matter of fact the method is iterative and therefore involves a loop. For a good accuracy the mesh shall be small which however increases the number of evaluation.</p> <p>Several methods have been developed to increase the accuracy for a given step, however at the cost of additional computation at every steps. As long as the improvement in accuracy for a given step is is higher than the overall cost then it is computationally a good method.</p> <p>The classical ODE method in this case is the (here the 4th order one) Runge-Kutta method which runs as follows</p> \\[ \\begin{equation*}     y(t+h) = y(t) + \\frac{h}{6}(k_1 + 2 k_2 + 2 k_3 + k_4) \\end{equation*} \\] <p>where</p> \\[ \\begin{equation*}   \\begin{cases}     k_1 &amp; = f(t, y(t))\\\\     k_2 &amp;= f\\left(t+\\frac{h}{2}, y(t) + h \\frac{k_1}{2}\\right)\\\\     k_3 &amp;= f\\left(t+\\frac{h}{2}, y(t) + h \\frac{k_2}{2}\\right)\\\\     k_4 &amp; = f\\left(t+h, y(t) + h k_3\\right)   \\end{cases}   \\end{equation*} \\] <pre><code>def ode_rk(f, y0, t):\n    y = np.zeros(len(t))\n    y[0] = y0\n\n    for n in range(0, len(t)-1):\n        h = t[n+1] -t[n]\n        k1 = f(t[n], y[n])\n        k2 = f(t[n] + h/2, y[n] + h * k1 /2)\n        k3 = f(t[n] + h/2, y[n] + h * k2 /2)\n        k4 = f(t[n] + h, y[n] + h * k3)\n        y[n+1] = y[n] + h * (k1 + 2 * k2 + 2 * k3 + k4) /6\n    return y\n</code></pre> <p>We can now run a test between the two methods</p> <pre><code>T = np.linspace(0, 2, 100)\nDT = np.array([0.1, 0.05])\ny0 = 1\n\nfig = go.Figure()\nfig.add_scatter(x=T, y=solution(T), name=\"theoretical\")\n\nfor dt in DT:\n    t = np.arange(0, 2, dt)\n    y = ode_euler(f, t, y0)\n    fig.add_scatter(x=t, y=y, mode = \"lines\" name=f\"num {dt}\")\nfig.show()\n</code></pre>"},{"location":"lecture/05-Derivatives/052-PDE/","title":"Partial Differential Equations","text":"<p>Partial differential equations are an extension of ODE where the evolution depends not only on time but takes into account the environment values in a neighborhood.</p> <p>The most prominent example of which is the diffusion of temperature in space which in its simplest form (one dimensional) is given by</p> \\[ \\partial_t u = \\frac{1}{2}\\partial_{xx} u \\] <p>This equation involves the second derivative for which we will consider the central difference:</p> \\[ \\begin{equation*}   f^{\\prime\\prime}(x) \\approx \\frac{f(x+h) + f(x-h) - f(x)}{2 h^2} \\end{equation*} \\]"},{"location":"lecture/05-Derivatives/053-gradient/","title":"Gradient Descent","text":"<p>Gradient descent is another application where derivatives plays a role. The main objective of gradient descent is to design a scheme that will converge to the (or better a local) minimum of a function.</p> <p>Given a smooth function \\(f:\\mathbb{R}^d \\to \\mathbb{R}\\), the goal is to find a local minimum \\(\\mathbf{x}^\\ast\\), that is</p> \\[ \\begin{equation*}   f(\\mathbf{x}^\\ast) \\leq f(\\mathbf{y}) \\end{equation*} \\] <p>for any \\(\\mathbf{y}\\) in a neighborhood of \\(\\mathbf{x}^\\ast\\). If this relation holds for any \\(\\mathbf{y}\\), then \\(\\mathbf{x}^\\ast\\) is called a global minimum.</p> <p>Since \\(f\\) is assumed to be smooth, it follows that a necessary (but not sufficient, think about a saddle point) condition for it is that</p> \\[ \\nabla f(\\mathbf{x}^\\ast) = 0 \\] <p>If an explicit solution or a direct root finding method is not available, we want to construct a sequence \\(\\mathbf{x}_0, \\mathbf{x}_1, \\ldots\\) such that \\(\\mathbf{x}_n \\to \\mathbf{x}^\\ast\\).</p> <p>The classical intuition behind the gradient descent, is the situation where you are in the mountains, it is foggy and you want to find your way down to the valley. Your vision is just local but you can estimate in a close radius in which direction the path is going downwards. You move further and update along the way the direction in which you are going.</p> <p>This principle rely on the following result:</p> <p>Descent Direction</p> <p>A vector \\(\\mathbf{d}\\) is called a descent direction at \\(\\mathbf{x}\\) if \\(f(\\mathbf{x} + t\\mathbf{d})&lt;f(\\mathbf{x})\\) for all \\(t&gt;0\\) sufficiently small.</p> <p>Proposition</p> <p>If \\(f\\) is smooth, then any \\(\\mathbf{d}\\) such that \\(\\mathbf{d}^\\top \\nabla f(\\mathbf{x})&lt;0\\) is a descent direction.</p> <p>Note that \\(\\inf_{\\|\\mathbf{d}\\|=1} \\mathbf{d}^\\top \\nabla f(\\mathbf{x}) = -\\nabla f(\\mathbf{x})\\), therefore \\(-\\nabla f(\\mathbf{x})\\) is called the steepest descent direction.</p> Proof <p>Let \\(\\mathbf{d}\\) be such that \\(\\mathbf{d}^\\top \\nabla f(\\mathbf{x}) &lt;0\\), by continuity of \\(\\nabla f\\) there exist \\(t_0\\) such that \\(\\mathbf{d}^\\top \\nabla f(\\mathbf{x}+t\\mathbf{d})&lt;0\\) for all \\(0&lt;t&lt;t_0\\). Applying Taylor for any \\(0&lt;t&lt;t_0\\) it holds</p> \\[ \\begin{equation*}   f(\\mathbf{x} + t \\mathbf{d}) = f(x) + t\\mathbf{d}^\\top \\nabla f(\\mathbf{x} + \\gamma t \\mathbf{d}) \\end{equation*} \\] <p>for some \\(0&lt;\\gamma &lt;1\\) from which follows the claim.</p>"},{"location":"lecture/05-Derivatives/053-gradient/#a-first-try","title":"A First Try","text":"<p>Following this result and the intuition, we therefore construct the sequence as follows. We start from \\(\\mathbf{x}_0\\) and update recursively with</p> \\[ \\begin{equation*}   \\mathbf{x}_{n+1} = \\mathbf{x}_n - \\gamma \\nabla f(\\mathbf{x}_n) \\end{equation*} \\] <p>and stop whenever we have something like \\(\\nabla f(\\mathbf{x}_n) \\approx 0\\). However from the proof, even if the steeest direction is \\(-\\nabla f(\\mathbf{x})\\), the amount in which you walk down along this direction is part of the Taylor expansion and it is not clear how large \\(\\gamma\\) shall be optimally.</p> <ul> <li>\\(\\gamma\\) large: Even if \\(\\nabla f(\\mathbf{x})\\) will decrease to \\(0\\) as you close the minimum, your iterative step \\(\\gamma\\) might be too large so that you are going to miss it by overshooting and will have to come back, eventually ending up into an oscillatory or unstable behavior.</li> <li>\\(\\gamma\\) small: You run into less problems of overshooting, but you will walk slowly making the overall scheme very slow.</li> </ul> <p>A lot of theory has been done on this topic for choosing the right step, with updating as you walk down. It depends very much on the problem, the properties of your function (convex, lipschitz, second differentiable, etc.)</p> <p>Though we do not enter in this problem here, we consider another strategy to improve the convergence called momentum closely related to exponential moving average, in the sense that we keep in our update a decaying memory of the previous gradients.</p> \\[ \\begin{equation*}   \\begin{cases}     \\delta_{n+1} &amp; = \\alpha \\delta_{n} + \\gamma \\nabla f(\\mathbf{x}_{n})\\\\     \\mathbf{x}_{n+1} &amp;= \\mathbf{x}_{n} - \\delta_{n+1}   \\end{cases} \\end{equation*} \\] <p>Doing a short expansion it follows that</p> \\[ \\begin{align*}   \\mathbf{x}_{n} &amp;= \\mathbf{x}_{n-1} - \\delta_{n}\\\\                 &amp; = \\mathbf{x}_{n-1} - \\gamma \\nabla f(\\mathbf{x}_{n-1}) - \\alpha \\delta_{n-1}\\\\                 &amp; = \\mathbf{x}_{n-1} - \\gamma \\nabla f(\\mathbf{x}_{n-1}) - \\gamma \\alpha \\nabla f(\\mathbf{x}_{n-1}) - \\alpha^2 \\delta_{n-2}\\\\                 &amp; \\vdots \\\\                 &amp; = \\mathbf{x}_{n-1} - \\gamma \\nabla f(\\mathbf{x}_{n-1}) - \\sum_{k=1}^{n-1}\\gamma \\alpha^k \\left(\\nabla f(\\mathbf{x}_{n-k}) + \\alpha \\delta_{n-1-k}\\right) \\end{align*} \\] <p>The second term in the sum is an exponential decaying function of the previous step size difference, therefore the term momentum (in time series it is often referred to as exponential moving average).</p>"},{"location":"lecture/05-Derivatives/053-gradient/#implementation-using-numpy","title":"Implementation using Numpy","text":"<pre><code>import numpy as np\n\n# define the gradient descent function with inputs\n# * max_iterations\n# * threshold\n# * x0 initialization\n# * obj_func\n# * grad_func\n# * alpha (learning rate)\n# * momentum\n# return the trajectory of the gradient descent as well as the value\ndef gradient_descent(\n    max_iterations,\n    threshold,\n    x0,\n    obj_func,\n    grad_func,\n    gamma=0.01,\n    alpha=0.9,\n):\n    # initialize the w and the history\n    x = x0\n    x_history = x\n    f_history = obj_func(x)\n    deltax = np.zeros(x.shape)\n\n    # initialize the iteration counter\n    i = 0\n\n    # initialize the previous loss\n    diff = 1e10\n\n    # loop until the max iterations\n    while i &lt; max_iterations and diff &gt; threshold:\n        deltax = alpha * deltax + gamma * grad_func(x)\n        x = x - deltax\n\n        # store the history\n        x_history = np.vstack([x_history, x])\n        f_history = np.vstack([f_history, obj_func(x)])\n\n        # update i and diff\n        i += 1\n        diff = np.abs(f_history[-1] - f_history[-2])\n\n    return x_history, f_history\n</code></pre> <p>With our gradient descent function at hand let us try on a simple function. We import some plotting functionalities too to illustrate.</p> <pre><code>import plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\n\n# define the objective and gradient functions\ndef func(x):\n    return np.sum(x**2)\n\ndef grad(x):\n    return 2 * x\n\n# plot the function\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\nX, Y = np.meshgrid(x, y)\ncart_prod = np.dstack((X, Y)).reshape(-1, 2)\nZ = np.apply_along_axis(func, 1, cart_prod)\n\nfig = go.Figure()\nfig.add_trace(\n    go.Surface(\n        x=x,\n        y=y,\n        z=Z.reshape(100, 100),\n        colorscale=\"Inferno\",\n        showscale=False,\n    )\n)\nfig.show()\n</code></pre> <p>We now run the gradient descent for different learning rate and moment values</p> <pre><code>learning_rates = np.array([0.05, 0.2, 0.5, 0.8])\nmomentums = np.array([0.0, 0.5, 0.9])\nmax_iterations = 5\nthreshold = 1e-6\n\nfig = make_subplots(cols=len(learning_rates), rows=len(momentums))\nfor idx, gamma in enumerate(learning_rates):\n    for idy, alpha in enumerate(momentums):\n        rng = np.random.default_rng(seed=10)\n        x0 = 20 * rng.random(2) - 10\n        x_history, f_history = gradient_descent(\n            max_iterations,\n            threshold,\n            x0,\n            func,\n            grad,\n            gamma=gamma,\n            alpha=alpha,\n        )\n\n        # add the plot of the function\n        fig.add_trace(\n            px.imshow(\n                Z.reshape(100, 100),\n                x=x,\n                y=y,\n                color_continuous_scale=\"Oranges\",\n                zmin=100,\n                zmax=200,\n            ).data[0],\n            row=idy + 1,\n            col=idx + 1,\n        )\n        # add the curve of the path with arrows\n        fig.add_trace(\n            go.Scatter(\n                x=x_history[:, 0],\n                y=x_history[:, 1],\n                mode=\"lines+markers\",\n                showlegend=False,\n                marker=dict(\n                    symbol=\"arrow\",\n                    size=15,\n                    angleref=\"previous\",\n                    color=\"blue\",\n                ),\n                line=dict(color=\"blue\"),\n            ),\n            row=idy + 1,\n            col=idx + 1,\n        )\nfig.update_layout(coloraxis_showscale=False)\nfig.show()\n</code></pre>"},{"location":"lecture/05-Derivatives/053-gradient/#basic-image-classification-on-real-data","title":"Basic Image Classification on Real Data","text":"<p><code>sklearn</code> provides several datasets, among which are blurred images of \\(0\\) and \\(1\\). The images are <code>8x8</code> pixels, that is arrays of length <code>64</code> where the value corresponds to the shade of grey (from white to black).</p> <pre><code># library to get the dataset as well as the train_test function to split the dataset\nimport sklearn.datasets as dt\nfrom sklearn.model_selection import train_test_split\n\n# load the images and their feature (if it is 0 or 1)\ndigits, target = dt.load_digits(n_class=2, return_X_y=True)\n\n# the shape of the dataset\nprint(digits.shape)\n\n# We plot a short sample \npx.imshow(digits.reshape(360, 8, 8)[:10, :, :], facet_col=0, binary_string=True)\n</code></pre> <p>The goal it to perform a simple linear regression but using gradient descent by minimizing among all \\(\\mathbf{w} = [w_0, \\ldots, w_{64}]^\\top\\) the objective function</p> \\[ \\frac{1}{N}\\sum (y_n - \\mathbf{w}^\\top \\bar{\\mathbf{x}}_n)^2 \\] <p>where \\(y_n\\) is either \\(0\\) or \\(1\\) for the label of the \\(n\\)-th image and \\(\\bar{\\mathbf{x}}_n = [1, \\mathbf{x}_n]\\) is the array of the image augmented with a \\(1\\).</p> <p>Compute the gradient descent and compare with the linear regression.</p>"},{"location":"lecture/06-Repairing-Improving/061-style/","title":"Style","text":"<p>Each programming language has its own set of rules in terms of coding in order to compile. However, beyond the requirement of the compiler, there are some additional unoffical rules in order to improve the readability, maintainance and exchange of code.</p> <p>As those set of rules are not enforce by the compiler, nothing prevents you to write the way you want. Furthermore, there are different schools of thoughts fighting against each other at which style should prevail, however here is a list of common standards plus some personal choice that will make your code readable.</p> <p>There are many references online about it, like here, there, there or there.</p> <p>In the following here are some my personal advices (I tend to deviate from the standard sometime). most of the advices here apply to <code>Python</code> but generically can be used for many other programming languages.</p>"},{"location":"lecture/06-Repairing-Improving/061-style/#developing-environment","title":"Developing Environment","text":"<p>Python scripts can be plainly written in a simple text editor and then run using python. However, since it is a scripting language, many editors allows to write pieces of code in cells that can be executed one after the other.</p>"},{"location":"lecture/06-Repairing-Improving/061-style/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Jupyter notebooks, (or its terminal form <code>Ipython</code>) allows to run code in cells without an editor since it runs locally on your browser. It is an easy way to develop some ideas as it allows to write code interleaved with <code>markdown</code> cells for additional comments, as well as show figures directly.</p> <p>One drawback is that it is really oriented to script on a single file. Furthermore, it saves the notebook with all the information (data imported, figures drawn, etc) rather than just the script and so notebook can become very large on the disk.</p>"},{"location":"lecture/06-Repairing-Improving/061-style/#vscode-pycharm-spyders-etc","title":"VScode, Pycharm, Spyders, etc.","text":"<p>Those editors are designed to develop (not necessarily only python) on project which might include multiple files. They have additional functionalities to debug, navigate the code across multiple files and show the results of different pieces (cells) of code directly. They are also modular in the sense that you can add additional functionalities (plugins) for special purposes.</p> <p>If you start to develop more intensively beyond just occasional single scripts, this is the recommended environment.</p>"},{"location":"lecture/06-Repairing-Improving/061-style/#old-school-terminal-editors","title":"Old School Terminal Editors","text":"<p>Terminal editors like <code>vi</code> or <code>Emacs</code> are working horses for developers for more than half a century. Their learning curve is very steep however they are extremely portable, take less space in memory, and are very modular with dozens of useful and powerful extension for coding.</p> <p>I personally use <code>neovim</code> which is a modern clone of <code>vi</code> in combination with several coding plugins and in combination with <code>Ipython</code> for <code>python</code> to run pieces of script in another terminal as <code>vscode</code> does.</p>"},{"location":"lecture/06-Repairing-Improving/061-style/#structure-your-project","title":"Structure your Project","text":"<p>After the choice of environment, instead of writing in loose <code>py</code> files all over the place try to keep a consistent organization of your files. Usually create a directory where you want to keep your scripts or longer project (with coding editors use the function <code>open directory</code> rather than <code>open file</code>).</p> <p>A typical directory can be organized as follows</p> <pre><code>.\n|-- data/\n|   |-- some_data.csv\n</code></pre>"},{"location":"material/ex01/","title":"Simple exercises","text":"<p>This is a subsample of the 100 exercises on Python that can be found here. Exists also in Chinese</p> <p>100+ Python challenging programming exercises</p> <ul> <li>Question 1</li> <li>Level 1</li> </ul> Question: Write a program which will find all such numbers which are divisible by 7 but are not a multiple of 5, between 2000 and 3200 (both included). The numbers obtained should be printed in a comma-separated sequence on a single line. Hints: Consider use <code>range(#begin, #end)</code> method Solution <pre><code>l=[]\nfor i in range(2000, 3201):\n    if (i%7==0) and (i%5!=0):\n        l.append(str(i))\n\nprint ','.join(l)\n</code></pre> <ul> <li>Question 2</li> <li>Level 1</li> </ul> Question: Write a program which can compute the factorial of a given numbers. The results should be printed in a comma-separated sequence on a single line. Suppose the following input is supplied to the program: 8 Then, the output should be: 40320 Hints: In case of input data being supplied to the question, it should be assumed to be a console input. Solution <pre><code>def fact(x):\n    if x == 0:\n        return 1\n    return x * fact(x - 1)\n\nraw_input = 10\nx=int(raw_input)\nprint fact(x)\n</code></pre> <ul> <li>Question 3</li> <li>Level 1</li> </ul> Question: With a given integral number n, write a program to generate a dictionary that contains (i, i*i) such that is an integral number between 1 and n (both included). and then the program should print the dictionary. Suppose the following input is supplied to the program: 8 Then, the output should be: <code>{1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64}</code> Hints: In case of input data being supplied to the question, it should be assumed to be a console input. Consider use <code>dict()</code> Solution <pre><code>n=20\nd=dict()\nfor i in range(1,n+1):\n    d[i]=i*i\n\nprint d\n</code></pre>"},{"location":"material/hw01/","title":"Homework 01","text":"Overall Info Due date: 2024-03-25 Returns in terms of a <code>*.py</code> file with comments for the code By group of 5-6 students"},{"location":"material/hw01/#1-datatypes-control-flows-functions","title":"1. Datatypes, Control Flows, Functions","text":"<p>Note</p> <p>For the following short exercises, no use of <code>numpy</code>.</p> <p>1.1 Using conditional statements and loops, count the number of numbers between 0 and 10.000 which are divisible by 3 or 7 and print it.</p> <p>1.2 Write a program that print the following pattern</p> <pre><code>ooooooooooooooooo                                                       \nooooooooooooooooo                                                       \nooooooooooooooooo                                                       \noooo                                                                    \noooo                                                                    \noooo                                                                    \nooooooooooooooooo                                                       \nooooooooooooooooo                                                       \nooooooooooooooooo                                                       \n             oooo                                                       \n             oooo                                                       \n             oooo                                                       \nooooooooooooooooo                                                       \nooooooooooooooooo                                                       \nooooooooooooooooo\n</code></pre> <p>1.3 Given a list <code>[\"apple\", \"orange\", \"cabage\", \"lemon\", \"potato\"]</code> extract the subarray containing cabage and potato, and return their indices in the original list. Furthermore, write a program that inverse the order of the list.</p> <p>1.5 Write a function that takes as input one of the name of the 12 month and return the number of days of this month (february is 28)</p> <p>1.6 Write a function that takes a list of numbers and return a ordered list. i.e. <code>[1, 4, 2] -&gt; [1, 2, 4]</code>. You can test this function by creating arbitraty list from numpy arrays <code>x = list(np.random.rand(N)</code>.</p>"},{"location":"material/hw01/#2-numpy","title":"2. Numpy","text":"<p>2.1 Using <code>numpy</code> and its functionalities redo the exercises 1.1 and 1.6</p> <p>2.2 Create two arrays of numbers between -1 and 1. The first one has consecutive numbers equally spaced by 0.01 while the second one has exactly 100 elements.</p> <p>2.3 Given natural numbers \\(n\\) and \\(d\\), create a \\(n\\times d\\) matrix of the form</p> <pre><code>[\n    [0, 1, ..., d-1],\n    [d, d+1, ..., 2d-1]\n    ...\n    [(n-1)d,...,(nd) -1 ]\n]\n</code></pre> <p>Note</p> <p>There are several ways to do it, but remember that loops are not efficient in python. Have a look at the shape manipulations in <code>numpy</code>.</p> <p>2.4 Create a 10x10 matrix where each line contains all the numbers 0 to 9 but shuffled uniformly randomly (check the random routines of numpy documentation)</p> <p>2.5 Normalize a random 5x5 matrix such that the smallest element is 0 and largest is 1</p> <p>2.6 Round a random array 5x5 with two digits after the coma</p> <p>2.7 Given a random array 100 find the position and value of the closest element to 0.6</p> <p>2.8 Write a function which given an array of size \\(N\\) of integers between 0 and 10 returns the histogram of the array by bins of 0.1 (that is number of elements between 0 and 0.1, number of elements between 0.1 and 0.2, ..., between 0.9 and 1</p>"},{"location":"material/hw01/#3-numpy-and-plotly-collatz-conjecture","title":"3. Numpy and Plotly: Collatz Conjecture","text":"<p>Collatz Conjecture</p> <p>Consider the following sequence \\(u = (u_i)\\) of natural numbers given by</p> \\[ \\begin{equation} u_{i+1} = \\begin{cases} u_i /2 &amp; \\text{if }u_i \\text{ is even}\\\\ 3u_i + 1 &amp; \\text{if }u_i \\text{ is odd} \\end{cases} \\end{equation} \\] <p>starting with a number \\(u_0 = n \\in \\mathbb{N}\\).</p> <p>The Collatz Conjecture states that whatever starting integer \\(n \\in \\mathbb{N}\\), the sequence will ultimately reach after some time \\(1\\). This Conjecture is still an open problem, some partial answer in this direction given recently by Terence Tao in 2019.</p> <p>For \\(n \\in \\mathbb{N}\\), denote by</p> \\[ \\tau(n) : = \\inf\\{i \\colon u_i = 1\\} \\] <p>which representes the first time where the sequence starting form \\(n\\) reaches \\(1\\). If this sequence never reaches \\(1\\) starting with \\(n\\), then \\(\\tau(n) = \\infty\\). Collatz conjecture states that \\(\\tau(n)&lt;\\infty\\) for every \\(n\\).</p> <p>There is strong confidence that this conjecture is true, and empirically, it holds for every natural number smaller than \\(2^{68}\\)</p> <p>3.1 Using the control flow <code>while</code>, given an integer \\(n\\) write a function that returns a <code>numpy</code> array \\((u_0, u_i, \\ldots, u_{\\tau(n)})\\). Mathematically, also write what is the domain and codomain of this function.</p> <p>3.2 Using <code>plotly</code>, provide a function where given a <code>numpy</code> array \\(x = (n_0, \\ldots, n_{d-1})\\) it plots \\(d\\) paths \\((u_0, \\ldots, u_{\\tau(n_k)})\\) where \\(u_0 = n_k\\) for \\(k=0, \\ldots, d-1\\).</p> <p>3.4 Write a third function where given a <code>numpy</code> array \\(x = (1, ldots, d)\\) it returns the array \\((\\tau(1), \\ldots, \\tau(d))\\).</p> <p>3.5 Using <code>bar</code> ploting functionality (<code>fig.add_bar(...)</code>) of <code>plotly</code>, write a function that plot the histogram of the previous function, that is \\((l_1, l_2, \\ldots)\\) where</p> \\[ \\begin{equation} l_k = \\text{Cardinality}\\{n\\colon \\tau(n) = k, \\quad 0\\leq n \\leq N\\} \\end{equation} \\] <p>for (depending on the performance of your computer) \\(N=1000, 10000, 100000\\).</p>"},{"location":"material/hw01/#4-scipy","title":"4. Scipy","text":"<p>4.1 Consider the function \\(x \\mapsto f(x) = x ** 2 + 10 * sin(x)\\) for \\(x \\in \\mathbb{R}\\)</p> <ul> <li>plot the function</li> <li>find the minimum</li> </ul> <p>4.2 For the following distributions:</p> <ul> <li>normal with std = 1, 2, 5</li> <li>student with degree of freedom 2, 3, 4, 6</li> </ul> <p>Plot the pdf and cdf of each class for different parameters and compare</p>"},{"location":"material/hw02/","title":"Homework 02","text":"Overall Info Due date: 2024-04-22 Returns in terms of a <code>*.py</code> file with comments for the code By group of 5-6 students"},{"location":"material/hw02/#1-monte-carlo-convergence","title":"1. Monte Carlo Convergence","text":"<p>We compute the following expectation</p> \\[ \\begin{equation} E[(X - K)^+] = \\int_{-\\infty}^{\\infty}(x-K)^+ dF_X(x) = \\int_{-\\infty}^{\\infty}(x-K)^+ f_X(x)dx \\end{equation} \\] <p>where \\(K\\) is a constant, \\(F_X\\) is the <code>cdf</code> of \\(X\\) and \\(f_X=dF_X/dx\\) is the <code>pdf</code> of \\(X\\).</p> <p>We assume throughout that \\(X\\sim \\mathcal{N}(\\mu, \\sigma^2)\\) is a guaussian distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Such a random variable is declared in <code>scipy</code> as follows</p> <pre><code>from scipy.stats import norm\n\nmu = 0.5\nsigma = 0.2\n\nRV = norm(loc = mu, scale = sigma)\n</code></pre> <p>1.1 Define a function <code>expectation</code> that takes as input the random variable <code>RV</code>, the constant <code>K</code> and return the value of the integral using <code>quad</code>.</p> <p>1.2 Define a function <code>mc_expectation</code> that takes as input the random variable <code>RV</code>, the constant <code>K</code> and the natural number <code>N</code> (the number of samples) that retuns a <code>numpy</code> vector <code>[I_0, ..., I_{N-1}]</code> where</p> \\[ \\begin{equation} I_k = \\frac{1}{k} \\sum_{l=0}^{k-1} (x_k-K)^+ \\end{equation} \\] <p>where <code>x = [x_0, \\ldots, x_{N-1}]</code> is a random sample from the distribution of \\(X\\).</p> <p>1.3 For \\(N = 100000\\), with a plotly graph, plot</p> <ul> <li>The constant function <code>expectation(RV, K)</code> for <code>n=0, ..., N-1</code></li> <li>10 functions <code>mc_expectation(RV, K, N)</code> (they already return a vector <code>I = [I_0, ..., I_{N-1}]</code>)</li> </ul> <p>Comment on the speed of convergence of monte carlo method (you can vary \\(N\\) as well as the number of monte carlo samples you compute).</p>"},{"location":"material/hw02/#2-quantile-exact-vs-mc-methods","title":"2. Quantile: exact vs MC methods","text":"<p>From the lecture we know</p> Value at Risk: The value at risk is defined as \\(V@R_{\\alpha}(X) = q_X(1-\\alpha) = F_X^{-1}(1-\\alpha)\\) where \\(q_X\\) is the quantile of \\(X\\). <p>Leads to three methods to compute it.</p> <ul> <li>You already have a <code>ppf</code> function at hand for the quantile</li> <li>You compute the inverse of the <code>cdf</code> using <code>root</code></li> <li>You have just random sample of the distribution and compute the empirical quantile.</li> </ul> <p>2.1 Define a <code>quantile00</code> function that takes as input a known <code>RV</code> and a number <code>0&lt;u&lt;1</code> and return the quantile with a root finding method to invert the <code>cdf</code></p> <p>2.2 Compare the speed of computation for different <code>RV</code> (normal, student, normal inverse guaussian from the <code>scipy.stats</code> library) of this function with respect to the <code>ppf</code> function of those random variable.</p> <p>2.3 Sometimes, you do not have access to the <code>ppf</code> or <code>cdf</code> explicitely but just have \\(N\\) random samples \\(x^N = (x_0^N, ..., x^N_{N-1})\\) of this random variable. Numpy has a functionality to return the quantile of a random sample (there are many ways to interpolate it, see documnetation). Given a random sample (as a numpy array) <code>x=[x_0, ..., x_{N-1}]</code> the quantile is given by <code>np.quantile(x, u)</code>. This empirical quantile converges to the normal quantile. Experiment with different random samples from a normal random variable (<code>rvs</code>) and plot the convergence as a function of \\(N\\) of the empirical quantile to the theoretical quantile.</p> Empirical Quantile <p>Note that the general definition of the (right) quantile is given by</p> \\[ \\begin{equation}     q_X(u) := \\inf \\left\\{ x \\in \\mathbb{R}\\colon P[X\\leq x] \\geq u \\right\\} = \\inf \\left\\{ x \\in \\mathbb{R}\\colon F_X(x) \\geq u \\right\\} \\end{equation} \\] <p>If you have a a random sample \\((x_0, \\ldots, x_{N-1})\\) of \\(X\\), denote by </p> \\[ \\begin{equation}     F^N(x) = \\frac{\\# \\{k \\colon x_k \\leq x\\}}{N} \\end{equation} \\] <p>It holds that \\(F^N \\to F_X\\) if \\(F_X\\) is continuous (is not easy to show, it holds in general gut in a distributional sense).</p> <p>Hence, the quantile \\(q^N\\) of \\(F^N\\) converges too to \\(q_X\\) as \\(N\\) is large.</p> <p>If you denote by \\((y_0, \\ldots, y_{N-1})\\) the reordering of \\((x_0, \\ldots, x_{N-1})\\) from the smallest to the largest value, then it holds</p> \\[     q^N(u) = y_{k_u} \\] <p>where </p> \\[     k_u =      \\begin{cases}         \\inf\\{k \\colon k\\geq Nu\\} &amp; \\text{if }Nu\\leq N-1\\\\         N-1 &amp; \\text{otherwize}     \\end{cases} \\]"},{"location":"material/hw02/#3-average-value-at-risk","title":"3. Average Value at Risk","text":"<p>We discussed in the lecture that the value at risk (which is a quantile) is not appropriate to estimate the risk. It has been replaced by the average value at risk which has different representations</p> \\[ \\begin{align} AV@R_{\\alpha}(X) &amp; = \\frac{1}{\\alpha}\\int_{1-\\alpha}^1 q_X(u) du\\\\ &amp; = \\inf\\left\\{x + \\frac{1}{\\alpha}E\\left[(X - x)^+\\right]\\colon x \\in \\mathbb{R}\\right\\}\\\\ &amp; = q_X(1-\\alpha) + \\frac{1}{\\alpha}E\\left[ (X - q_X(1-\\alpha))^+ \\right] \\end{align} \\] <p>3.1 Implement the <code>AVaR0</code>, <code>AVaR1</code> and <code>AV@R2</code> functions with input <code>RV</code> and <code>alpha</code> and return with <code>quad</code> the result for the first, second and third representation using <code>ppf</code> and/or <code>minimize</code> depending on the representation</p> <p>3.2 Implement the <code>mc_AVaR0</code>, <code>mc_AVaR1</code> and <code>mc_AVaR2</code> functions with input <code>x = [x_0, \\ldots, x_{N-1}]</code> numpy random sample of \\(X\\) and <code>alpha</code> using Monte carlo and empirical quantile for each representations</p> <p>3.3 Compare numerically the speed and accuracy of each method.</p>"},{"location":"material/hw02/#4-multidimensional","title":"4. Multidimensional","text":"<p>Usually in risk managment, the random variable \\(X\\) is a combination of many random variables</p> \\[ \\begin{equation} X = \\sum_{k=1}^d w^k X^k \\end{equation} \\] <p>where the vector \\((X^1, \\ldots, X^d)\\) has a given <code>cdf</code> or random samples, and \\(w^1, \\ldots, w^d\\) are numbers representing the contribution of each factor \\(X^k\\) to the total risk \\(X\\).</p> <p>Throughout, we will consider that \\((X^1, \\ldots, X^d)\\) is a \\(d\\)-dimensional normal Gaussian random variable.</p> Multivariate Gaussian distribution <p>The multivariate Gaussian distribution is the multidimensional extension of a Gaussian distribution for a vector of random variables \\(X=(X^1, \\ldots, X^d)\\). The parameters are the mean \\(\\mathbf{\\mu} = (\\mu^1, \\ldots, \\mu^d)\\) and the covariance matrix (positive semi definite matrix) \\(\\mathbf{\\Sigma} \\in \\mathbb{R}^{d\\times d}\\).</p> <p>The <code>pdf</code> of this multidimensional random variable is given by</p> \\[ \\begin{equation} f(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2} \\sqrt{\\det(\\mathbf{\\Sigma})}} \\exp\\left(-\\frac{1}{2}\\left(\\mathbf{x} - \\mathbf{\\mu}\\right)^\\top \\mathbf{\\Sigma}\\left(\\mathbf{x} - \\mathbf{\\mu}\\right)\\right)  \\end{equation} \\] <p>Denoting by \\(\\sigma^k = \\sqrt{\\mathbf{\\Sigma}^{k,k}}\\) the square root of the diagonal of \\(\\mathbf{\\Sigma}\\), then each random variable \\(X^k\\) is a normal distribution \\(\\mathcal{N}(\\mu^k, (\\sigma^k)^2)\\).</p> <p>However, the different components of the random vector might be dependent as </p> \\[ \\begin{equation} E[(X^k - \\mu^k)(X^l - \\mu^l)] = \\mathbf{\\Sigma}^{k,l} \\end{equation} \\] <p>4.1 Extend the functions <code>mc_AVaR0</code>, <code>mc_AVaR1</code> and <code>mc_AVaR2</code> to multidimensional samples. Use the following two case scenarios to compare accuracy and execution time by varying \\(N\\) the number of samples </p> <pre><code># case of 2 dimensions\nimport numpy as np\nfrom scipy.stats import multivariate_normal\n\nmu = np.array([0.2, 0.5])\nSigma = np.array(\n    [\n        [ 1.        , -0.26315789],\n        [-0.26315789,  1.        ]\n    ]\n)\nw = np.array([2, 5])\n\nRV_2dim = multivariate_normal(mean = mu, cov = Sigma)\n\n# generate N random samples (you get a numpy array Nx2)\nN = 10\nsamples = RV_2dim.rvs(N)\n\n# case of 5 dimensions\nmu = np.array([0.2, 0.5, -0.1, 0, 0.6])\n\nSigma = np.array(\n    [\n        [1.        , 0.2688825 , 0.401427  , 0.19473116, 0.66256879],\n        [0.2688825 , 1.        , 0.3907619 , 0.43373298, 0.43199657],\n        [0.401427  , 0.3907619 , 1.        , 0.27893741, 0.61330745],\n        [0.19473116, 0.43373298, 0.27893741, 1.        , 0.46849892],\n        [0.66256879, 0.43199657, 0.61330745, 0.46849892, 1.        ]\n    ]\n)\nw = np.array([2, 5, -2, 3, 6])\n\nRV_5dim = multivariate_normal(mean = mu, cov = Sigma)\n\n# generate N random samples (you get a numpy array Nx5)\nN = 10\nsamples = RV_5dim.rvs(N)\n</code></pre> <p>Hint: Do not forget that numpy gives you access to the <code>dot</code> product to compute \\(\\sum w^k X^k\\).</p> <p>4.2 Optional: you can try for the two dimensional case to implement with <code>dblquad</code> the direct computation of the <code>AVaR1</code> to compare speed and accuracy.</p> Any dimensional multidimensional Gaussian distributions <p>To create a multidimensional guassian vector, you need to provide the mean vector \\(\\mathbf{\\mu} = (\\mu^1, \\ldots, \\mu^d)\\) as well as the covariance matrix \\(\\mathbf{\\Sigma}\\) which is usually calibrated to data. In our case we can generate some of these distribution using <code>numpy</code> and <code>scipy</code></p> <pre><code># we import the multivariate normal as well as a function to generate Sigma\nfrom scipy.stats import multivariate_normal, random_correlation\nimport numpy as np\n\n# create a random vector of positive eigenvalues and then random covariance\nd=4\neigenvalues = np.random.rand(d)\nSigma = random_correlation.rvs(eigenvalues) # covariance matrix\nmu = no.random.rand(d)                      # vector of mean\n\nRV = multivariate_normal(mean = mu, cov = Sigma)\n\n# generate random samples (N random vectors or dimension d each so Nxd numpy array)\nN = 1000\nsamples = RV.rvs(N)\nprint(samples)\n</code></pre>"},{"location":"material/hw03/","title":"Homework 03","text":"Overall Info Due date: 2024-05-24 Returns in terms of a <code>*.py</code> file with comments for the code By group of 5-6 students"},{"location":"material/hw03/#1-linear-regression-fake-data","title":"1. Linear Regression Fake Data","text":"<p>To illustrate the principle of linear regression we consider the following model</p> \\[ \\begin{equation*}     Y = a + b_1 X_1 +b_2 X_2 + \\varepsilon = \\mathbf{X}\\mathbf{b} + \\varepsilon \\end{equation*} \\] <p>where</p> \\[ \\begin{equation*}     \\mathbf{b} =     \\begin{bmatrix}         a\\\\         b_1\\\\         b_2     \\end{bmatrix}     \\quad     \\text{and}     \\quad     \\mathbf{X} =     \\begin{bmatrix}         1 &amp; X_1 &amp;  X_2     \\end{bmatrix} \\end{equation*} \\] <p>We assume that</p> \\[ \\begin{equation*}     \\begin{bmatrix}         X_1 \\\\         X_2     \\end{bmatrix}     \\sim     \\mathcal{N}\\left(     \\begin{bmatrix}         \\mu_1 \\\\         \\mu_2     \\end{bmatrix}     ,     \\begin{bmatrix}         \\sigma_1^2 &amp; \\sigma_1 \\sigma_2 \\rho\\\\         \\sigma_1 \\sigma_2 \\rho &amp; \\sigma_2^2     \\end{bmatrix}     \\right)     \\quad \\text{and}\\quad     \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2) \\end{equation*} \\] <p>with \\(\\varepsilon\\) independent of \\((X_1, X_2)\\).</p> <p>Consider the following specifications</p> <pre><code>import numpy as np\n\nb = np.array([1, 1, -0.5]).T\n\nmu = np.array([1, 0]).T\nsigma1 = 1\nsigma2 = 0.5\nrho = 0.4\nSigma = np.array(\n    [\n        [sigma1 ** 2, sigma1 * sigma2 * rho],\n        [sigma1 * sigma2 * rho, sigma2 ** 2]\n    ]\n)\n\nsigma = 2\n\n# We prepare the dataset\n\n# Fix random number generator and number of samples\nrng = np.random.default_rng(seed = 150)\nN = 1000\n\n# generate samples\nX = rng.multivariate_normal(mu, Sigma, size = N)        # size N x 2\nepsilon = rng.normal(0, sigma, size = N)                # size N\n\n# Add an axis of 1 to X\nX = np.append(np.ones((N, 1)), X, axis = 1)             # append Nx1 to Nx2 -&gt; Nx3\n\n# Generate Y\nY = X.dot(b) + epsilon                                  # size N\nY\n</code></pre>"},{"location":"material/hw03/#question-1","title":"Question 1:","text":"<ul> <li>Generate a scatter plot of \\(Y\\) against \\(X_1\\) and \\(X_2\\), as well as a scatter plot of \\(X_1\\) against \\(X_2\\).</li> <li>Implement the computation of \\(\\hat{\\mathbf{b}}(N)\\) from the \\(N\\) sample data and compare the obtained values with the true one.</li> <li>Return the residual error \\(\\hat{\\varepsilon}(N)=Y - \\hat{\\mathbf{b}}(N)\\cdot \\mathbf{X}\\) and plot its histogram.</li> </ul>"},{"location":"material/hw03/#question-2","title":"Question 2:","text":"<p>Since we fixed the random generator with <code>seed=150</code> you always get the same result for \\(\\hat{\\mathbf{b}}(N)\\) as well as \\(\\hat{\\varepsilon}(N)\\).</p> <p>Two source of error can come into the linear regression: The randomness in the sample (the seed) as well as the number of samples <code>N</code>.</p> <ul> <li> <p>Define a function <code>f(N, M)</code> where <code>N</code> is the number of samples, and \\(M\\) is the number of trials where you draw this sample. This function shall return</p> </li> <li> <p>an array \\(\\hat{\\mathbf{b}}(N)\\) of size <code>3xM</code> for each computation of the regression coefficient (in the random generator you set <code>seed = None</code>)</p> </li> <li> <p>an array \\(\\hat{\\varepsilon}(N)\\) of size <code>NxM</code> of the corresponding residual for each sample drawn.</p> </li> <li> <p>Fix <code>M=100</code> and for <code>N = 10, 100</code> and <code>1000</code> plot the histogram of</p> </li> <li> <p><code>a(N)</code>, <code>b_1(N)</code> and <code>b_2(N)</code> (they are arrays of <code>M=100</code> values)</p> </li> <li>mean and standard deviation in the direction of the <code>N</code> axis of \\(\\hat{\\varepsilon}(N)\\) (you get two arrays of <code>M=100</code> values)</li> </ul>"},{"location":"material/hw03/#2-linear-regression-real-data","title":"2. Linear Regression Real Data","text":"<p>As seen in the lecture, we are given a set of data in a dataframe <code>df</code> with</p> <ul> <li>a column <code>y</code> for the serie of outputs;</li> <li><code>d</code> columns <code>x_0</code>, ..., <code>x_d</code> for the series of outputs</li> </ul> <p>In the following example we consider a dataset about wine</p>"},{"location":"material/hw03/#question-1_1","title":"Question 1:","text":"<p>Proceed through the following:</p> <ul> <li>Load the dataset with pandas, check if the data are correct and provide some descriptive statistics.</li> </ul> <p>The output \\(\\mathbf{y}\\) is <code>quality</code>, the column <code>type</code> stands for the rows that are either <code>red</code> or <code>white</code>, all the other columns are characteristics of the wine.</p> <ul> <li> <p>Using plotly scatter plot, visualize for <code>red</code> and <code>white</code> the relation between each input dimension and output dimension.</p> </li> <li> <p>Implement using <code>statsmodels</code> the ols regression of <code>quality</code> against the inputs and show the results.</p> </li> </ul>"},{"location":"material/hw03/#question-2_1","title":"Question 2.","text":"<p>The number of features is quite large and it is not clear which feature is relevant or not in the linear regression. The goal is to reduce the number of features as to explain as much as possible the output.</p> <p>Without entering in feature selection overall, we just want to see which input is the most relevant. One indicator for the goodness of a linear regression is <code>rsquared</code> which can be obtained from the returned fitted model <code>est = sm.OLS(y, X).fit()</code> and then <code>est.rsquared</code>.</p> <p>We just try to get the two best features</p> <ul> <li>Loop through every single input feature, perform the linear regression, get the rsquared.</li> <li>take the feature with the largest rsquared.</li> <li>Loop through each other feature, perform the linear regression together with the previously selected and first feature</li> <li>select the feature with the largest rsquared.</li> </ul> <p>Note</p> <p>Normally you should also consider if the new feature collected is not strongly colinear with the first one. To do so you should double check the VIF factor, see <code>variance_inflation_factor</code>.</p>"},{"location":"material/hw03/#3-clustering","title":"3. Clustering","text":"<p>The principle of clustering is as follows. Given a set \\(X = \\{x_1, \\ldots, x_N\\}\\) of \\(N\\) points in \\(\\mathbb{R}^d\\), the goal is to find a partition (cluster) \\(C_1, \\ldots C_K \\subseteq X\\) which somehow group similar points.</p> <p>Similarity between points is defined in terms of some distance \\(d(x,y)\\). The clustering aims to find an optimal cluster \\(C_1^\\ast, \\ldots, C_K^\\ast\\) such that</p> \\[ \\begin{equation} \\sum_{k=1}^K \\frac{1}{\\# C^\\ast_k}\\sum_{x, y \\in C_k^\\ast} d(x, y) \\leq \\sum_{k=1}^K \\frac{1}{\\# C_k}\\sum_{x, y \\in C_k} d(x, y) \\end{equation} \\] <p>for any other cluster \\(C_1, \\ldots, C_K\\).</p> <p>We denote by \\(\\mathfrak{C}\\) the set of all clusters (or partitions) \\(\\mathcal{C} = \\{C_1, \\ldots, C_K\\}\\) of \\(X\\) in \\(K\\) elements and define</p> \\[ \\begin{equation*}     F(\\mathcal{C}) = \\sum_{C \\in \\mathcal{C}} \\frac{1}{\\# C}\\sum_{x, y \\in C} d(x, y) \\end{equation*} \\] <p>the problem can therefore be reformulated into an optimization problem</p> \\[ \\begin{equation*}     \\mathcal{C}^\\ast = (C_1^\\ast, \\ldots, C_K^\\ast) = \\mathrm{argmin}\\left\\{F(\\mathcal{C})\\colon \\mathcal{C} \\in \\mathfrak{C}\\right\\} \\end{equation*} \\] <p>Computing \\(F\\) for a given cluster \\(\\mathcal{C}\\) is relatively fast as long as the distance is quick to compute. However, the optimization problem itself is very difficult. Indeed, it is a minimization problem on a set \\(\\mathfrak{C}\\) which does not have a suitable topology to define derivatives for instance. Hence, the only way a priori would be a brute force optimization, that is running through every possible partition, which is however not suitable since the cardinality of \\(\\mathfrak{C}\\) is gigantic. It corresponds to the stirling number of the second kind \\({ N \\brace K}\\):</p> \\[ \\begin{equation*}     \\#\\mathfrak{C} := {N\\brace K} = \\sum_{k=0}^K \\frac{(-1)^{K-k} k^N}{(K-k)!k!} \\sim_{N\\to \\infty} \\frac{K^N}{K!} \\end{equation*} \\] <p>meaning that for a fixed number \\(K\\), the cardinality is growing exponentially in the size of the set. The problem can be refined and some better approximation can be found but in general this is NP-Hard.</p> <p>However, with some assumptions about the distance, and geometrical consideration, an honnest and fast algorithm can be designed to achieve some local optimum.</p> <p>We consider as ''distance''' the square of the euclidean norm, that is \\(d(x,y) = \\|x - y\\|^2\\).</p>"},{"location":"material/hw03/#question-1_2","title":"Question 1.","text":"<p>Show that</p> \\[ \\begin{equation*}     \\frac{1}{\\# C} \\sum_{x, y \\in C} \\| x - y \\|^2 = 2 \\sum_{x \\in c} \\|x - \\mu\\|^2 \\end{equation*} \\] <p>where \\(\\mu = \\frac{1}{\\# C} \\sum x\\) is the average/barycenter or centroid of \\(C\\).</p>"},{"location":"material/hw03/#question-2_2","title":"Question 2.","text":"<p>It follows that</p> \\[ \\begin{equation*}     F(\\mathcal{C}) = \\sum_{C \\in \\mathcal{C}} \\sum_{x \\in X} \\|x - \\mu_{C}\\|^2 \\end{equation*} \\] <p>With this reformulation in term of geometric center of \\(C\\) leads to the following idea for an algorithm to select a partition.</p> <ol> <li>Initialize \\(K\\) centers \\(\\mu_1(0), \\ldots, \\mu_K(0)\\) by choosing \\(K\\)-points in \\(X\\).</li> <li> <p>Recursively: While \\(\\mathcal{C}(n+1) \\neq \\mathcal{C}(n)\\) at the end of the following do:</p> </li> <li> <p>Given \\(K\\) \\(\\mu_1(n), \\ldots \\mu_K(n)\\) define \\(K\\) sets \\(C_1(n+1), \\ldots, C_K(n+1)\\):</p> <p>\\(C_k(n+1) = \\left\\{x \\in X\\colon \\|x - \\mu_K(k)\\|^2 \\leq \\|x - \\mu_K(j)\\|^2 \\text{ for any }j\\neq k\\right\\}\\)</p> <p>If some point is assigned to two or more then set it to a single one. The best way to do it, is to assign the points to the first cluster, then assign the remaining points to the second one, etc.</p> </li> <li> <p>update the new centers \\(\\mu_1(n+1), \\ldots, \\mu_K(n+1)\\):</p> <p>\\(\\mu_k(n+1) = \\frac{1}{\\# C_k(n+1)} \\sum_{x \\in C_k(n+1)} x\\)</p> </li> </ol> <ul> <li>Show that at each step \\(F(\\mathcal{C}(n+1))\\leq F(\\mathcal{C}(n)\\), hence we find a sequence along which the cost function is decreasing.</li> <li>Show that the algorythm finishes after a finite number of steps.</li> <li>implement the algorythm in numpy by choosing randomly \\(k\\) elements of the set \\(X\\). (the set \\(X\\) can be represented by a numpy array <code>Nxd</code>.)</li> </ul>"},{"location":"material/hw03/#question-2_3","title":"Question 2.","text":"<p>Consider the dataset California Housing which represents the housing data for California.</p> <ul> <li>Load the dataset and select the columns <code>longitude</code>, <code>lattitude</code> and <code>medianIncome</code> as final dataframe <code>df</code>.</li> <li>Install (using conda or pip) the package <code>scikit-learn</code> which is a standard machine learning library.</li> <li>Cluster the data with 4 clusters using Kmeans: <code>from sklearn.cluster import KMeans</code>.</li> <li>given a numpy array <code>X</code> of size <code>N x d</code>, computing the cluster with <code>Kmeans</code> is done as follows <code>result = KMeans(n_clusters = 4).fit(X)</code> and the labels for the cluster are given by <code>result.labels_</code> which is an array of size <code>N</code>.</li> <li>join the cluster values in the dataframe as a new column.</li> <li>plot using plotly express scatter the scatter plot latitude against longitude with a different color for each cluster.</li> </ul>"},{"location":"material/hw04/","title":"Homework 04","text":"Overall Info Due date: 2024-06-26 Returns in terms of a <code>*.py</code> file with comments for the code By group of 5-6 students"},{"location":"material/hw04/#1-ode-implementation","title":"1. ODE implementation","text":"<p>Harmonic Oscillator:</p> \\[ \\begin{equation*}   x^{\\prime\\prime} + \\omega^2 x = 0 \\end{equation*} \\] <ul> <li>setting \\(v = x^\\prime\\), convert this second order ode into a two dimensional first order ODE \\((x, v)\\).</li> <li>Use Euler scheme to compute the solution with \\(\\omega=1\\) with \\(x(0) = 1\\), \\(v(0) =1\\)</li> <li>plot the curve \\(t\\mapsto (x(t), v(t))\\) for \\(0\\leq t \\leq 10\\).</li> </ul> <p>Consider the Damped harmonic oscillator:</p> \\[ \\begin{equation*}   x^{\\prime\\prime} + 2\\gamma x^\\prime +  \\omega^2 x = 0 \\end{equation*} \\] <ul> <li>same notation convert this equation into first order ode in two dimensions.</li> <li>Use RK 4th order to compute the solution \\(\\omega = 1\\) and \\(\\gamma = 0.1\\) and \\(x(0) = v(0)=1\\).</li> <li>plot the solution and compare with the previous solution</li> </ul>"},{"location":"material/hw04/#2-forward-vs-backward-explicit-vs-implicit","title":"2. Forward vs Backward (explicit vs implicit).","text":"<p>Explicit Euler</p> \\[ \\begin{equation*}   y(t+h) = y(t) + h f(t, y(t)) \\end{equation*} \\] <p>Implicit Euler</p> \\[ y(t+h) = y(t) + h f(t+h, y(t+h)) \\] <p>This second method involves solving a root problem to get \\(y(t+h)\\).</p> <ul> <li>Using <code>root</code> from <code>scipy.minimize</code> implement the implicit Euler</li> </ul> <p>Consider the ODE</p> \\[ \\begin{equation*}   y^\\prime = -\\lambda y, \\quad y(0) = 1 \\end{equation*} \\] <p>with solution \\(y(t) = e^{-\\lambda t}\\)</p> <ul> <li>use explicit Euler, RK 4th to compare with implicit Euler (precision and speed)</li> </ul>"},{"location":"material/hw04/#3-gradient-descent","title":"3. Gradient Descent","text":"<p>We implement the gradient descent on the blurred \\(0\\) and \\(1\\) images from <code>sklearn</code> package.</p> <pre><code>import numpy as np\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\n\nimport sklearn.datasets as dt\nfrom sklearn.model_selection import train_test_split\n\n# load the images and their feature (if it is 0 or 1)\ndigits, target = dt.load_digits(n_class=2, return_X_y=True)\n\n\n# the shape of the dataset\nprint(digits.shape)\n\n# We plot a short sample \npx.imshow(digits.reshape(360, 8, 8)[:10, :, :], facet_col=0, binary_string=True)\n</code></pre> <p>The goal it to perform a simple linear regression but using gradient descent by minimizing among all \\(\\mathbf{w} = [w_0, \\ldots, w_{64}]^\\top\\) the objective function</p> \\[ \\frac{1}{N}\\sum (y_n - \\mathbf{w}^\\top \\bar{\\mathbf{x}}_n)^2 \\] <p>where \\(y_n\\) is either \\(0\\) or \\(1\\) for the label of the \\(n\\)-th image and \\(\\bar{\\mathbf{x}}_n = [1, \\mathbf{x}_n]\\) is the array of the image augmented with a \\(1\\).</p> <ul> <li>modify the objective function and gradient of which so that it takes as input \\(\\mathbf{w}\\) \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) and modify the gradient descent function.</li> </ul> <p>Since our goal is to find a correct specification to further classify data, we calibrate the model on a training set and evaluate its accuracy on a testing set. To split the two set of data we use <code>sklearn</code></p> <pre><code>x_train, x_test, y_train, y_test = train_test_split(\n  digits,\n  target,\n  test_size=0.2,\n  random_state=10\n)\n</code></pre> <ul> <li>Using the gradient descent calibrate on the training set <code>x_train</code>, <code>y_train</code> the optimal value of \\(\\mathbf{w}\\). Plot the value of <code>f_history</code> (choose a maximum number of iteration of 100 and 0.1 as threshold and tweak a little bit with momentum and learning rate until you reach a satisfactory convergence rate and error)</li> </ul> <p>Now we want to be able to classify the data. We proceed as follows</p> <p>If \\(\\mathbf{w}^\\top \\bar{\\mathbf{x}} \\geq 0.5\\) it is classified as \\(1\\) otherwise it is classified as \\(0\\).</p> <ul> <li> <p>Given \\(\\mathbf{w}\\), \\(N\\) samples \\((\\mathbf{x}_1, y_1), \\ldots,(\\mathbf{x}_N, y_N)\\), design a function <code>accuracy(w, x, y)</code> that returns the percentage of correctly classified values.</p> </li> <li> <p>Using this function, provide the accuracy of your solutions on the testing set <code>(x_train, y_train)</code> and on the test set <code>(x_test, y_test)</code>.</p> </li> <li> <p>Optional Question: compare the gradient descent method with the traditional OLS linear regression method.</p> </li> </ul>"},{"location":"material/material/","title":"Setup","text":"<p>Any normal computer with either Linux, Windows of MacOS will do it.</p> <p>What is primarily needed:</p> <ul> <li>Python</li> <li>A Code editor</li> </ul>"},{"location":"material/material/#python","title":"Python","text":"<p>Python can be installed in many different ways (In linux it is for instance most of the time already in the system). Several python (with different versions) can run under the same computer.</p> <p>However the most simple and best advice is to use Anaconda</p> <ol> <li>Step 1: Download Anaconda for your platform</li> <li>Step 2: Install on your computer</li> </ol> Miniconda <p>Anaconda comes with a GUI software to manage the environment and packages with point and click. It also installs a default set of packages such as <code>Jupyther</code>, <code>numpy</code>, etc. If you prefer to install a minimal version and install only the packages you need one after the other you can install Miniconda</p> <p>Anaconda usually comes with a python interpreter called <code>ipython</code> that allows to run code directly from a console or an editor.</p>"},{"location":"material/material/#code-editor","title":"Code Editor","text":"<p>Two write code you only need an editor, however dedicated editors allows you to program more efficiently.</p> <ul> <li>Jupyther Notebook:     Allows you to run on the browser so called notebook where you can input code, text, and run each cell.     Good for pure beginner.</li> <li>VSCode:     Is a multi platform open source editor maintained and released by Microsoft.     It is a great environment for development.     The principle is that it is a basic editor in which you can install so called plugins (mini apps like in wechat or allipay).     Download and install.     Then go to the plugins repository and install the <code>python</code> plugin from Microsoft.</li> </ul> <p>Good practice</p> <p>It is recommended to have a directory in your computer containing your code files (for organization purposes and also because python will run as environment in this directory).</p>"},{"location":"material/material/#installing-additional-libraries","title":"Installing additional libraries","text":"<p>Python can be extended with libraries this is one of the strength of it that will perform tasks for you. Installing a new library can be done in three ways with anaconda:</p> <ol> <li>Use the GUI and search for the library</li> <li>Open a terminal and type <code>conda install &lt;library&gt;</code></li> <li>Open a terminal and use pip with <code>pip install &lt;library&gt;</code></li> </ol> <p>Warning</p> <p>The first and second options are preferable usually. Indeed, libraries have a complex system of inter-dependence and since you are likely using Anaconda, the tool <code>conda</code> will manage the inter-dependence of each packages better. It is however slower.</p>"},{"location":"material/material/#which-libraries","title":"Which libraries","text":"<p>In the lecture we will use quite a lot of libraries and install them on the go. Fundamentally the following ones will be recurrent</p> <ul> <li><code>numpy</code>: multidimensional array library</li> <li><code>pandas</code>: data analysis (tabular) framework </li> <li><code>scipy</code>: scientific library</li> <li><code>pytorch</code>: AI and ML library with tensors</li> <li><code>plotly</code>: Data visualization</li> </ul>"}]}